travis_fold:start:worker_info[0K[33;1mWorker information[0m
hostname: 56c16b5b-ade5-4242-b70f-873eca1f6f38@1.production-2-worker-org-04-packet
version: v4.0.0 https://github.com/travis-ci/worker/tree/e5cb567e10c0eefe380e81c9a2229ac8fb6a16ce
instance: 1464bfe travisci/ci-garnet:packer-1512502276-986baf0 (via amqp)
startup: 1.28474462s
travis_fold:end:worker_info[0Ktravis_fold:start:system_info[0K[33;1mBuild system information[0m
Build language: python
Build group: stable
Build dist: trusty
Build id: 418482132
Job id: 418482134
Runtime kernel version: 4.4.0-112-generic
travis-build version: ff4bc709c
[34m[1mBuild image provisioning date and time[0m
Tue Dec  5 20:11:19 UTC 2017
[34m[1mOperating System Details[0m
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.5 LTS
Release:	14.04
Codename:	trusty
[34m[1mCookbooks Version[0m
7c2c6a6 https://github.com/travis-ci/travis-cookbooks/tree/7c2c6a6
[34m[1mgit version[0m
git version 2.15.1
[34m[1mbash version[0m
GNU bash, version 4.3.11(1)-release (x86_64-pc-linux-gnu)
[34m[1mgcc version[0m
gcc (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

[34m[1mdocker version[0m
Client:
 Version:      17.09.0-ce
 API version:  1.32
 Go version:   go1.8.3
 Git commit:   afdb6d4
 Built:        Tue Sep 26 22:39:28 2017
 OS/Arch:      linux/amd64
[34m[1mclang version[0m
clang version 5.0.0 (tags/RELEASE_500/final)
Target: x86_64-unknown-linux-gnu
Thread model: posix
InstalledDir: /usr/local/clang-5.0.0/bin
[34m[1mjq version[0m
jq-1.5
[34m[1mbats version[0m
Bats 0.4.0
[34m[1mshellcheck version[0m
0.4.6
[34m[1mshfmt version[0m
v2.0.0
[34m[1mccache version[0m
ccache version 3.1.9

Copyright (C) 2002-2007 Andrew Tridgell
Copyright (C) 2009-2011 Joel Rosdahl

This program is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free Software
Foundation; either version 3 of the License, or (at your option) any later
version.
[34m[1mcmake version[0m
cmake version 3.9.2

CMake suite maintained and supported by Kitware (kitware.com/cmake).
[34m[1mheroku version[0m
heroku-cli/6.14.39-addc925 (linux-x64) node-v9.2.0
[34m[1mimagemagick version[0m
Version: ImageMagick 6.7.7-10 2017-07-31 Q16 http://www.imagemagick.org
[34m[1mmd5deep version[0m
4.2
[34m[1mmercurial version[0m
Mercurial Distributed SCM (version 4.2.2)
(see https://mercurial-scm.org for more information)

Copyright (C) 2005-2017 Matt Mackall and others
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
[34m[1mmysql version[0m
mysql  Ver 14.14 Distrib 5.6.33, for debian-linux-gnu (x86_64) using  EditLine wrapper
[34m[1mopenssl version[0m
OpenSSL 1.0.1f 6 Jan 2014
[34m[1mpacker version[0m
Packer v1.0.2

Your version of Packer is out of date! The latest version
is 1.1.2. You can update by downloading from www.packer.io
[34m[1mpostgresql client version[0m
psql (PostgreSQL) 9.6.6
[34m[1mragel version[0m
Ragel State Machine Compiler version 6.8 Feb 2013
Copyright (c) 2001-2009 by Adrian Thurston
[34m[1msubversion version[0m
svn, version 1.8.8 (r1568071)
   compiled Aug 10 2017, 17:20:39 on x86_64-pc-linux-gnu

Copyright (C) 2013 The Apache Software Foundation.
This software consists of contributions made by many people;
see the NOTICE file for more information.
Subversion is open source software, see http://subversion.apache.org/

The following repository access (RA) modules are available:

* ra_svn : Module for accessing a repository using the svn network protocol.
  - with Cyrus SASL authentication
  - handles 'svn' scheme
* ra_local : Module for accessing a repository on local disk.
  - handles 'file' scheme
* ra_serf : Module for accessing a repository via WebDAV protocol using serf.
  - using serf 1.3.3
  - handles 'http' scheme
  - handles 'https' scheme

[34m[1msudo version[0m
Sudo version 1.8.9p5
Configure options: --prefix=/usr -v --with-all-insults --with-pam --with-fqdn --with-logging=syslog --with-logfac=authpriv --with-env-editor --with-editor=/usr/bin/editor --with-timeout=15 --with-password-timeout=0 --with-passprompt=[sudo] password for %p:  --without-lecture --with-tty-tickets --disable-root-mailer --enable-admin-flag --with-sendmail=/usr/sbin/sendmail --with-timedir=/var/lib/sudo --mandir=/usr/share/man --libexecdir=/usr/lib/sudo --with-sssd --with-sssd-lib=/usr/lib/x86_64-linux-gnu --with-selinux
Sudoers policy plugin version 1.8.9p5
Sudoers file grammar version 43

Sudoers path: /etc/sudoers
Authentication methods: 'pam'
Syslog facility if syslog is being used for logging: authpriv
Syslog priority to use when user authenticates successfully: notice
Syslog priority to use when user authenticates unsuccessfully: alert
Send mail if the user is not in sudoers
Use a separate timestamp for each user/tty combo
Lecture user the first time they run sudo
Root may run sudo
Allow some information gathering to give useful error messages
Require fully-qualified hostnames in the sudoers file
Visudo will honor the EDITOR environment variable
Set the LOGNAME and USER environment variables
Length at which to wrap log file lines (0 for no wrap): 80
Authentication timestamp timeout: 15.0 minutes
Password prompt timeout: 0.0 minutes
Number of tries to enter a password: 3
Umask to use or 0777 to use user's: 022
Path to mail program: /usr/sbin/sendmail
Flags for mail program: -t
Address to send mail to: root
Subject line for mail messages: *** SECURITY information for %h ***
Incorrect password message: Sorry, try again.
Path to authentication timestamp dir: /var/lib/sudo
Default password prompt: [sudo] password for %p: 
Default user to run commands as: root
Value to override user's $PATH with: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin
Path to the editor for use by visudo: /usr/bin/editor
When to require a password for 'list' pseudocommand: any
When to require a password for 'verify' pseudocommand: all
File descriptors >= 3 will be closed before executing a command
Environment variables to check for sanity:
	TZ
	TERM
	LINGUAS
	LC_*
	LANGUAGE
	LANG
	COLORTERM
Environment variables to remove:
	RUBYOPT
	RUBYLIB
	PYTHONUSERBASE
	PYTHONINSPECT
	PYTHONPATH
	PYTHONHOME
	TMPPREFIX
	ZDOTDIR
	READNULLCMD
	NULLCMD
	FPATH
	PERL5DB
	PERL5OPT
	PERL5LIB
	PERLLIB
	PERLIO_DEBUG 
	JAVA_TOOL_OPTIONS
	SHELLOPTS
	GLOBIGNORE
	PS4
	BASH_ENV
	ENV
	TERMCAP
	TERMPATH
	TERMINFO_DIRS
	TERMINFO
	_RLD*
	LD_*
	PATH_LOCALE
	NLSPATH
	HOSTALIASES
	RES_OPTIONS
	LOCALDOMAIN
	CDPATH
	IFS
Environment variables to preserve:
	JAVA_HOME
	TRAVIS
	CI
	DEBIAN_FRONTEND
	XAUTHORIZATION
	XAUTHORITY
	PS2
	PS1
	PATH
	LS_COLORS
	KRB5CCNAME
	HOSTNAME
	HOME
	DISPLAY
	COLORS
Locale to use while parsing sudoers: C
Directory in which to store input/output logs: /var/log/sudo-io
File in which to store the input/output log: %{seq}
Add an entry to the utmp/utmpx file when allocating a pty
PAM service name to use
PAM service name to use for login shells
Create a new PAM session for the command to run in
Maximum I/O log sequence number: 0

Local IP address and netmask pairs:
	172.17.0.2/255.255.0.0

Sudoers I/O plugin version 1.8.9p5
[34m[1mgzip version[0m
gzip 1.6
Copyright (C) 2007, 2010, 2011 Free Software Foundation, Inc.
Copyright (C) 1993 Jean-loup Gailly.
This is free software.  You may redistribute copies of it under the terms of
the GNU General Public License <http://www.gnu.org/licenses/gpl.html>.
There is NO WARRANTY, to the extent permitted by law.

Written by Jean-loup Gailly.
[34m[1mzip version[0m
Copyright (c) 1990-2008 Info-ZIP - Type 'zip "-L"' for software license.
This is Zip 3.0 (July 5th 2008), by Info-ZIP.
Currently maintained by E. Gordon.  Please send bug reports to
the authors using the web page at www.info-zip.org; see README for details.

Latest sources and executables are at ftp://ftp.info-zip.org/pub/infozip,
as of above date; see http://www.info-zip.org/ for other sites.

Compiled with gcc 4.8.2 for Unix (Linux ELF) on Oct 21 2013.

Zip special compilation options:
	USE_EF_UT_TIME       (store Universal Time)
	BZIP2_SUPPORT        (bzip2 library version 1.0.6, 6-Sept-2010)
	    bzip2 code and library copyright (c) Julian R Seward
	    (See the bzip2 license for terms of use)
	SYMLINK_SUPPORT      (symbolic links supported)
	LARGE_FILE_SUPPORT   (can read and write large files on file system)
	ZIP64_SUPPORT        (use Zip64 to store large files in archives)
	UNICODE_SUPPORT      (store and read UTF-8 Unicode paths)
	STORE_UNIX_UIDs_GIDs (store UID/GID sizes/values using new extra field)
	UIDGID_NOT_16BIT     (old Unix 16-bit UID/GID extra field not used)
	[encryption, version 2.91 of 05 Jan 2007] (modified for Zip 3)

Encryption notice:
	The encryption code of this program is not copyrighted and is
	put in the public domain.  It was originally written in Europe
	and, to the best of our knowledge, can be freely distributed
	in both source and object forms from any country, including
	the USA under License Exception TSU of the U.S. Export
	Administration Regulations (section 740.13(e)) of 6 June 2002.

Zip environment options:
             ZIP:  [none]
          ZIPOPT:  [none]
[34m[1mvim version[0m
VIM - Vi IMproved 7.4 (2013 Aug 10, compiled Nov 24 2016 16:43:18)
Included patches: 1-52
Extra patches: 8.0.0056
Modified by pkg-vim-maintainers@lists.alioth.debian.org
Compiled by buildd@
Huge version without GUI.  Features included (+) or not (-):
+acl             +farsi           +mouse_netterm   +syntax
+arabic          +file_in_path    +mouse_sgr       +tag_binary
+autocmd         +find_in_path    -mouse_sysmouse  +tag_old_static
-balloon_eval    +float           +mouse_urxvt     -tag_any_white
-browse          +folding         +mouse_xterm     -tcl
++builtin_terms  -footer          +multi_byte      +terminfo
+byte_offset     +fork()          +multi_lang      +termresponse
+cindent         +gettext         -mzscheme        +textobjects
-clientserver    -hangul_input    +netbeans_intg   +title
-clipboard       +iconv           +path_extra      -toolbar
+cmdline_compl   +insert_expand   -perl            +user_commands
+cmdline_hist    +jumplist        +persistent_undo +vertsplit
+cmdline_info    +keymap          +postscript      +virtualedit
+comments        +langmap         +printer         +visual
+conceal         +libcall         +profile         +visualextra
+cryptv          +linebreak       +python          +viminfo
+cscope          +lispindent      -python3         +vreplace
+cursorbind      +listcmds        +quickfix        +wildignore
+cursorshape     +localmap        +reltime         +wildmenu
+dialog_con      -lua             +rightleft       +windows
+diff            +menu            -ruby            +writebackup
+digraphs        +mksession       +scrollbind      -X11
-dnd             +modify_fname    +signs           -xfontset
-ebcdic          +mouse           +smartindent     -xim
+emacs_tags      -mouseshape      -sniff           -xsmp
+eval            +mouse_dec       +startuptime     -xterm_clipboard
+ex_extra        +mouse_gpm       +statusline      -xterm_save
+extra_search    -mouse_jsbterm   -sun_workshop    -xpm
   system vimrc file: "$VIM/vimrc"
     user vimrc file: "$HOME/.vimrc"
 2nd user vimrc file: "~/.vim/vimrc"
      user exrc file: "$HOME/.exrc"
  fall-back for $VIM: "/usr/share/vim"
Compilation: gcc -c -I. -Iproto -DHAVE_CONFIG_H     -g -O2 -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -U_FORTIFY_SOURCE -D_FORTIFY_SOURCE=1      
Linking: gcc   -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,--as-needed -o vim        -lm -ltinfo -lnsl  -lselinux  -lacl -lattr -lgpm -ldl    -L/usr/lib/python2.7/config-x86_64-linux-gnu -lpython2.7 -lpthread -ldl -lutil -lm -Xlinker -export-dynamic -Wl,-O1 -Wl,-Bsymbolic-functions      
[34m[1miptables version[0m
iptables v1.4.21
[34m[1mcurl version[0m
curl 7.35.0 (x86_64-pc-linux-gnu) libcurl/7.35.0 OpenSSL/1.0.1f zlib/1.2.8 libidn/1.28 librtmp/2.3
[34m[1mwget version[0m
GNU Wget 1.15 built on linux-gnu.
[34m[1mrsync version[0m
rsync  version 3.1.0  protocol version 31
[34m[1mgimme version[0m
v1.2.0
[34m[1mnvm version[0m
0.33.6
[34m[1mperlbrew version[0m
/home/travis/perl5/perlbrew/bin/perlbrew  - App::perlbrew/0.80
[34m[1mphpenv version[0m
rbenv 1.1.1-25-g6aa70b6
[34m[1mrvm version[0m
rvm 1.29.3 (latest) by Michal Papis, Piotr Kuczynski, Wayne E. Seguin [https://rvm.io]
[34m[1mdefault ruby version[0m
ruby 2.4.1p111 (2017-03-22 revision 58053) [x86_64-linux]
[34m[1mCouchDB version[0m
couchdb 1.6.1
[34m[1mElasticSearch version[0m
5.5.0
[34m[1mInstalled Firefox version[0m
firefox 56.0.2
[34m[1mMongoDB version[0m
MongoDB 3.4.10
[34m[1mPhantomJS version[0m
2.1.1
[34m[1mPre-installed PostgreSQL versions[0m
9.2.24
9.3.20
9.4.15
9.5.10
9.6.6
[34m[1mRabbitMQ Version[0m
3.6.14
[34m[1mRedis version[0m
redis-server 4.0.6
[34m[1mriak version[0m
2.2.3
[34m[1mPre-installed Go versions[0m
1.7.4
[34m[1mant version[0m
Apache Ant(TM) version 1.9.3 compiled on April 8 2014
[34m[1mmvn version[0m
Apache Maven 3.5.2 (138edd61fd100ec658bfa2d307c43b76940a5d7d; 2017-10-18T07:58:13Z)
Maven home: /usr/local/maven-3.5.2
Java version: 1.8.0_151, vendor: Oracle Corporation
Java home: /usr/lib/jvm/java-8-oracle/jre
Default locale: en_US, platform encoding: UTF-8
OS name: "linux", version: "4.4.0-101-generic", arch: "amd64", family: "unix"
[34m[1mgradle version[0m

------------------------------------------------------------
Gradle 4.0.1
------------------------------------------------------------

Build time:   2017-07-07 14:02:41 UTC
Revision:     38e5dc0f772daecca1d2681885d3d85414eb6826

Groovy:       2.4.11
Ant:          Apache Ant(TM) version 1.9.6 compiled on June 29 2015
JVM:          1.8.0_151 (Oracle Corporation 25.151-b12)
OS:           Linux 4.4.0-101-generic amd64

[34m[1mlein version[0m
Leiningen 2.8.1 on Java 1.8.0_151 Java HotSpot(TM) 64-Bit Server VM
[34m[1mPre-installed Node.js versions[0m
v4.8.6
v6.12.0
v6.12.1
v8.9
v8.9.1
[34m[1mphpenv versions[0m
  system
  5.6
* 5.6.32 (set by /home/travis/.phpenv/version)
  7.0
  7.0.25
  7.1
  7.1.11
  hhvm
  hhvm-stable
[34m[1mcomposer --version[0m
Composer version 1.5.2 2017-09-11 16:59:25
[34m[1mPre-installed Ruby versions[0m
ruby-2.2.7
ruby-2.3.4
ruby-2.4.1
travis_fold:end:system_info[0K
[32;1mNetwork availability confirmed.[0m


travis_fold:start:git.checkout[0Ktravis_time:start:0febed80[0K$ git clone --depth=50 https://github.com/scikit-learn/scikit-learn.git scikit-learn/scikit-learn
Cloning into 'scikit-learn/scikit-learn'...
remote: Counting objects: 1765, done.[K
remote: Compressing objects:   0% (1/1489)   [Kremote: Compressing objects:   1% (15/1489)   [Kremote: Compressing objects:   2% (30/1489)   [Kremote: Compressing objects:   3% (45/1489)   [Kremote: Compressing objects:   4% (60/1489)   [Kremote: Compressing objects:   5% (75/1489)   [Kremote: Compressing objects:   6% (90/1489)   [Kremote: Compressing objects:   7% (105/1489)   [Kremote: Compressing objects:   8% (120/1489)   [Kremote: Compressing objects:   9% (135/1489)   [Kremote: Compressing objects:  10% (149/1489)   [Kremote: Compressing objects:  11% (164/1489)   [Kremote: Compressing objects:  12% (179/1489)   [Kremote: Compressing objects:  13% (194/1489)   [Kremote: Compressing objects:  14% (209/1489)   [Kremote: Compressing objects:  15% (224/1489)   [Kremote: Compressing objects:  16% (239/1489)   [Kremote: Compressing objects:  17% (254/1489)   [Kremote: Compressing objects:  18% (269/1489)   [Kremote: Compressing objects:  19% (283/1489)   [Kremote: Compressing objects:  20% (298/1489)   [Kremote: Compressing objects:  21% (313/1489)   [Kremote: Compressing objects:  22% (328/1489)   [Kremote: Compressing objects:  23% (343/1489)   [Kremote: Compressing objects:  24% (358/1489)   [Kremote: Compressing objects:  25% (373/1489)   [Kremote: Compressing objects:  26% (388/1489)   [Kremote: Compressing objects:  27% (403/1489)   [Kremote: Compressing objects:  28% (417/1489)   [Kremote: Compressing objects:  29% (432/1489)   [Kremote: Compressing objects:  30% (447/1489)   [Kremote: Compressing objects:  31% (462/1489)   [Kremote: Compressing objects:  32% (477/1489)   [Kremote: Compressing objects:  33% (492/1489)   [Kremote: Compressing objects:  34% (507/1489)   [Kremote: Compressing objects:  35% (522/1489)   [Kremote: Compressing objects:  36% (537/1489)   [Kremote: Compressing objects:  37% (551/1489)   [Kremote: Compressing objects:  38% (566/1489)   [Kremote: Compressing objects:  39% (581/1489)   [Kremote: Compressing objects:  40% (596/1489)   [Kremote: Compressing objects:  41% (611/1489)   [Kremote: Compressing objects:  42% (626/1489)   [Kremote: Compressing objects:  43% (641/1489)   [Kremote: Compressing objects:  44% (656/1489)   [Kremote: Compressing objects:  45% (671/1489)   [Kremote: Compressing objects:  46% (685/1489)   [Kremote: Compressing objects:  47% (700/1489)   [Kremote: Compressing objects:  48% (715/1489)   [Kremote: Compressing objects:  49% (730/1489)   [Kremote: Compressing objects:  50% (745/1489)   [Kremote: Compressing objects:  51% (760/1489)   [Kremote: Compressing objects:  52% (775/1489)   [Kremote: Compressing objects:  53% (790/1489)   [Kremote: Compressing objects:  54% (805/1489)   [Kremote: Compressing objects:  55% (819/1489)   [Kremote: Compressing objects:  56% (834/1489)   [Kremote: Compressing objects:  57% (849/1489)   [Kremote: Compressing objects:  58% (864/1489)   [Kremote: Compressing objects:  59% (879/1489)   [Kremote: Compressing objects:  60% (894/1489)   [Kremote: Compressing objects:  61% (909/1489)   [Kremote: Compressing objects:  62% (924/1489)   [Kremote: Compressing objects:  63% (939/1489)   [Kremote: Compressing objects:  64% (953/1489)   [Kremote: Compressing objects:  65% (968/1489)   [Kremote: Compressing objects:  66% (983/1489)   [Kremote: Compressing objects:  67% (998/1489)   [Kremote: Compressing objects:  68% (1013/1489)   [Kremote: Compressing objects:  69% (1028/1489)   [Kremote: Compressing objects:  70% (1043/1489)   [Kremote: Compressing objects:  71% (1058/1489)   [Kremote: Compressing objects:  72% (1073/1489)   [Kremote: Compressing objects:  73% (1087/1489)   [Kremote: Compressing objects:  74% (1102/1489)   [Kremote: Compressing objects:  75% (1117/1489)   [Kremote: Compressing objects:  76% (1132/1489)   [Kremote: Compressing objects:  77% (1147/1489)   [Kremote: Compressing objects:  78% (1162/1489)   [Kremote: Compressing objects:  79% (1177/1489)   [Kremote: Compressing objects:  80% (1192/1489)   [Kremote: Compressing objects:  81% (1207/1489)   [Kremote: Compressing objects:  82% (1221/1489)   [Kremote: Compressing objects:  83% (1236/1489)   [Kremote: Compressing objects:  84% (1251/1489)   [Kremote: Compressing objects:  85% (1266/1489)   [Kremote: Compressing objects:  86% (1281/1489)   [Kremote: Compressing objects:  87% (1296/1489)   [Kremote: Compressing objects:  88% (1311/1489)   [Kremote: Compressing objects:  89% (1326/1489)   [Kremote: Compressing objects:  90% (1341/1489)   [Kremote: Compressing objects:  91% (1355/1489)   [Kremote: Compressing objects:  92% (1370/1489)   [Kremote: Compressing objects:  93% (1385/1489)   [Kremote: Compressing objects:  94% (1400/1489)   [Kremote: Compressing objects:  95% (1415/1489)   [Kremote: Compressing objects:  96% (1430/1489)   [Kremote: Compressing objects:  97% (1445/1489)   [Kremote: Compressing objects:  98% (1460/1489)   [Kremote: Compressing objects:  99% (1475/1489)   [Kremote: Compressing objects: 100% (1489/1489)   [Kremote: Compressing objects: 100% (1489/1489), done.[K
Receiving objects:   0% (1/1765)   Receiving objects:   1% (18/1765)   Receiving objects:   2% (36/1765)   Receiving objects:   3% (53/1765)   Receiving objects:   4% (71/1765)   Receiving objects:   5% (89/1765)   Receiving objects:   6% (106/1765)   Receiving objects:   7% (124/1765)   Receiving objects:   8% (142/1765)   Receiving objects:   9% (159/1765)   Receiving objects:  10% (177/1765)   Receiving objects:  11% (195/1765)   Receiving objects:  12% (212/1765)   Receiving objects:  13% (230/1765)   Receiving objects:  14% (248/1765)   Receiving objects:  15% (265/1765)   Receiving objects:  16% (283/1765)   Receiving objects:  17% (301/1765)   Receiving objects:  18% (318/1765)   Receiving objects:  19% (336/1765)   Receiving objects:  20% (353/1765)   Receiving objects:  21% (371/1765)   Receiving objects:  22% (389/1765)   Receiving objects:  23% (406/1765)   Receiving objects:  24% (424/1765)   Receiving objects:  25% (442/1765)   Receiving objects:  26% (459/1765)   Receiving objects:  27% (477/1765)   Receiving objects:  28% (495/1765)   Receiving objects:  29% (512/1765)   Receiving objects:  30% (530/1765)   Receiving objects:  31% (548/1765)   Receiving objects:  32% (565/1765)   Receiving objects:  33% (583/1765)   Receiving objects:  34% (601/1765)   Receiving objects:  35% (618/1765)   Receiving objects:  36% (636/1765)   Receiving objects:  37% (654/1765)   Receiving objects:  38% (671/1765)   Receiving objects:  39% (689/1765)   Receiving objects:  40% (706/1765)   Receiving objects:  41% (724/1765)   Receiving objects:  42% (742/1765)   Receiving objects:  43% (759/1765)   Receiving objects:  44% (777/1765)   Receiving objects:  45% (795/1765)   Receiving objects:  46% (812/1765)   Receiving objects:  47% (830/1765)   Receiving objects:  48% (848/1765)   Receiving objects:  49% (865/1765)   Receiving objects:  50% (883/1765)   Receiving objects:  51% (901/1765)   Receiving objects:  52% (918/1765)   Receiving objects:  53% (936/1765)   Receiving objects:  54% (954/1765)   Receiving objects:  55% (971/1765)   Receiving objects:  56% (989/1765)   Receiving objects:  57% (1007/1765)   Receiving objects:  58% (1024/1765)   Receiving objects:  59% (1042/1765)   Receiving objects:  60% (1059/1765)   Receiving objects:  61% (1077/1765)   Receiving objects:  62% (1095/1765)   Receiving objects:  63% (1112/1765)   Receiving objects:  64% (1130/1765)   Receiving objects:  65% (1148/1765)   Receiving objects:  66% (1165/1765)   Receiving objects:  67% (1183/1765)   Receiving objects:  68% (1201/1765)   Receiving objects:  69% (1218/1765)   Receiving objects:  70% (1236/1765)   Receiving objects:  71% (1254/1765)   Receiving objects:  72% (1271/1765)   Receiving objects:  73% (1289/1765)   Receiving objects:  74% (1307/1765)   Receiving objects:  75% (1324/1765)   Receiving objects:  76% (1342/1765)   Receiving objects:  77% (1360/1765)   Receiving objects:  78% (1377/1765)   Receiving objects:  79% (1395/1765)   Receiving objects:  80% (1412/1765)   Receiving objects:  81% (1430/1765)   Receiving objects:  82% (1448/1765)   Receiving objects:  83% (1465/1765)   Receiving objects:  84% (1483/1765)   Receiving objects:  85% (1501/1765)   Receiving objects:  86% (1518/1765)   Receiving objects:  87% (1536/1765)   Receiving objects:  88% (1554/1765)   Receiving objects:  89% (1571/1765)   Receiving objects:  90% (1589/1765)   Receiving objects:  91% (1607/1765)   Receiving objects:  92% (1624/1765)   Receiving objects:  93% (1642/1765)   Receiving objects:  94% (1660/1765)   Receiving objects:  95% (1677/1765)   Receiving objects:  96% (1695/1765)   Receiving objects:  97% (1713/1765)   Receiving objects:  98% (1730/1765)   Receiving objects:  99% (1748/1765)   Receiving objects: 100% (1765/1765)   Receiving objects: 100% (1765/1765), 6.30 MiB | 16.83 MiB/s, done.
remote: Total 1765 (delta 445), reused 727 (delta 256), pack-reused 0[K
Resolving deltas:   0% (0/445)   Resolving deltas:  11% (49/445)   Resolving deltas:  12% (54/445)   Resolving deltas:  16% (73/445)   Resolving deltas:  17% (76/445)   Resolving deltas:  18% (81/445)   Resolving deltas:  20% (90/445)   Resolving deltas:  21% (94/445)   Resolving deltas:  22% (102/445)   Resolving deltas:  23% (103/445)   Resolving deltas:  24% (107/445)   Resolving deltas:  25% (112/445)   Resolving deltas:  28% (126/445)   Resolving deltas:  30% (134/445)   Resolving deltas:  31% (138/445)   Resolving deltas:  32% (143/445)   Resolving deltas:  33% (147/445)   Resolving deltas:  34% (152/445)   Resolving deltas:  35% (156/445)   Resolving deltas:  36% (161/445)   Resolving deltas:  37% (165/445)   Resolving deltas:  38% (171/445)   Resolving deltas:  39% (174/445)   Resolving deltas:  40% (178/445)   Resolving deltas:  41% (183/445)   Resolving deltas:  48% (218/445)   Resolving deltas:  49% (219/445)   Resolving deltas:  50% (223/445)   Resolving deltas:  52% (234/445)   Resolving deltas:  53% (236/445)   Resolving deltas:  54% (241/445)   Resolving deltas:  55% (246/445)   Resolving deltas:  56% (250/445)   Resolving deltas:  57% (254/445)   Resolving deltas:  58% (259/445)   Resolving deltas:  59% (263/445)   Resolving deltas:  60% (270/445)   Resolving deltas:  61% (274/445)   Resolving deltas:  62% (276/445)   Resolving deltas:  63% (281/445)   Resolving deltas:  64% (285/445)   Resolving deltas:  65% (292/445)   Resolving deltas:  66% (294/445)   Resolving deltas:  67% (299/445)   Resolving deltas:  68% (304/445)   Resolving deltas:  69% (309/445)   Resolving deltas:  70% (312/445)   Resolving deltas:  71% (316/445)   Resolving deltas:  72% (321/445)   Resolving deltas:  73% (325/445)   Resolving deltas:  74% (330/445)   Resolving deltas:  75% (334/445)   Resolving deltas:  76% (340/445)   Resolving deltas:  77% (343/445)   Resolving deltas:  78% (348/445)   Resolving deltas:  80% (357/445)   Resolving deltas:  81% (361/445)   Resolving deltas:  82% (365/445)   Resolving deltas:  83% (370/445)   Resolving deltas:  84% (374/445)   Resolving deltas:  85% (379/445)   Resolving deltas:  86% (384/445)   Resolving deltas:  88% (392/445)   Resolving deltas:  90% (402/445)   Resolving deltas:  91% (408/445)   Resolving deltas:  92% (410/445)   Resolving deltas:  93% (414/445)   Resolving deltas:  94% (421/445)   Resolving deltas:  96% (428/445)   Resolving deltas:  98% (437/445)   Resolving deltas:  99% (443/445)   Resolving deltas: 100% (445/445)   Resolving deltas: 100% (445/445), done.

travis_time:end:0febed80:start=1534811745358510434,finish=1534811748058206807,duration=2699696373[0K$ cd scikit-learn/scikit-learn
travis_time:start:1156c596[0K$ git fetch origin +refs/pull/11870/merge:
remote: Counting objects: 147899, done.[K
remote: Compressing objects:   0% (1/36428)   [Kremote: Compressing objects:   1% (365/36428)   [Kremote: Compressing objects:   2% (729/36428)   [Kremote: Compressing objects:   3% (1093/36428)   [Kremote: Compressing objects:   4% (1458/36428)   [Kremote: Compressing objects:   5% (1822/36428)   [Kremote: Compressing objects:   6% (2186/36428)   [Kremote: Compressing objects:   7% (2550/36428)   [Kremote: Compressing objects:   8% (2915/36428)   [Kremote: Compressing objects:   9% (3279/36428)   [Kremote: Compressing objects:  10% (3643/36428)   [Kremote: Compressing objects:  11% (4008/36428)   [Kremote: Compressing objects:  12% (4372/36428)   [Kremote: Compressing objects:  13% (4736/36428)   [Kremote: Compressing objects:  14% (5100/36428)   [Kremote: Compressing objects:  15% (5465/36428)   [Kremote: Compressing objects:  16% (5829/36428)   [Kremote: Compressing objects:  17% (6193/36428)   [Kremote: Compressing objects:  18% (6558/36428)   [Kremote: Compressing objects:  19% (6922/36428)   [Kremote: Compressing objects:  20% (7286/36428)   [Kremote: Compressing objects:  21% (7650/36428)   [Kremote: Compressing objects:  22% (8015/36428)   [Kremote: Compressing objects:  23% (8379/36428)   [Kremote: Compressing objects:  24% (8743/36428)   [Kremote: Compressing objects:  25% (9107/36428)   [Kremote: Compressing objects:  26% (9472/36428)   [Kremote: Compressing objects:  27% (9836/36428)   [Kremote: Compressing objects:  28% (10200/36428)   [Kremote: Compressing objects:  29% (10565/36428)   [Kremote: Compressing objects:  30% (10929/36428)   [Kremote: Compressing objects:  31% (11293/36428)   [Kremote: Compressing objects:  32% (11657/36428)   [Kremote: Compressing objects:  33% (12022/36428)   [Kremote: Compressing objects:  34% (12386/36428)   [Kremote: Compressing objects:  35% (12750/36428)   [Kremote: Compressing objects:  36% (13115/36428)   [Kremote: Compressing objects:  37% (13479/36428)   [Kremote: Compressing objects:  38% (13843/36428)   [Kremote: Compressing objects:  39% (14207/36428)   [Kremote: Compressing objects:  40% (14572/36428)   [Kremote: Compressing objects:  41% (14936/36428)   [Kremote: Compressing objects:  42% (15300/36428)   [Kremote: Compressing objects:  43% (15665/36428)   [Kremote: Compressing objects:  44% (16029/36428)   [Kremote: Compressing objects:  45% (16393/36428)   [Kremote: Compressing objects:  46% (16757/36428)   [Kremote: Compressing objects:  47% (17122/36428)   [Kremote: Compressing objects:  48% (17486/36428)   [Kremote: Compressing objects:  49% (17850/36428)   [Kremote: Compressing objects:  50% (18214/36428)   [Kremote: Compressing objects:  51% (18579/36428)   [Kremote: Compressing objects:  52% (18943/36428)   [Kremote: Compressing objects:  53% (19307/36428)   [Kremote: Compressing objects:  54% (19672/36428)   [Kremote: Compressing objects:  55% (20036/36428)   [Kremote: Compressing objects:  56% (20400/36428)   [Kremote: Compressing objects:  57% (20764/36428)   [Kremote: Compressing objects:  58% (21129/36428)   [Kremote: Compressing objects:  59% (21493/36428)   [Kremote: Compressing objects:  60% (21857/36428)   [Kremote: Compressing objects:  61% (22222/36428)   [Kremote: Compressing objects:  62% (22586/36428)   [Kremote: Compressing objects:  63% (22950/36428)   [Kremote: Compressing objects:  64% (23314/36428)   [Kremote: Compressing objects:  65% (23679/36428)   [Kremote: Compressing objects:  66% (24043/36428)   [Kremote: Compressing objects:  67% (24407/36428)   [Kremote: Compressing objects:  68% (24772/36428)   [Kremote: Compressing objects:  69% (25136/36428)   [Kremote: Compressing objects:  70% (25500/36428)   [Kremote: Compressing objects:  71% (25864/36428)   [Kremote: Compressing objects:  72% (26229/36428)   [Kremote: Compressing objects:  73% (26593/36428)   [Kremote: Compressing objects:  74% (26957/36428)   [Kremote: Compressing objects:  75% (27321/36428)   [Kremote: Compressing objects:  76% (27686/36428)   [Kremote: Compressing objects:  77% (28050/36428)   [Kremote: Compressing objects:  78% (28414/36428)   [Kremote: Compressing objects:  79% (28779/36428)   [Kremote: Compressing objects:  80% (29143/36428)   [Kremote: Compressing objects:  81% (29507/36428)   [Kremote: Compressing objects:  82% (29871/36428)   [Kremote: Compressing objects:  83% (30236/36428)   [Kremote: Compressing objects:  84% (30600/36428)   [Kremote: Compressing objects:  85% (30964/36428)   [Kremote: Compressing objects:  86% (31329/36428)   [Kremote: Compressing objects:  87% (31693/36428)   [Kremote: Compressing objects:  88% (32057/36428)   [Kremote: Compressing objects:  89% (32421/36428)   [Kremote: Compressing objects:  90% (32786/36428)   [Kremote: Compressing objects:  91% (33150/36428)   [Kremote: Compressing objects:  92% (33514/36428)   [Kremote: Compressing objects:  93% (33879/36428)   [Kremote: Compressing objects:  94% (34243/36428)   [Kremote: Compressing objects:  95% (34607/36428)   [Kremote: Compressing objects:  96% (34971/36428)   [Kremote: Compressing objects:  97% (35336/36428)   [Kremote: Compressing objects:  98% (35700/36428)   [Kremote: Compressing objects:  99% (36064/36428)   [Kremote: Compressing objects: 100% (36428/36428)   [Kremote: Compressing objects: 100% (36428/36428), done.[K
Receiving objects:   0% (1/147899)   Receiving objects:   1% (1479/147899)   Receiving objects:   2% (2958/147899)   Receiving objects:   3% (4437/147899)   Receiving objects:   4% (5916/147899)   Receiving objects:   5% (7395/147899)   Receiving objects:   6% (8874/147899)   Receiving objects:   7% (10353/147899)   Receiving objects:   8% (11832/147899)   Receiving objects:   9% (13311/147899)   Receiving objects:  10% (14790/147899)   Receiving objects:  11% (16269/147899)   Receiving objects:  12% (17748/147899)   Receiving objects:  13% (19227/147899)   Receiving objects:  14% (20706/147899)   Receiving objects:  15% (22185/147899)   Receiving objects:  16% (23664/147899)   Receiving objects:  17% (25143/147899)   Receiving objects:  18% (26622/147899)   Receiving objects:  19% (28101/147899)   Receiving objects:  20% (29580/147899)   Receiving objects:  21% (31059/147899)   Receiving objects:  22% (32538/147899)   Receiving objects:  23% (34017/147899)   Receiving objects:  24% (35496/147899)   Receiving objects:  25% (36975/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  26% (38454/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  27% (39933/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  28% (41412/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  29% (42891/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  30% (44370/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  31% (45849/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  32% (47328/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  33% (48807/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  33% (48907/147899), 8.39 MiB | 16.78 MiB/s   Receiving objects:  34% (50286/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  35% (51765/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  36% (53244/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  37% (54723/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  38% (56202/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  39% (57681/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  40% (59160/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  41% (60639/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  42% (62118/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  43% (63597/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  44% (65076/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  45% (66555/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  46% (68034/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  47% (69513/147899), 19.93 MiB | 19.92 MiB/s   Receiving objects:  48% (70992/147899), 31.88 MiB | 21.25 MiB/s   Receiving objects:  49% (72471/147899), 31.88 MiB | 21.25 MiB/s   Receiving objects:  50% (73950/147899), 31.88 MiB | 21.25 MiB/s   Receiving objects:  50% (74580/147899), 44.45 MiB | 22.22 MiB/s   Receiving objects:  51% (75429/147899), 44.45 MiB | 22.22 MiB/s   Receiving objects:  52% (76908/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  53% (78387/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  54% (79866/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  55% (81345/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  56% (82824/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  57% (84303/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  58% (85782/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  59% (87261/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  60% (88740/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  61% (90219/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  62% (91698/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  63% (93177/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  64% (94656/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  65% (96135/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  66% (97614/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  67% (99093/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  68% (100572/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  69% (102051/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  70% (103530/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  71% (105009/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  72% (106488/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  73% (107967/147899), 56.41 MiB | 22.56 MiB/s   Receiving objects:  74% (109446/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  75% (110925/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  76% (112404/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  77% (113883/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  78% (115362/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  79% (116841/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  80% (118320/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  81% (119799/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  82% (121278/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  83% (122757/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  84% (124236/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  85% (125715/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  86% (127194/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  87% (128673/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  88% (130152/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  89% (131631/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  90% (133110/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  91% (134589/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  92% (136068/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  93% (137547/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  94% (139026/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  95% (140505/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  96% (141984/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  97% (143463/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  98% (144942/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects:  99% (146421/147899), 68.36 MiB | 22.78 MiB/s   remote: Total 147899 (delta 112405), reused 146218 (delta 110732), pack-reused 0[K
Receiving objects: 100% (147899/147899), 68.36 MiB | 22.78 MiB/s   Receiving objects: 100% (147899/147899), 77.97 MiB | 23.14 MiB/s, done.
Resolving deltas:   0% (0/112405)   Resolving deltas:   1% (1130/112405)   Resolving deltas:   2% (2261/112405)   Resolving deltas:   3% (3379/112405)   Resolving deltas:   4% (4502/112405)   Resolving deltas:   5% (5629/112405)   Resolving deltas:   6% (6755/112405)   Resolving deltas:   7% (7961/112405)   Resolving deltas:   8% (8995/112405)   Resolving deltas:   9% (10226/112405)   Resolving deltas:  10% (11253/112405)   Resolving deltas:  11% (12379/112405)   Resolving deltas:  12% (13607/112405)   Resolving deltas:  13% (14614/112405)   Resolving deltas:  14% (16298/112405)   Resolving deltas:  15% (16877/112405)   Resolving deltas:  16% (18002/112405)   Resolving deltas:  17% (19211/112405)   Resolving deltas:  18% (20243/112405)   Resolving deltas:  19% (21722/112405)   Resolving deltas:  20% (22487/112405)   Resolving deltas:  21% (23631/112405)   Resolving deltas:  22% (24768/112405)   Resolving deltas:  23% (25859/112405)   Resolving deltas:  24% (26985/112405)   Resolving deltas:  25% (28177/112405)   Resolving deltas:  26% (29340/112405)   Resolving deltas:  27% (30367/112405)   Resolving deltas:  28% (31483/112405)   Resolving deltas:  29% (32643/112405)   Resolving deltas:  30% (33742/112405)   Resolving deltas:  31% (34932/112405)   Resolving deltas:  32% (36122/112405)   Resolving deltas:  33% (37192/112405)   Resolving deltas:  34% (38245/112405)   Resolving deltas:  35% (39507/112405)   Resolving deltas:  36% (40573/112405)   Resolving deltas:  37% (41627/112405)   Resolving deltas:  38% (42793/112405)   Resolving deltas:  39% (44639/112405)   Resolving deltas:  40% (44964/112405)   Resolving deltas:  41% (46088/112405)   Resolving deltas:  42% (47264/112405)   Resolving deltas:  43% (48395/112405)   Resolving deltas:  44% (49593/112405)   Resolving deltas:  45% (50663/112405)   Resolving deltas:  46% (52413/112405)   Resolving deltas:  47% (52877/112405)   Resolving deltas:  48% (54068/112405)   Resolving deltas:  49% (55296/112405)   Resolving deltas:  50% (56206/112405)   Resolving deltas:  51% (57330/112405)   Resolving deltas:  52% (58452/112405)   Resolving deltas:  53% (59773/112405)   Resolving deltas:  54% (60700/112405)   Resolving deltas:  55% (62235/112405)   Resolving deltas:  56% (63126/112405)   Resolving deltas:  57% (64073/112405)   Resolving deltas:  58% (65195/112405)   Resolving deltas:  59% (66374/112405)   Resolving deltas:  60% (67446/112405)   Resolving deltas:  61% (68577/112405)   Resolving deltas:  62% (69714/112405)   Resolving deltas:  63% (70839/112405)   Resolving deltas:  64% (72125/112405)   Resolving deltas:  65% (73067/112405)   Resolving deltas:  66% (74202/112405)   Resolving deltas:  67% (75325/112405)   Resolving deltas:  68% (76543/112405)   Resolving deltas:  69% (77823/112405)   Resolving deltas:  69% (78438/112405)   Resolving deltas:  70% (78781/112405)   Resolving deltas:  71% (79875/112405)   Resolving deltas:  72% (81025/112405)   Resolving deltas:  73% (82126/112405)   Resolving deltas:  74% (83277/112405)   Resolving deltas:  74% (83842/112405)   Resolving deltas:  75% (84311/112405)   Resolving deltas:  76% (85520/112405)   Resolving deltas:  77% (86560/112405)   Resolving deltas:  78% (87679/112405)   Resolving deltas:  79% (88807/112405)   Resolving deltas:  80% (89928/112405)   Resolving deltas:  81% (91049/112405)   Resolving deltas:  82% (92286/112405)   Resolving deltas:  83% (93345/112405)   Resolving deltas:  84% (94426/112405)   Resolving deltas:  85% (95582/112405)   Resolving deltas:  86% (96720/112405)   Resolving deltas:  87% (97886/112405)   Resolving deltas:  88% (98918/112405)   Resolving deltas:  89% (100075/112405)   Resolving deltas:  90% (101366/112405)   Resolving deltas:  91% (102359/112405)   Resolving deltas:  92% (103451/112405)   Resolving deltas:  93% (104551/112405)   Resolving deltas:  94% (105664/112405)   Resolving deltas:  95% (106874/112405)   Resolving deltas:  96% (107979/112405)   Resolving deltas:  97% (109060/112405)   Resolving deltas:  98% (110183/112405)   Resolving deltas:  99% (111363/112405)   Resolving deltas: 100% (112405/112405)   Resolving deltas: 100% (112405/112405), completed with 754 local objects.
From https://github.com/scikit-learn/scikit-learn
 * branch                refs/pull/11870/merge -> FETCH_HEAD

travis_time:end:1156c596:start=1534811748063570112,finish=1534811758780056429,duration=10716486317[0K$ git checkout -qf FETCH_HEAD
travis_fold:end:git.checkout[0K
[33;1mSetting environment variables from .travis.yml[0m
$ export TEST_DIR=/tmp/sklearn
$ export OMP_NUM_THREADS=4
$ export OPENBLAS_NUM_THREADS=4
$ export DISTRIB="conda"
$ export PYTHON_VERSION="3.4"
$ export INSTALL_MKL="false"
$ export NUMPY_VERSION="1.10.4"
$ export SCIPY_VERSION="0.16.1"
$ export CYTHON_VERSION="0.25.2"
$ export PILLOW_VERSION="4.0.0"
$ export COVERAGE=true

travis_time:start:139a8cd1[0K$ source ~/virtualenv/python2.7/bin/activate

travis_time:end:139a8cd1:start=1534811758966698233,finish=1534811758972914494,duration=6216261[0Ktravis_fold:start:cache.1[0KSetting up build cache
$ export CASHER_DIR=$HOME/.casher
travis_time:start:020b75e6[0K$ Installing caching utilities

travis_time:end:020b75e6:start=1534811763327117470,finish=1534811763382061556,duration=54944086[0Ktravis_time:start:0df3a9cc[0K
travis_time:end:0df3a9cc:start=1534811763389202100,finish=1534811763394945726,duration=5743626[0Ktravis_time:start:2065c625[0K[32;1mattempting to download cache archive[0m
[32;1mfetching PR.11870/cache-linux-trusty-b3e938f5828910ac816b7eacbfd746a62df2020465fdab71d64ed17fbb393e30--python-2.7.tgz[0m
[32;1mfetching PR.11870/cache--python-2.7.tgz[0m
[32;1mfetching master/cache-linux-trusty-b3e938f5828910ac816b7eacbfd746a62df2020465fdab71d64ed17fbb393e30--python-2.7.tgz[0m
[32;1mfound cache[0m

travis_time:end:2065c625:start=1534811763401663939,finish=1534811769722798881,duration=6321134942[0Ktravis_time:start:0cc6bec8[0K
travis_time:end:0cc6bec8:start=1534811769728175581,finish=1534811769732512982,duration=4337401[0Ktravis_time:start:0785ac7e[0K[32;1madding /home/travis/.cache/pip to cache[0m
[32;1madding /home/travis/.ccache to cache[0m

travis_time:end:0785ac7e:start=1534811769737694963,finish=1534811772311913966,duration=2574219003[0Ktravis_fold:end:cache.1[0K$ python --version
Python 2.7.14
$ pip --version
pip 9.0.1 from /home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages (python 2.7)
travis_fold:start:install[0Ktravis_time:start:1c12e68b[0K$ source build_tools/travis/install.sh
List files from cached directories
pip:
http  wheels
Set cache size limit to 100.0 Mbytes
cache directory                     /home/travis/.ccache
cache hit (direct)                  8324
cache hit (preprocessed)            3770
cache miss                         11313
called for link                     3619
files in cache                       107
cache size                          47.0 Mbytes
max cache size                     100.0 Mbytes
--2018-08-21 00:36:13--  https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
Resolving repo.continuum.io (repo.continuum.io)... 104.16.19.10, 104.16.18.10, 2400:cb00:2048:1::6810:120a, ...
Connecting to repo.continuum.io (repo.continuum.io)|104.16.19.10|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 58468498 (56M) [application/x-sh]
Saving to: ‘miniconda.sh’

 0% [                                       ] 0           --.-K/s              26% [=========>                             ] 15,325,994  73.1MB/s             54% [====================>                  ] 32,054,058  75.0MB/s             77% [=============================>         ] 45,046,570  70.7MB/s             100%[======================================>] 58,468,498  72.7MB/s   in 0.8s   

2018-08-21 00:36:14 (72.7 MB/s) - ‘miniconda.sh’ saved [58468498/58468498]

PREFIX=/home/travis/miniconda
installing: python-3.6.5-hc3d631a_2 ...
Python 3.6.5 :: Anaconda, Inc.
installing: ca-certificates-2018.03.07-0 ...
installing: conda-env-2.6.0-h36134e3_1 ...
installing: libgcc-ng-7.2.0-hdf63c60_3 ...
installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...
installing: libffi-3.2.1-hd88cf55_4 ...
installing: ncurses-6.1-hf484d3e_0 ...
installing: openssl-1.0.2o-h20670df_0 ...
installing: tk-8.6.7-hc745277_3 ...
installing: xz-5.2.4-h14c3975_4 ...
installing: yaml-0.1.7-had09818_2 ...
installing: zlib-1.2.11-ha838bed_2 ...
installing: libedit-3.1.20170329-h6b74fdf_2 ...
installing: readline-7.0-ha6073c6_4 ...
installing: sqlite-3.23.1-he433501_0 ...
installing: asn1crypto-0.24.0-py36_0 ...
installing: certifi-2018.4.16-py36_0 ...
installing: chardet-3.0.4-py36h0f667ec_1 ...
installing: idna-2.6-py36h82fb2a8_1 ...
installing: pycosat-0.6.3-py36h0a5515d_0 ...
installing: pycparser-2.18-py36hf9f622e_1 ...
installing: pysocks-1.6.8-py36_0 ...
installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...
installing: six-1.11.0-py36h372c433_1 ...
installing: cffi-1.11.5-py36h9745a5d_0 ...
installing: setuptools-39.2.0-py36_0 ...
installing: cryptography-2.2.2-py36h14c3975_0 ...
installing: wheel-0.31.1-py36_0 ...
installing: pip-10.0.1-py36_0 ...
installing: pyopenssl-18.0.0-py36_0 ...
installing: urllib3-1.22-py36hbe7ace6_0 ...
installing: requests-2.18.4-py36he2e5f8d_1 ...
installing: conda-4.5.4-py36_0 ...
installation finished.
Solving environment: - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / done

## Package Plan ##

  environment location: /home/travis/miniconda

  added / updated specs: 
    - conda


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    conda-4.5.10               |           py36_0         1.0 MB
    openssl-1.0.2p             |       h14c3975_0         3.5 MB
    certifi-2018.8.13          |           py36_0         138 KB
    ------------------------------------------------------------
                                           Total:         4.6 MB

The following packages will be UPDATED:

    certifi: 2018.4.16-py36_0  --> 2018.8.13-py36_0 
    conda:   4.5.4-py36_0      --> 4.5.10-py36_0    
    openssl: 1.0.2o-h20670df_0 --> 1.0.2p-h14c3975_0


Downloading and Extracting Packages
conda-4.5.10         |  1.0 MB | :   0% 0/1 [00:00<?, ?it/s]conda-4.5.10         |  1.0 MB | :  80% 0.8001111426898978/1 [00:00<00:00,  5.82it/s]conda-4.5.10         |  1.0 MB | :  98% 0.97550014210454/1 [00:00<00:00,  2.35it/s]  conda-4.5.10         |  1.0 MB | : 100% 1.0/1 [00:00<00:00,  2.75it/s]             
openssl-1.0.2p       |  3.5 MB | :   0% 0/1 [00:00<?, ?it/s]openssl-1.0.2p       |  3.5 MB | :  76% 0.7567906892179296/1 [00:00<00:00,  5.99it/s]openssl-1.0.2p       |  3.5 MB | :  94% 0.936743953493063/1 [00:00<00:00,  1.22s/it] openssl-1.0.2p       |  3.5 MB | : 100% 1.0/1 [00:00<00:00,  1.05it/s]              
certifi-2018.8.13    |  138 KB | :   0% 0/1 [00:00<?, ?it/s]certifi-2018.8.13    |  138 KB | : 100% 1.0/1 [00:00<00:00, 29.15it/s]
Preparing transaction: \ done
Verifying transaction: / - \ | done
Executing transaction: - \ | done
Solving environment: - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - \ | / - done

## Package Plan ##

  environment location: /home/travis/miniconda/envs/testenv

  added / updated specs: 
    - cython=0.25.2
    - nomkl
    - numpy=1.10.4
    - pillow=4.0.0
    - pip
    - pytest
    - pytest-cov
    - python=3.4
    - scipy=0.16.1


The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    wheel-0.29.0               |           py34_0          82 KB
    py-1.4.32                  |           py34_0         135 KB
    setuptools-27.2.0          |           py34_0         528 KB
    blas-1.0                   |         openblas          48 KB
    nomkl-3.0                  |                0          48 KB
    libtiff-4.0.9              |       he85c1e1_1         566 KB
    freetype-2.5.5             |                2         2.5 MB
    libgfortran-1.0            |                0         170 KB
    libpng-1.6.34              |       hb9fc6fc_0         334 KB
    coverage-4.2               |           py34_0         213 KB
    scipy-0.16.1               |np110py34_nomkl_1        27.8 MB
    pytest-cov-2.3.1           |           py34_0          19 KB
    readline-6.2               |                2         606 KB
    pytest-3.0.5               |           py34_0         266 KB
    libgcc-ng-8.2.0            |       hdf63c60_1         7.6 MB
    openblas-0.2.14            |                3         3.5 MB
    pillow-4.0.0               |           py34_0         864 KB
    cython-0.25.2              |           py34_0         6.8 MB
    sqlite-3.13.0              |                0         4.0 MB
    jpeg-9b                    |       h024ee3a_2         248 KB
    tk-8.5.18                  |                0         1.9 MB
    pip-9.0.1                  |           py34_1         1.7 MB
    libstdcxx-ng-8.2.0         |       hdf63c60_1         2.9 MB
    numpy-1.10.4               |     py34_nomkl_0         5.9 MB
    python-3.4.5               |                0        15.2 MB
    ------------------------------------------------------------
                                           Total:        83.9 MB

The following NEW packages will be INSTALLED:

    blas:            1.0-openblas            
    ca-certificates: 2018.03.07-0            
    coverage:        4.2-py34_0              
    cython:          0.25.2-py34_0           
    freetype:        2.5.5-2                 
    jpeg:            9b-h024ee3a_2           
    libgcc-ng:       8.2.0-hdf63c60_1        
    libgfortran:     1.0-0                   
    libpng:          1.6.34-hb9fc6fc_0       
    libstdcxx-ng:    8.2.0-hdf63c60_1        
    libtiff:         4.0.9-he85c1e1_1        
    nomkl:           3.0-0                   
    numpy:           1.10.4-py34_nomkl_0      [nomkl]
    openblas:        0.2.14-3                
    openssl:         1.0.2p-h14c3975_0       
    pillow:          4.0.0-py34_0            
    pip:             9.0.1-py34_1            
    py:              1.4.32-py34_0           
    pytest:          3.0.5-py34_0            
    pytest-cov:      2.3.1-py34_0            
    python:          3.4.5-0                 
    readline:        6.2-2                   
    scipy:           0.16.1-np110py34_nomkl_1 [nomkl]
    setuptools:      27.2.0-py34_0           
    sqlite:          3.13.0-0                
    tk:              8.5.18-0                
    wheel:           0.29.0-py34_0           
    xz:              5.2.4-h14c3975_4        
    zlib:            1.2.11-ha838bed_2       


Downloading and Extracting Packages
wheel-0.29.0         | 82 KB     | :   0% 0/1 [00:00<?, ?it/s]wheel-0.29.0         | 82 KB     | :  15% 0.14618303810418873/1 [00:00<00:00,  1.37it/s]wheel-0.29.0         | 82 KB     | : 100% 1.0/1 [00:00<00:00,  7.10it/s]                
py-1.4.32            | 135 KB    | :   0% 0/1 [00:00<?, ?it/s]py-1.4.32            | 135 KB    | : 100% 1.0/1 [00:00<00:00, 15.13it/s]
setuptools-27.2.0    | 528 KB    | :   0% 0/1 [00:00<?, ?it/s]setuptools-27.2.0    | 528 KB    | :   2% 0.022748394024103523/1 [00:00<00:04,  5.11s/it]setuptools-27.2.0    | 528 KB    | : 100% 1.0/1 [00:00<00:00,  5.88it/s]                 
blas-1.0             | 48 KB     | :   0% 0/1 [00:00<?, ?it/s]blas-1.0             | 48 KB     | : 100% 1.0/1 [00:00<00:00, 29.01it/s]
nomkl-3.0            | 48 KB     | :   0% 0/1 [00:00<?, ?it/s]nomkl-3.0            | 48 KB     | : 100% 1.0/1 [00:00<00:00, 33.35it/s]
libtiff-4.0.9        | 566 KB    | :   0% 0/1 [00:00<?, ?it/s]libtiff-4.0.9        | 566 KB    | :  93% 0.9301899405680966/1 [00:00<00:00,  9.27it/s]libtiff-4.0.9        | 566 KB    | : 100% 1.0/1 [00:00<00:00,  2.69it/s]               
freetype-2.5.5       | 2.5 MB    | :   0% 0/1 [00:00<?, ?it/s]freetype-2.5.5       | 2.5 MB    | :  77% 0.7692386817745429/1 [00:00<00:00,  7.61it/s]freetype-2.5.5       | 2.5 MB    | : 100% 0.9977942212561131/1 [00:00<00:00,  1.68it/s]freetype-2.5.5       | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  2.03it/s]               
libgfortran-1.0      | 170 KB    | :   0% 0/1 [00:00<?, ?it/s]libgfortran-1.0      | 170 KB    | : 100% 1.0/1 [00:00<00:00, 15.61it/s]
libpng-1.6.34        | 334 KB    | :   0% 0/1 [00:00<?, ?it/s]libpng-1.6.34        | 334 KB    | :  95% 0.9537763236447493/1 [00:00<00:00,  7.72it/s]libpng-1.6.34        | 334 KB    | : 100% 1.0/1 [00:00<00:00,  7.35it/s]               
coverage-4.2         | 213 KB    | :   0% 0/1 [00:00<?, ?it/s]coverage-4.2         | 213 KB    | : 100% 1.0/1 [00:00<00:00, 11.42it/s]
scipy-0.16.1         | 27.8 MB   | :   0% 0/1 [00:00<?, ?it/s]scipy-0.16.1         | 27.8 MB   | :  28% 0.27984026935283446/1 [00:00<00:00,  2.80it/s]scipy-0.16.1         | 27.8 MB   | :  49% 0.4889840496060054/1 [00:00<00:00,  2.54it/s] scipy-0.16.1         | 27.8 MB   | :  75% 0.7517533851463211/1 [00:00<00:00,  2.56it/s]scipy-0.16.1         | 27.8 MB   | :  92% 0.9153792870009985/1 [00:03<00:00,  6.11s/it]scipy-0.16.1         | 27.8 MB   | : 100% 1.0/1 [00:05<00:00,  5.21s/it]               
pytest-cov-2.3.1     | 19 KB     | :   0% 0/1 [00:00<?, ?it/s]pytest-cov-2.3.1     | 19 KB     | : 100% 1.0/1 [00:00<00:00, 33.45it/s]
readline-6.2         | 606 KB    | :   0% 0/1 [00:00<?, ?it/s]readline-6.2         | 606 KB    | :  91% 0.9117818704739058/1 [00:00<00:00,  9.05it/s]readline-6.2         | 606 KB    | : 100% 1.0/1 [00:00<00:00,  6.65it/s]               
pytest-3.0.5         | 266 KB    | :   0% 0/1 [00:00<?, ?it/s]pytest-3.0.5         | 266 KB    | : 100% 1.0/1 [00:00<00:00, 10.75it/s]
libgcc-ng-8.2.0      | 7.6 MB    | :   0% 0/1 [00:00<?, ?it/s]libgcc-ng-8.2.0      | 7.6 MB    | :  43% 0.42873966606510044/1 [00:00<00:00,  4.29it/s]libgcc-ng-8.2.0      | 7.6 MB    | :  76% 0.7566590390189323/1 [00:00<00:00,  3.91it/s] libgcc-ng-8.2.0      | 7.6 MB    | :  94% 0.9443927159757535/1 [00:01<00:00,  1.57s/it]libgcc-ng-8.2.0      | 7.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.38s/it]               
openblas-0.2.14      | 3.5 MB    | :   0% 0/1 [00:00<?, ?it/s]openblas-0.2.14      | 3.5 MB    | :  44% 0.4412177791894822/1 [00:00<00:00,  3.82it/s]openblas-0.2.14      | 3.5 MB    | :  77% 0.7748892080568426/1 [00:00<00:00,  3.65it/s]openblas-0.2.14      | 3.5 MB    | :  95% 0.9547703026494776/1 [00:00<00:00,  1.07it/s]openblas-0.2.14      | 3.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.23it/s]               
pillow-4.0.0         | 864 KB    | :   0% 0/1 [00:00<?, ?it/s]pillow-4.0.0         | 864 KB    | :  81% 0.8148183314343588/1 [00:00<00:00,  5.69it/s]pillow-4.0.0         | 864 KB    | :  99% 0.9861239216537355/1 [00:00<00:00,  2.85it/s]pillow-4.0.0         | 864 KB    | : 100% 1.0/1 [00:00<00:00,  3.48it/s]               
cython-0.25.2        | 6.8 MB    | :   0% 0/1 [00:00<?, ?it/s]cython-0.25.2        | 6.8 MB    | :   6% 0.06048140430260615/1 [00:00<00:01,  1.66s/it]cython-0.25.2        | 6.8 MB    | :  52% 0.5166839967565497/1 [00:00<00:00,  1.24s/it] cython-0.25.2        | 6.8 MB    | :  77% 0.7655523611063845/1 [00:00<00:00,  1.01it/s]cython-0.25.2        | 6.8 MB    | :  93% 0.9328842463435948/1 [00:01<00:00,  2.21s/it]cython-0.25.2        | 6.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.55s/it]               
sqlite-3.13.0        | 4.0 MB    | :   0% 0/1 [00:00<?, ?it/s]sqlite-3.13.0        | 4.0 MB    | :  76% 0.7598276867722359/1 [00:00<00:00,  7.54it/s]sqlite-3.13.0        | 4.0 MB    | :  99% 0.9863558668722738/1 [00:00<00:00,  1.29it/s]sqlite-3.13.0        | 4.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.49it/s]               
jpeg-9b              | 248 KB    | :   0% 0/1 [00:00<?, ?it/s]jpeg-9b              | 248 KB    | : 100% 1.0/1 [00:00<00:00, 15.27it/s]
tk-8.5.18            | 1.9 MB    | :   0% 0/1 [00:00<?, ?it/s]tk-8.5.18            | 1.9 MB    | :  82% 0.8200012702830792/1 [00:00<00:00,  8.17it/s]tk-8.5.18            | 1.9 MB    | : 100% 1.0/1 [00:00<00:00,  2.48it/s]               
pip-9.0.1            | 1.7 MB    | :   0% 0/1 [00:00<?, ?it/s]pip-9.0.1            | 1.7 MB    | :  78% 0.78362348915938/1 [00:00<00:00,  7.62it/s]pip-9.0.1            | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  2.25it/s]             
libstdcxx-ng-8.2.0   | 2.9 MB    | :   0% 0/1 [00:00<?, ?it/s]libstdcxx-ng-8.2.0   | 2.9 MB    | :  62% 0.6226851269128302/1 [00:00<00:00,  5.49it/s]libstdcxx-ng-8.2.0   | 2.9 MB    | :  81% 0.8069120814920329/1 [00:00<00:00,  3.42it/s]libstdcxx-ng-8.2.0   | 2.9 MB    | :  98% 0.9769787720682253/1 [00:00<00:00,  1.42it/s]libstdcxx-ng-8.2.0   | 2.9 MB    | : 100% 1.0/1 [00:00<00:00,  1.79it/s]               
numpy-1.10.4         | 5.9 MB    | :   0% 0/1 [00:00<?, ?it/s]numpy-1.10.4         | 5.9 MB    | :   0% 0.003980047292212332/1 [00:00<00:27, 27.36s/it]numpy-1.10.4         | 5.9 MB    | :  17% 0.16517196262681177/1 [00:00<00:16, 19.34s/it] numpy-1.10.4         | 5.9 MB    | :  27% 0.2686531922243324/1 [00:00<00:10, 13.88s/it] numpy-1.10.4         | 5.9 MB    | :  33% 0.33034392525362355/1 [00:00<00:07, 11.42s/it]numpy-1.10.4         | 5.9 MB    | :  63% 0.6288474721695485/1 [00:00<00:03,  8.10s/it] numpy-1.10.4         | 5.9 MB    | :  77% 0.7662518597765337/1 [00:00<00:01,  5.90s/it]numpy-1.10.4         | 5.9 MB    | :  89% 0.8899649964428004/1 [00:01<00:00,  5.25s/it]numpy-1.10.4         | 5.9 MB    | :  98% 0.9848227902405275/1 [00:01<00:00,  5.26s/it]numpy-1.10.4         | 5.9 MB    | : 100% 1.0/1 [00:01<00:00,  1.95s/it]               
python-3.4.5         | 15.2 MB   | :   0% 0/1 [00:00<?, ?it/s]python-3.4.5         | 15.2 MB   | :  11% 0.11239255099143051/1 [00:00<00:00,  1.12it/s]python-3.4.5         | 15.2 MB   | :  39% 0.39491355245619075/1 [00:00<00:00,  1.37it/s]python-3.4.5         | 15.2 MB   | :  75% 0.75192452998273/1 [00:00<00:00,  1.68it/s]   python-3.4.5         | 15.2 MB   | :  93% 0.9344982743443803/1 [00:02<00:00,  4.30s/it]python-3.4.5         | 15.2 MB   | : 100% 1.0/1 [00:03<00:00,  3.62s/it]               
Preparing transaction: | / - \ | done
Verifying transaction: - \ | / - \ | / - \ | / - \ | / - \ | done
Executing transaction: - \ | / - \ | / done
#
# To activate this environment, use:
# > source activate testenv
#
# To deactivate an active environment, use:
# > source deactivate
#

Collecting pytest==3.5
  Using cached https://files.pythonhosted.org/packages/ed/96/271c93f75212c06e2a7ec3e2fa8a9c90acee0a4838dc05bf379ea09aae31/pytest-3.5.0-py2.py3-none-any.whl
Collecting py>=1.5.0 (from pytest==3.5)
  Using cached https://files.pythonhosted.org/packages/f3/bd/83369ff2dee18f22f27d16b78dd651e8939825af5f8b0b83c38729069962/py-1.5.4-py2.py3-none-any.whl
Collecting more-itertools>=4.0.0 (from pytest==3.5)
  Using cached https://files.pythonhosted.org/packages/79/b1/eace304ef66bd7d3d8b2f78cc374b73ca03bc53664d78151e9df3b3996cc/more_itertools-4.3.0-py3-none-any.whl
Collecting attrs>=17.4.0 (from pytest==3.5)
  Using cached https://files.pythonhosted.org/packages/41/59/cedf87e91ed541be7957c501a92102f9cc6363c623a7666d69d51c78ac5b/attrs-18.1.0-py2.py3-none-any.whl
Collecting six>=1.10.0 (from pytest==3.5)
  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl
Requirement already satisfied: setuptools in /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/setuptools-27.2.0-py3.4.egg (from pytest==3.5)
Collecting pluggy<0.7,>=0.5 (from pytest==3.5)
  Using cached https://files.pythonhosted.org/packages/ba/65/ded3bc40bbf8d887f262f150fbe1ae6637765b5c9534bd55690ed2c0b0f7/pluggy-0.6.0-py3-none-any.whl
Installing collected packages: py, six, more-itertools, attrs, pluggy, pytest
  Found existing installation: py 1.4.32
    Uninstalling py-1.4.32:
      Successfully uninstalled py-1.4.32
  Found existing installation: pytest 3.0.5
    Uninstalling pytest-3.0.5:
      Successfully uninstalled pytest-3.0.5
Successfully installed attrs-18.1.0 more-itertools-4.3.0 pluggy-0.6.0 py-1.5.4 pytest-3.5.0 six-1.11.0
Requirement already satisfied: coverage in /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages
Collecting codecov
  Using cached https://files.pythonhosted.org/packages/8b/28/4c1950a61c3c5786f0f34d643d0d28ec832433c9a7c0bd157690d4eb1d5f/codecov-2.0.15-py2.py3-none-any.whl
Collecting requests>=2.7.9 (from codecov)
  Using cached https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl
Collecting chardet<3.1.0,>=3.0.2 (from requests>=2.7.9->codecov)
  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl
Collecting urllib3<1.24,>=1.21.1 (from requests>=2.7.9->codecov)
  Using cached https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl
Collecting certifi>=2017.4.17 (from requests>=2.7.9->codecov)
  Using cached https://files.pythonhosted.org/packages/16/1f/50d729c104b21c1042aa51560da6141d1cab476ba7015d92b2111c8db841/certifi-2018.8.13-py2.py3-none-any.whl
Collecting idna<2.8,>=2.5 (from requests>=2.7.9->codecov)
  Using cached https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl
Installing collected packages: chardet, urllib3, certifi, idna, requests, codecov
Successfully installed certifi-2018.8.13 chardet-3.0.4 codecov-2.0.15 idna-2.7 requests-2.19.1 urllib3-1.23
Python 3.4.5 :: Continuum Analytics, Inc.
numpy 1.10.4
scipy 0.16.1
Partial import of sklearn during the build process.
[39mblas_opt_info:[0m
[39mblas_mkl_info:[0m
[39m  libraries mkl,vml,guide not found in ['/home/travis/miniconda/envs/testenv/lib'][0m
[39m  NOT AVAILABLE[0m
[39m[0m
[39mopenblas_info:[0m
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/distutils/system_info.py:635: UserWarning: Specified path  is invalid.
  warnings.warn('Specified path %s is invalid.' % d)
[39m  FOUND:[0m
[39m    library_dirs = ['/home/travis/miniconda/envs/testenv/lib'][0m
[39m    libraries = ['openblas'][0m
[39m    language = c[0m
[39m    define_macros = [('HAVE_CBLAS', None)][0m
[39m[0m
[39m  FOUND:[0m
[39m    library_dirs = ['/home/travis/miniconda/envs/testenv/lib'][0m
[39m    libraries = ['openblas'][0m
[39m    language = c[0m
[39m    define_macros = [('HAVE_CBLAS', None)][0m
[39m[0m
sklearn/manifold/_barnes_hut_tsne.pyx: cannot find cimported module 'sklearn.neighbors'
Compiling sklearn/__check_build/_check_build.pyx because it changed.
Compiling sklearn/cluster/_dbscan_inner.pyx because it changed.
Compiling sklearn/cluster/_optics_inner.pyx because it changed.
Compiling sklearn/cluster/_hierarchical.pyx because it changed.
Compiling sklearn/cluster/_k_means_elkan.pyx because it changed.
Compiling sklearn/cluster/_k_means.pyx because it changed.
Compiling sklearn/datasets/_svmlight_format.pyx because it changed.
Compiling sklearn/decomposition/_online_lda.pyx because it changed.
Compiling sklearn/decomposition/cdnmf_fast.pyx because it changed.
Compiling sklearn/ensemble/_gradient_boosting.pyx because it changed.
Compiling sklearn/feature_extraction/_hashing.pyx because it changed.
Compiling sklearn/manifold/_utils.pyx because it changed.
Compiling sklearn/manifold/_barnes_hut_tsne.pyx because it changed.
Compiling sklearn/metrics/cluster/expected_mutual_info_fast.pyx because it changed.
Compiling sklearn/metrics/pairwise_fast.pyx because it changed.
Compiling sklearn/neighbors/ball_tree.pyx because it changed.
Compiling sklearn/neighbors/kd_tree.pyx because it changed.
Compiling sklearn/neighbors/dist_metrics.pyx because it changed.
Compiling sklearn/neighbors/typedefs.pyx because it changed.
Compiling sklearn/neighbors/quad_tree.pyx because it changed.
Compiling sklearn/tree/_tree.pyx because it changed.
Compiling sklearn/tree/_splitter.pyx because it changed.
Compiling sklearn/tree/_criterion.pyx because it changed.
Compiling sklearn/tree/_utils.pyx because it changed.
Compiling sklearn/svm/libsvm.pyx because it changed.
Compiling sklearn/svm/liblinear.pyx because it changed.
Compiling sklearn/svm/libsvm_sparse.pyx because it changed.
Compiling sklearn/_isotonic.pyx because it changed.
Compiling sklearn/linear_model/cd_fast.pyx because it changed.
Compiling sklearn/linear_model/sgd_fast.pyx because it changed.
Compiling sklearn/linear_model/sag_fast.pyx because it changed.
Compiling sklearn/utils/sparsefuncs_fast.pyx because it changed.
Compiling sklearn/utils/arrayfuncs.pyx because it changed.
Compiling sklearn/utils/murmurhash.pyx because it changed.
Compiling sklearn/utils/lgamma.pyx because it changed.
Compiling sklearn/utils/graph_shortest_path.pyx because it changed.
Compiling sklearn/utils/fast_dict.pyx because it changed.
Compiling sklearn/utils/seq_dataset.pyx because it changed.
Compiling sklearn/utils/weight_vector.pyx because it changed.
Compiling sklearn/utils/_random.pyx because it changed.
Compiling sklearn/utils/_logistic_sigmoid.pyx because it changed.
[ 1/41] Cythonizing sklearn/__check_build/_check_build.pyx
[ 2/41] Cythonizing sklearn/_isotonic.pyx
[ 3/41] Cythonizing sklearn/cluster/_dbscan_inner.pyx
[ 4/41] Cythonizing sklearn/cluster/_hierarchical.pyx
[ 5/41] Cythonizing sklearn/cluster/_k_means.pyx
[ 6/41] Cythonizing sklearn/cluster/_k_means_elkan.pyx
[ 7/41] Cythonizing sklearn/cluster/_optics_inner.pyx
[ 8/41] Cythonizing sklearn/datasets/_svmlight_format.pyx
[ 9/41] Cythonizing sklearn/decomposition/_online_lda.pyx
[10/41] Cythonizing sklearn/decomposition/cdnmf_fast.pyx
[11/41] Cythonizing sklearn/ensemble/_gradient_boosting.pyx
[12/41] Cythonizing sklearn/feature_extraction/_hashing.pyx
[13/41] Cythonizing sklearn/linear_model/cd_fast.pyx
[14/41] Cythonizing sklearn/linear_model/sag_fast.pyx
[15/41] Cythonizing sklearn/linear_model/sgd_fast.pyx
[16/41] Cythonizing sklearn/manifold/_barnes_hut_tsne.pyx
[17/41] Cythonizing sklearn/manifold/_utils.pyx
[18/41] Cythonizing sklearn/metrics/cluster/expected_mutual_info_fast.pyx
[19/41] Cythonizing sklearn/metrics/pairwise_fast.pyx
[20/41] Cythonizing sklearn/neighbors/ball_tree.pyx
[21/41] Cythonizing sklearn/neighbors/dist_metrics.pyx
[22/41] Cythonizing sklearn/neighbors/kd_tree.pyx
[23/41] Cythonizing sklearn/neighbors/quad_tree.pyx
[24/41] Cythonizing sklearn/neighbors/typedefs.pyx
[25/41] Cythonizing sklearn/svm/liblinear.pyx
[26/41] Cythonizing sklearn/svm/libsvm.pyx
[27/41] Cythonizing sklearn/svm/libsvm_sparse.pyx
[28/41] Cythonizing sklearn/tree/_criterion.pyx
[29/41] Cythonizing sklearn/tree/_splitter.pyx
[30/41] Cythonizing sklearn/tree/_tree.pyx
[31/41] Cythonizing sklearn/tree/_utils.pyx
[32/41] Cythonizing sklearn/utils/_logistic_sigmoid.pyx
[33/41] Cythonizing sklearn/utils/_random.pyx
[34/41] Cythonizing sklearn/utils/arrayfuncs.pyx
[35/41] Cythonizing sklearn/utils/fast_dict.pyx
[36/41] Cythonizing sklearn/utils/graph_shortest_path.pyx
[37/41] Cythonizing sklearn/utils/lgamma.pyx
[38/41] Cythonizing sklearn/utils/murmurhash.pyx
[39/41] Cythonizing sklearn/utils/seq_dataset.pyx
[40/41] Cythonizing sklearn/utils/sparsefuncs_fast.pyx
[41/41] Cythonizing sklearn/utils/weight_vector.pyx
[39mrunning develop[0m
[39mrunning build_scripts[0m
[39mrunning egg_info[0m
[39mrunning build_src[0m
[39mbuild_src[0m
[39mbuilding library "libsvm-skl" sources[0m
[39mbuilding extension "sklearn.__check_build._check_build" sources[0m
[39mbuilding extension "sklearn.cluster._dbscan_inner" sources[0m
[39mbuilding extension "sklearn.cluster._optics_inner" sources[0m
[39mbuilding extension "sklearn.cluster._hierarchical" sources[0m
[39mbuilding extension "sklearn.cluster._k_means_elkan" sources[0m
[39mbuilding extension "sklearn.cluster._k_means" sources[0m
[39mbuilding extension "sklearn.datasets._svmlight_format" sources[0m
[39mbuilding extension "sklearn.decomposition._online_lda" sources[0m
[39mbuilding extension "sklearn.decomposition.cdnmf_fast" sources[0m
[39mbuilding extension "sklearn.ensemble._gradient_boosting" sources[0m
[39mbuilding extension "sklearn.feature_extraction._hashing" sources[0m
[39mbuilding extension "sklearn.manifold._utils" sources[0m
[39mbuilding extension "sklearn.manifold._barnes_hut_tsne" sources[0m
[39mbuilding extension "sklearn.metrics.cluster.expected_mutual_info_fast" sources[0m
[39mbuilding extension "sklearn.metrics.pairwise_fast" sources[0m
[39mbuilding extension "sklearn.neighbors.ball_tree" sources[0m
[39mbuilding extension "sklearn.neighbors.kd_tree" sources[0m
[39mbuilding extension "sklearn.neighbors.dist_metrics" sources[0m
[39mbuilding extension "sklearn.neighbors.typedefs" sources[0m
[39mbuilding extension "sklearn.neighbors.quad_tree" sources[0m
[39mbuilding extension "sklearn.tree._tree" sources[0m
[39mbuilding extension "sklearn.tree._splitter" sources[0m
[39mbuilding extension "sklearn.tree._criterion" sources[0m
[39mbuilding extension "sklearn.tree._utils" sources[0m
[39mbuilding extension "sklearn.svm.libsvm" sources[0m
[39mbuilding extension "sklearn.svm.liblinear" sources[0m
[39mbuilding extension "sklearn.svm.libsvm_sparse" sources[0m
[39mbuilding extension "sklearn._isotonic" sources[0m
[39mbuilding extension "sklearn.linear_model.cd_fast" sources[0m
[39mbuilding extension "sklearn.linear_model.sgd_fast" sources[0m
[39mbuilding extension "sklearn.linear_model.sag_fast" sources[0m
[39mbuilding extension "sklearn.utils.sparsefuncs_fast" sources[0m
[39mbuilding extension "sklearn.utils.arrayfuncs" sources[0m
[39mbuilding extension "sklearn.utils.murmurhash" sources[0m
[39mbuilding extension "sklearn.utils.lgamma" sources[0m
[39mbuilding extension "sklearn.utils.graph_shortest_path" sources[0m
[39mbuilding extension "sklearn.utils.fast_dict" sources[0m
[39mbuilding extension "sklearn.utils.seq_dataset" sources[0m
[39mbuilding extension "sklearn.utils.weight_vector" sources[0m
[39mbuilding extension "sklearn.utils._random" sources[0m
[39mbuilding extension "sklearn.utils._logistic_sigmoid" sources[0m
[39mbuilding data_files sources[0m
[39mbuild_src: building npy-pkg config files[0m
[39mcreating scikit_learn.egg-info[0m
[39mwriting top-level names to scikit_learn.egg-info/top_level.txt[0m
[39mwriting dependency_links to scikit_learn.egg-info/dependency_links.txt[0m
[39mwriting requirements to scikit_learn.egg-info/requires.txt[0m
[39mwriting scikit_learn.egg-info/PKG-INFO[0m
[39mwriting manifest file 'scikit_learn.egg-info/SOURCES.txt'[0m
[39mreading manifest file 'scikit_learn.egg-info/SOURCES.txt'[0m
[39mreading manifest template 'MANIFEST.in'[0m
[39mwriting manifest file 'scikit_learn.egg-info/SOURCES.txt'[0m
[39mrunning build_ext[0m
[39mcustomize UnixCCompiler[0m
[39mcustomize UnixCCompiler using build_clib[0m
[39mbuilding 'libsvm-skl' library[0m
[39mcompiling C++ sources[0m
[39mC compiler: /usr/lib/ccache/g++ -DNDEBUG -g -fwrapv -O3 -Wall -fPIC
[0m
[39mcreating build[0m
[39mcreating build/temp.linux-x86_64-3.4[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/svm[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/svm/src[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/svm/src/libsvm[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mg++: sklearn/svm/src/libsvm/libsvm_template.cpp[0m
[39mar: adding 1 object files to build/temp.linux-x86_64-3.4/liblibsvm-skl.a[0m
[39mcustomize UnixCCompiler[0m
[39mcustomize UnixCCompiler using build_ext[0m
[31mresetting extension 'sklearn.svm.liblinear' language from 'c' to 'c++'.[0m
[39mcustomize UnixCCompiler[0m
[39mcustomize UnixCCompiler using build_ext[0m
[39mbuilding 'sklearn.__check_build._check_build' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/__check_build[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/__check_build/_check_build.c[0m
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/__check_build/_check_build.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lpython3.4m -o sklearn/__check_build/_check_build.cpython-34m.so[0m
[39mbuilding 'sklearn.cluster._dbscan_inner' extension[0m
[39mcompiling C++ sources[0m
[39mC compiler: /usr/lib/ccache/g++ -DNDEBUG -g -fwrapv -O3 -Wall -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/cluster[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mg++: sklearn/cluster/_dbscan_inner.cpp[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/cluster/_dbscan_inner.cpp:470:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39m/usr/lib/ccache/g++ -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/cluster/_dbscan_inner.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lpython3.4m -o sklearn/cluster/_dbscan_inner.cpython-34m.so[0m
[39mbuilding 'sklearn.cluster._optics_inner' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/cluster/_optics_inner.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/cluster/_optics_inner.c:453:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/cluster/_optics_inner.c: In function ‘__pyx_pw_7sklearn_7cluster_13_optics_inner_1quick_scan’:
sklearn/cluster/_optics_inner.c:20541:35: warning: ‘__pyx_v_idx’ may be used uninitialized in this function [-Wmaybe-uninitialized]
             return PyInt_FromLong((long) value);
                                   ^
sklearn/cluster/_optics_inner.c:2132:7: note: ‘__pyx_v_idx’ was declared here
   int __pyx_v_idx;
       ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/cluster/_optics_inner.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/cluster/_optics_inner.cpython-34m.so[0m
[39mbuilding 'sklearn.cluster._hierarchical' extension[0m
[39mcompiling C++ sources[0m
[39mC compiler: /usr/lib/ccache/g++ -DNDEBUG -g -fwrapv -O3 -Wall -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mg++: sklearn/cluster/_hierarchical.cpp[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/cluster/_hierarchical.cpp:468:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39m/usr/lib/ccache/g++ -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/cluster/_hierarchical.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/cluster/_hierarchical.cpython-34m.so[0m
[39mbuilding 'sklearn.cluster._k_means_elkan' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/cluster/_k_means_elkan.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/cluster/_k_means_elkan.c:453:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/cluster/_k_means_elkan.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/cluster/_k_means_elkan.cpython-34m.so[0m
[39mbuilding 'sklearn.cluster._k_means' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/cluster/_k_means.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/cluster/_k_means.c:467:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/cluster/_k_means.c: In function ‘__pyx_fuse_0__pyx_f_7sklearn_7cluster_8_k_means__assign_labels_array’:
sklearn/cluster/_k_means.c:3609:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_sdot;
               ^
sklearn/cluster/_k_means.c: In function ‘__pyx_fuse_1__pyx_f_7sklearn_7cluster_8_k_means__assign_labels_array’:
sklearn/cluster/_k_means.c:4395:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_ddot;
               ^
sklearn/cluster/_k_means.c: In function ‘__pyx_fuse_0__pyx_f_7sklearn_7cluster_8_k_means__assign_labels_csr’:
sklearn/cluster/_k_means.c:5887:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_sdot;
               ^
sklearn/cluster/_k_means.c: In function ‘__pyx_fuse_1__pyx_f_7sklearn_7cluster_8_k_means__assign_labels_csr’:
sklearn/cluster/_k_means.c:6738:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_ddot;
               ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/cluster/_k_means.o -L/home/travis/miniconda/envs/testenv/lib -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lopenblas -lm -lpython3.4m -o sklearn/cluster/_k_means.cpython-34m.so[0m
[39mbuilding 'sklearn.datasets._svmlight_format' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/datasets[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/datasets/_svmlight_format.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/datasets/_svmlight_format.c:451:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/datasets/_svmlight_format.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lpython3.4m -o sklearn/datasets/_svmlight_format.cpython-34m.so[0m
[39mbuilding 'sklearn.decomposition._online_lda' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/decomposition[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/decomposition/_online_lda.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/decomposition/_online_lda.c:454:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/decomposition/_online_lda.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/decomposition/_online_lda.cpython-34m.so[0m
[39mbuilding 'sklearn.decomposition.cdnmf_fast' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/decomposition/cdnmf_fast.c[0m
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/decomposition/cdnmf_fast.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/decomposition/cdnmf_fast.cpython-34m.so[0m
[39mbuilding 'sklearn.ensemble._gradient_boosting' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/ensemble[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/ensemble/_gradient_boosting.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/ensemble/_gradient_boosting.c:450:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/ensemble/_gradient_boosting.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lpython3.4m -o sklearn/ensemble/_gradient_boosting.cpython-34m.so[0m
[39mbuilding 'sklearn.feature_extraction._hashing' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/feature_extraction[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/feature_extraction/_hashing.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/feature_extraction/_hashing.c:454:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/feature_extraction/_hashing.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/feature_extraction/_hashing.cpython-34m.so[0m
[39mbuilding 'sklearn.manifold._utils' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/manifold[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'
extra options: '-O3'[0m
[39mgcc: sklearn/manifold/_utils.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/manifold/_utils.c:458:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/manifold/_utils.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/manifold/_utils.cpython-34m.so[0m
[39mbuilding 'sklearn.manifold._barnes_hut_tsne' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'
extra options: '-O4'[0m
[39mgcc: sklearn/manifold/_barnes_hut_tsne.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/manifold/_barnes_hut_tsne.c:468:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/manifold/_barnes_hut_tsne.c: In function ‘__pyx_f_7sklearn_8manifold_16_barnes_hut_tsne_compute_gradient_negative’:
sklearn/manifold/_barnes_hut_tsne.c:3220:9: warning: variable ‘__pyx_v_force’ set but not used [-Wunused-but-set-variable]
   float __pyx_v_force[3];
         ^
sklearn/manifold/_barnes_hut_tsne.c:3219:9: warning: variable ‘__pyx_v_iQ’ set but not used [-Wunused-but-set-variable]
   float __pyx_v_iQ[1];
         ^
sklearn/manifold/_barnes_hut_tsne.c: In function ‘__pyx_pf_7sklearn_8manifold_16_barnes_hut_tsne_gradient.isra.29’:
sklearn/manifold/_barnes_hut_tsne.c:3121:83: warning: ‘__pyx_v_buff[1]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
         (__pyx_v_pos_f[__pyx_t_20]) = ((__pyx_v_pos_f[__pyx_t_20]) + (__pyx_v_dij * (__pyx_v_buff[__pyx_v_ax])));
                                                                                   ^
sklearn/manifold/_barnes_hut_tsne.c:2797:9: note: ‘__pyx_v_buff[1]’ was declared here
   float __pyx_v_buff[3];
         ^
sklearn/manifold/_barnes_hut_tsne.c:3121:83: warning: ‘__pyx_v_buff[0]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
         (__pyx_v_pos_f[__pyx_t_20]) = ((__pyx_v_pos_f[__pyx_t_20]) + (__pyx_v_dij * (__pyx_v_buff[__pyx_v_ax])));
                                                                                   ^
sklearn/manifold/_barnes_hut_tsne.c:2797:9: note: ‘__pyx_v_buff[0]’ was declared here
   float __pyx_v_buff[3];
         ^
sklearn/manifold/_barnes_hut_tsne.c:3653:74: warning: ‘__pyx_v_neg_force[2]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
       (__pyx_v_neg_f[((__pyx_v_i * __pyx_v_n_dimensions) + __pyx_v_ax)]) = (__pyx_v_neg_force[__pyx_v_ax]);
                                                                          ^
sklearn/manifold/_barnes_hut_tsne.c:3221:9: note: ‘__pyx_v_neg_force[2]’ was declared here
   float __pyx_v_neg_force[3];
         ^
sklearn/manifold/_barnes_hut_tsne.c:3653:74: warning: ‘__pyx_v_neg_force[1]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
       (__pyx_v_neg_f[((__pyx_v_i * __pyx_v_n_dimensions) + __pyx_v_ax)]) = (__pyx_v_neg_force[__pyx_v_ax]);
                                                                          ^
sklearn/manifold/_barnes_hut_tsne.c:3221:9: note: ���__pyx_v_neg_force[1]’ was declared here
   float __pyx_v_neg_force[3];
         ^
sklearn/manifold/_barnes_hut_tsne.c:3653:74: warning: ‘__pyx_v_neg_force[0]’ may be used uninitialized in this function [-Wmaybe-uninitialized]
       (__pyx_v_neg_f[((__pyx_v_i * __pyx_v_n_dimensions) + __pyx_v_ax)]) = (__pyx_v_neg_force[__pyx_v_ax]);
                                                                          ^
sklearn/manifold/_barnes_hut_tsne.c:3221:9: note: ‘__pyx_v_neg_force[0]’ was declared here
   float __pyx_v_neg_force[3];
         ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/manifold/_barnes_hut_tsne.o -L/home/travis/miniconda/envs/testenv/lib -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lopenblas -lpython3.4m -o sklearn/manifold/_barnes_hut_tsne.cpython-34m.so[0m
[39mbuilding 'sklearn.metrics.cluster.expected_mutual_info_fast' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/metrics[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/metrics/cluster[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/metrics/cluster/expected_mutual_info_fast.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/metrics/cluster/expected_mutual_info_fast.c:454:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/metrics/cluster/expected_mutual_info_fast.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/metrics/cluster/expected_mutual_info_fast.cpython-34m.so[0m
[39mbuilding 'sklearn.metrics.pairwise_fast' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/metrics/pairwise_fast.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/metrics/pairwise_fast.c:466:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/metrics/pairwise_fast.o -L/home/travis/miniconda/envs/testenv/lib -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lopenblas -lm -lpython3.4m -o sklearn/metrics/pairwise_fast.cpython-34m.so[0m
[39mbuilding 'sklearn.neighbors.ball_tree' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/neighbors[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/neighbors/ball_tree.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/neighbors/ball_tree.c:453:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/neighbors/ball_tree.c: In function ‘__pyx_f_7sklearn_9neighbors_9ball_tree_10BinaryTree__kde_single_depthfirst’:
sklearn/neighbors/ball_tree.c:20516:63: warning: ‘__pyx_v_sample_weight’ may be used uninitialized in this function [-Wmaybe-uninitialized]
         __pyx_t_12 = PyFloat_FromDouble((__pyx_v_sample_weight[(__pyx_v_idx_array[__pyx_v_i])])); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 2363, __pyx_L1_error)
                                                               ^
sklearn/neighbors/ball_tree.c:20193:50: note: ‘__pyx_v_sample_weight’ was declared here
   __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_sample_weight;
                                                  ^
sklearn/neighbors/ball_tree.c: In function ‘__pyx_f_7sklearn_9neighbors_9ball_tree_10BinaryTree__kde_single_breadthfirst’:
sklearn/neighbors/ball_tree.c:23163:52: warning: ‘__pyx_v_sample_weight’ may be used uninitialized in this function [-Wmaybe-uninitialized]
     __pyx_v_N = (__pyx_v_N + (__pyx_v_sample_weight[(__pyx_v_idx_array[__pyx_v_i])]));
                                                    ^
sklearn/neighbors/ball_tree.c:19319:50: note: ‘__pyx_v_sample_weight’ was declared here
   __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_sample_weight;
                                                  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/neighbors/ball_tree.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/neighbors/ball_tree.cpython-34m.so[0m
[39mbuilding 'sklearn.neighbors.kd_tree' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/neighbors/kd_tree.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/neighbors/kd_tree.c:453:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/neighbors/kd_tree.c: In function ‘__pyx_f_7sklearn_9neighbors_7kd_tree_10BinaryTree__kde_single_breadthfirst’:
sklearn/neighbors/kd_tree.c:23134:52: warning: ‘__pyx_v_sample_weight’ may be used uninitialized in this function [-Wmaybe-uninitialized]
     __pyx_v_N = (__pyx_v_N + (__pyx_v_sample_weight[(__pyx_v_idx_array[__pyx_v_i])]));
                                                    ^
sklearn/neighbors/kd_tree.c:19290:50: note: ‘__pyx_v_sample_weight’ was declared here
   __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_sample_weight;
                                                  ^
sklearn/neighbors/kd_tree.c: In function ‘__pyx_f_7sklearn_9neighbors_7kd_tree_10BinaryTree__kde_single_depthfirst’:
sklearn/neighbors/kd_tree.c:20487:63: warning: ‘__pyx_v_sample_weight’ may be used uninitialized in this function [-Wmaybe-uninitialized]
         __pyx_t_12 = PyFloat_FromDouble((__pyx_v_sample_weight[(__pyx_v_idx_array[__pyx_v_i])])); if (unlikely(!__pyx_t_12)) __PYX_ERR(0, 2363, __pyx_L1_error)
                                                               ^
sklearn/neighbors/kd_tree.c:20164:50: note: ‘__pyx_v_sample_weight’ was declared here
   __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_sample_weight;
                                                  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/neighbors/kd_tree.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/neighbors/kd_tree.cpython-34m.so[0m
[39mbuilding 'sklearn.neighbors.dist_metrics' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/neighbors/dist_metrics.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/neighbors/dist_metrics.c:454:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/neighbors/dist_metrics.c: In function ‘__pyx_f_7sklearn_9neighbors_12dist_metrics_18SEuclideanDistance_dist’:
sklearn/neighbors/dist_metrics.c:6712:94: warning: passing argument 1 of ‘__pyx_f_7sklearn_9neighbors_12dist_metrics_18SEuclideanDistance_rdist’ from incompatible pointer type [enabled by default]
   __pyx_t_1 = __pyx_f_7sklearn_9neighbors_12dist_metrics_18SEuclideanDistance_rdist(((struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_DistanceMetric *)__pyx_v_self), __pyx_v_x1, __pyx_v_x2, __pyx_v_size); if (unlikely(__pyx_t_1 == -1.0)) __PYX_ERR(1, 463, __pyx_L1_error)
                                                                                              ^
sklearn/neighbors/dist_metrics.c:6517:54: note: expected ‘struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_SEuclideanDistance *’ but argument is of type ‘struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_DistanceMetric *’
 static __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t __pyx_f_7sklearn_9neighbors_12dist_metrics_18SEuclideanDistance_rdist(struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_SEuclideanDistance *__pyx_v_self, __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_x1, __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_x2, __pyx_t_7sklearn_9neighbors_8typedefs_ITYPE_t __pyx_v_size) {
                                                      ^
sklearn/neighbors/dist_metrics.c: In function ‘__pyx_f_7sklearn_9neighbors_12dist_metrics_17MinkowskiDistance_dist’:
sklearn/neighbors/dist_metrics.c:7523:93: warning: passing argument 1 of ‘__pyx_f_7sklearn_9neighbors_12dist_metrics_17MinkowskiDistance_rdist’ from incompatible pointer type [enabled by default]
   __pyx_t_1 = __pyx_f_7sklearn_9neighbors_12dist_metrics_17MinkowskiDistance_rdist(((struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_DistanceMetric *)__pyx_v_self), __pyx_v_x1, __pyx_v_x2, __pyx_v_size); if (unlikely(__pyx_t_1 == -1.0)) __PYX_ERR(1, 552, __pyx_L1_error)
                                                                                             ^
sklearn/neighbors/dist_metrics.c:7444:54: note: expected ‘struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_MinkowskiDistance *’ but argument is of type ‘struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_DistanceMetric *’
 static __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t __pyx_f_7sklearn_9neighbors_12dist_metrics_17MinkowskiDistance_rdist(struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_MinkowskiDistance *__pyx_v_self, __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_x1, __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_x2, __pyx_t_7sklearn_9neighbors_8typedefs_ITYPE_t __pyx_v_size) {
                                                      ^
sklearn/neighbors/dist_metrics.c: In function ‘__pyx_f_7sklearn_9neighbors_12dist_metrics_18WMinkowskiDistance_dist’:
sklearn/neighbors/dist_metrics.c:8223:94: warning: passing argument 1 of ‘__pyx_f_7sklearn_9neighbors_12dist_metrics_18WMinkowskiDistance_rdist’ from incompatible pointer type [enabled by default]
   __pyx_t_1 = __pyx_f_7sklearn_9neighbors_12dist_metrics_18WMinkowskiDistance_rdist(((struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_DistanceMetric *)__pyx_v_self), __pyx_v_x1, __pyx_v_x2, __pyx_v_size); if (unlikely(__pyx_t_1 == -1.0)) __PYX_ERR(1, 611, __pyx_L1_error)
                                                                                              ^
sklearn/neighbors/dist_metrics.c:8038:54: note: expected ‘struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_WMinkowskiDistance *’ but argument is of type ‘struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_DistanceMetric *’
 static __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t __pyx_f_7sklearn_9neighbors_12dist_metrics_18WMinkowskiDistance_rdist(struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_WMinkowskiDistance *__pyx_v_self, __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_x1, __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_x2, __pyx_t_7sklearn_9neighbors_8typedefs_ITYPE_t __pyx_v_size) {
                                                      ^
sklearn/neighbors/dist_metrics.c: In function ‘__pyx_f_7sklearn_9neighbors_12dist_metrics_19MahalanobisDistance_dist’:
sklearn/neighbors/dist_metrics.c:9081:95: warning: passing argument 1 of ‘__pyx_f_7sklearn_9neighbors_12dist_metrics_19MahalanobisDistance_rdist’ from incompatible pointer type [enabled by default]
   __pyx_t_1 = __pyx_f_7sklearn_9neighbors_12dist_metrics_19MahalanobisDistance_rdist(((struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_DistanceMetric *)__pyx_v_self), __pyx_v_x1, __pyx_v_x2, __pyx_v_size); if (unlikely(__pyx_t_1 == -1.0)) __PYX_ERR(1, 684, __pyx_L1_error)
                                                                                               ^
sklearn/neighbors/dist_metrics.c:8841:54: note: expected ‘struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_MahalanobisDistance *’ but argument is of type ‘struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_DistanceMetric *’
 static __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t __pyx_f_7sklearn_9neighbors_12dist_metrics_19MahalanobisDistance_rdist(struct __pyx_obj_7sklearn_9neighbors_12dist_metrics_MahalanobisDistance *__pyx_v_self, __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_x1, __pyx_t_7sklearn_9neighbors_8typedefs_DTYPE_t *__pyx_v_x2, __pyx_t_7sklearn_9neighbors_8typedefs_ITYPE_t __pyx_v_size) {
                                                      ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/neighbors/dist_metrics.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/neighbors/dist_metrics.cpython-34m.so[0m
[39mbuilding 'sklearn.neighbors.typedefs' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/neighbors/typedefs.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/neighbors/typedefs.c:453:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/neighbors/typedefs.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/neighbors/typedefs.cpython-34m.so[0m
[39mbuilding 'sklearn.neighbors.quad_tree' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/neighbors/quad_tree.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/neighbors/quad_tree.c:453:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/neighbors/quad_tree.c:2710:1: warning: function declaration isn’t a prototype [-Wstrict-prototypes]
 static PyObject *__pyx_pf_7sklearn_9neighbors_9quad_tree_9_QuadTree_16test_summarize(); /* proto */
 ^
sklearn/neighbors/quad_tree.c: In function ‘__pyx_f_7sklearn_9neighbors_9quad_tree_9_QuadTree__get_cell_ndarray’:
sklearn/neighbors/quad_tree.c:6994:3: warning: passing argument 1 of ‘(struct PyObject * (*)(struct PyTypeObject *, struct PyArray_Descr *, int,  npy_intp *, npy_intp *, void *, int,  struct PyObject *))*(PyArray_API + 752u)’ from incompatible pointer type [enabled by default]
   __pyx_t_2 = PyArray_NewFromDescr(((PyObject *)__pyx_ptype_5numpy_ndarray), ((PyArray_Descr *)__pyx_t_1), 1, __pyx_v_shape, __pyx_v_strides, ((void *)__pyx_v_self->cells), NPY_DEFAULT, Py_None); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 574, __pyx_L1_error)
   ^
sklearn/neighbors/quad_tree.c:6994:3: note: expected ‘struct PyTypeObject *’ but argument is of type ‘struct PyObject *’
sklearn/neighbors/quad_tree.c: At top level:
sklearn/neighbors/quad_tree.c:7419:18: warning: function declaration isn’t a prototype [-Wstrict-prototypes]
 static PyObject *__pyx_pf_7sklearn_9neighbors_9quad_tree_9_QuadTree_16test_summarize() {
                  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/neighbors/quad_tree.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/neighbors/quad_tree.cpython-34m.so[0m
[39mbuilding 'sklearn.tree._tree' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/tree[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'
extra options: '-O3'[0m
[39mgcc: sklearn/tree/_tree.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/tree/_tree.c:456:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/tree/_tree.c: In function ‘__pyx_f_7sklearn_4tree_5_tree_4Tree__get_node_ndarray’:
sklearn/tree/_tree.c:14116:3: warning: passing argument 1 of ‘(struct PyObject * (*)(struct PyTypeObject *, struct PyArray_Descr *, int,  npy_intp *, npy_intp *, void *, int,  struct PyObject *))*(PyArray_API + 752u)’ from incompatible pointer type [enabled by default]
   __pyx_t_2 = PyArray_NewFromDescr(((PyObject *)__pyx_ptype_5numpy_ndarray), ((PyArray_Descr *)__pyx_t_1), 1, __pyx_v_shape, __pyx_v_strides, ((void *)__pyx_v_self->nodes), NPY_DEFAULT, Py_None); if (unlikely(!__pyx_t_2)) __PYX_ERR(0, 1123, __pyx_L1_error)
   ^
sklearn/tree/_tree.c:14116:3: note: expected ‘struct PyTypeObject *’ but argument is of type ‘struct PyObject *’
sklearn/tree/_tree.c: In function ‘__pyx_f_7sklearn_4tree_5_tree_20BestFirstTreeBuilder_build’:
sklearn/tree/_tree.c:5361:13: warning: ���__pyx_v_split_node_right.improvement’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.improvement’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.impurity_right’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.impurity_right’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.impurity_left’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.impurity_left’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.impurity’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.impurity’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.is_leaf’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.is_leaf’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.depth’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.depth’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.pos’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.pos’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.end’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.end’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.start’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.start’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_right.node_id’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5653:60: note: ‘__pyx_v_split_node_right.node_id’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_right;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_left.node_id’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5652:60: note: ‘__pyx_v_split_node_left.node_id’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_left;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_left.end’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5652:60: note: ‘__pyx_v_split_node_left.end’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_left;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_left.pos’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5652:60: note: ‘__pyx_v_split_node_left.pos’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_left;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_left.is_leaf’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5652:60: note: ‘__pyx_v_split_node_left.is_leaf’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_left;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_left.impurity’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5652:60: note: ‘__pyx_v_split_node_left.impurity’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_left;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_left.impurity_left’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5652:60: note: ‘__pyx_v_split_node_left.impurity_left’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_left;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_left.impurity_right’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5652:60: note: ‘__pyx_v_split_node_left.impurity_right’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_left;
                                                            ^
sklearn/tree/_tree.c:5361:13: warning: ‘__pyx_v_split_node_left.improvement’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   __pyx_t_1 = ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_PriorityHeap *)__pyx_v_frontier->__pyx_vtab)->push(__pyx_v_frontier, __pyx_v_rec->node_id, __pyx_v_rec->start, __pyx_v_rec->end, __pyx_v_rec->pos, __pyx_v_rec->depth, __pyx_v_rec->is_leaf, __pyx_v_rec->improvement, __pyx_v_rec->impurity, __pyx_v_rec->impurity_left, __pyx_v_rec->impurity_right); if (unlikely(__pyx_t_1 == -1)) __PYX_ERR(0, 290, __pyx_L1_error)
             ^
sklearn/tree/_tree.c:5652:60: note: ‘__pyx_v_split_node_left.improvement’ was declared here
   struct __pyx_t_7sklearn_4tree_6_utils_PriorityHeapRecord __pyx_v_split_node_left;
                                                            ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/tree/_tree.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/tree/_tree.cpython-34m.so[0m
[39mbuilding 'sklearn.tree._splitter' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'
extra options: '-O3'[0m
[39mgcc: sklearn/tree/_splitter.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/tree/_splitter.c:456:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/tree/_splitter.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/tree/_splitter.cpython-34m.so[0m
[39mbuilding 'sklearn.tree._criterion' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'
extra options: '-O3'[0m
[39mgcc: sklearn/tree/_criterion.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/tree/_criterion.c:456:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/tree/_criterion.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/tree/_criterion.cpython-34m.so[0m
[39mbuilding 'sklearn.tree._utils' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'
extra options: '-O3'[0m
[39mgcc: sklearn/tree/_utils.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/tree/_utils.c:456:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/tree/_utils.c: In function ‘__pyx_f_7sklearn_4tree_6_utils_24WeightedMedianCalculator_remove’:
sklearn/tree/_utils.c:8755:142: warning: ‘__pyx_v_original_median’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_WeightedMedianCalculator *)__pyx_v_self->__pyx_vtab)->update_median_parameters_post_remove(__pyx_v_self, __pyx_v_data, __pyx_v_weight, __pyx_v_original_median);
                                                                                                                                              ^
sklearn/tree/_utils.c:8707:43: note: ‘__pyx_v_original_median’ was declared here
   __pyx_t_7sklearn_4tree_6_utils_DOUBLE_t __pyx_v_original_median;
                                           ^
sklearn/tree/_utils.c: In function ‘__pyx_f_7sklearn_4tree_6_utils_24WeightedMedianCalculator_pop’:
sklearn/tree/_utils.c:8867:142: warning: ‘__pyx_v_original_median’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_WeightedMedianCalculator *)__pyx_v_self->__pyx_vtab)->update_median_parameters_post_remove(__pyx_v_self, (__pyx_v_data[0]), (__pyx_v_weight[0]), __pyx_v_original_median);
                                                                                                                                              ^
sklearn/tree/_utils.c:8790:10: note: ‘__pyx_v_original_median’ was declared here
   double __pyx_v_original_median;
          ^
sklearn/tree/_utils.c: In function ‘__pyx_f_7sklearn_4tree_6_utils_24WeightedMedianCalculator_push’:
sklearn/tree/_utils.c:8399:140: warning: ‘__pyx_v_original_median’ may be used uninitialized in this function [-Wmaybe-uninitialized]
   ((struct __pyx_vtabstruct_7sklearn_4tree_6_utils_WeightedMedianCalculator *)__pyx_v_self->__pyx_vtab)->update_median_parameters_post_push(__pyx_v_self, __pyx_v_data, __pyx_v_weight, __pyx_v_original_median);
                                                                                                                                            ^
sklearn/tree/_utils.c:8349:43: note: ‘__pyx_v_original_median’ was declared here
   __pyx_t_7sklearn_4tree_6_utils_DOUBLE_t __pyx_v_original_median;
                                           ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/tree/_utils.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/tree/_utils.cpython-34m.so[0m
[39mbuilding 'sklearn.svm.libsvm' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -Isklearn/svm/src/libsvm -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/svm/libsvm.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/svm/libsvm.c:458:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39m/usr/lib/ccache/g++ -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/svm/libsvm.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -llibsvm-skl -lpython3.4m -o sklearn/svm/libsvm.cpython-34m.so[0m
[39mbuilding 'sklearn.svm.liblinear' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/svm/liblinear.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/svm/liblinear.c:468:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
In file included from sklearn/svm/liblinear.c:471:0:
sklearn/svm/src/liblinear/liblinear_helper.c: In function ‘set_problem’:
sklearn/svm/src/liblinear/liblinear_helper.c:145:28: warning: assignment from incompatible pointer type [enabled by default]
     problem->sample_weight = sample_weight;
                            ^
sklearn/svm/src/liblinear/liblinear_helper.c: In function ‘csr_set_problem’:
sklearn/svm/src/liblinear/liblinear_helper.c:174:28: warning: assignment from incompatible pointer type [enabled by default]
     problem->sample_weight = sample_weight;
                            ^
[39mcompiling C++ sources[0m
[39mC compiler: /usr/lib/ccache/g++ -DNDEBUG -g -fwrapv -O3 -Wall -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/svm/src/liblinear[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mg++: sklearn/svm/src/liblinear/tron.cpp[0m
[39mg++: sklearn/svm/src/liblinear/linear.cpp[0m
sklearn/svm/src/liblinear/linear.cpp: In function ‘model* train(const problem*, const parameter*)’:
sklearn/svm/src/liblinear/linear.cpp:2370:6: warning: unused variable ‘n_iter’ [-Wunused-variable]
  int n_iter;
      ^
[39m/usr/lib/ccache/g++ -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/svm/liblinear.o build/temp.linux-x86_64-3.4/sklearn/svm/src/liblinear/linear.o build/temp.linux-x86_64-3.4/sklearn/svm/src/liblinear/tron.o -L/home/travis/miniconda/envs/testenv/lib -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lopenblas -lm -lpython3.4m -o sklearn/svm/liblinear.cpython-34m.so[0m
[39mbuilding 'sklearn.svm.libsvm_sparse' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -Isklearn/svm/src/libsvm -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/svm/libsvm_sparse.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/svm/libsvm_sparse.c:456:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39m/usr/lib/ccache/g++ -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/svm/libsvm_sparse.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -llibsvm-skl -lpython3.4m -o sklearn/svm/libsvm_sparse.cpython-34m.so[0m
[39mbuilding 'sklearn._isotonic' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/_isotonic.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/_isotonic.c:453:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/_isotonic.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/_isotonic.cpython-34m.so[0m
[39mbuilding 'sklearn.linear_model.cd_fast' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/linear_model[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/linear_model/cd_fast.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/linear_model/cd_fast.c:467:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/linear_model/cd_fast.c: In function ‘__pyx_pf_7sklearn_12linear_model_7cd_fast_8enet_coordinate_descent’:
sklearn/linear_model/cd_fast.c:4651:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_sdot;
               ^
sklearn/linear_model/cd_fast.c:4660:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_axpy = cblas_saxpy;
                ^
sklearn/linear_model/cd_fast.c:4669:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_asum = cblas_sasum;
                ^
sklearn/linear_model/cd_fast.c: In function ‘__pyx_pf_7sklearn_12linear_model_7cd_fast_10enet_coordinate_descent’:
sklearn/linear_model/cd_fast.c:6031:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_ddot;
               ^
sklearn/linear_model/cd_fast.c:6040:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_axpy = cblas_daxpy;
                ^
sklearn/linear_model/cd_fast.c:6049:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_asum = cblas_dasum;
                ^
sklearn/linear_model/cd_fast.c: In function ‘__pyx_pf_7sklearn_12linear_model_7cd_fast_14sparse_enet_coordinate_descent’:
sklearn/linear_model/cd_fast.c:8211:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_sdot;
               ^
sklearn/linear_model/cd_fast.c:8220:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_asum = cblas_sasum;
                ^
sklearn/linear_model/cd_fast.c: In function ‘__pyx_pf_7sklearn_12linear_model_7cd_fast_16sparse_enet_coordinate_descent’:
sklearn/linear_model/cd_fast.c:10151:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_ddot;
               ^
sklearn/linear_model/cd_fast.c:10160:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_asum = cblas_dasum;
                ^
sklearn/linear_model/cd_fast.c: In function ‘__pyx_pf_7sklearn_12linear_model_7cd_fast_20enet_coordinate_descent_gram’:
sklearn/linear_model/cd_fast.c:12591:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_sdot;
               ^
sklearn/linear_model/cd_fast.c:12600:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_axpy = cblas_saxpy;
                ^
sklearn/linear_model/cd_fast.c:12609:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_asum = cblas_sasum;
                ^
sklearn/linear_model/cd_fast.c: In function ‘__pyx_pf_7sklearn_12linear_model_7cd_fast_22enet_coordinate_descent_gram’:
sklearn/linear_model/cd_fast.c:14094:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_ddot;
               ^
sklearn/linear_model/cd_fast.c:14103:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_axpy = cblas_daxpy;
                ^
sklearn/linear_model/cd_fast.c:14112:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_asum = cblas_dasum;
                ^
sklearn/linear_model/cd_fast.c: In function ‘__pyx_pf_7sklearn_12linear_model_7cd_fast_26enet_coordinate_descent_multi_task’:
sklearn/linear_model/cd_fast.c:16208:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_sdot;
               ^
sklearn/linear_model/cd_fast.c:16217:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_nrm2 = cblas_snrm2;
                ^
sklearn/linear_model/cd_fast.c:16226:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_asum = cblas_sasum;
                ^
sklearn/linear_model/cd_fast.c:16235:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_copy = cblas_scopy;
                ^
sklearn/linear_model/cd_fast.c:16253:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_ger = cblas_sger;
               ^
sklearn/linear_model/cd_fast.c:16262:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_gemv = cblas_sgemv;
                ^
sklearn/linear_model/cd_fast.c: In function ‘__pyx_pf_7sklearn_12linear_model_7cd_fast_28enet_coordinate_descent_multi_task’:
sklearn/linear_model/cd_fast.c:17915:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_dot = cblas_ddot;
               ^
sklearn/linear_model/cd_fast.c:17924:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_nrm2 = cblas_dnrm2;
                ^
sklearn/linear_model/cd_fast.c:17933:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_asum = cblas_dasum;
                ^
sklearn/linear_model/cd_fast.c:17942:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_copy = cblas_dcopy;
                ^
sklearn/linear_model/cd_fast.c:17960:15: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_ger = cblas_dger;
               ^
sklearn/linear_model/cd_fast.c:17969:16: warning: assignment from incompatible pointer type [enabled by default]
   __pyx_v_gemv = cblas_dgemv;
                ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/linear_model/cd_fast.o -L/home/travis/miniconda/envs/testenv/lib -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lopenblas -lm -lpython3.4m -o sklearn/linear_model/cd_fast.cpython-34m.so[0m
[39mbuilding 'sklearn.linear_model.sgd_fast' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/linear_model/sgd_fast.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/linear_model/sgd_fast.c:468:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/linear_model/sgd_fast.o -L/home/travis/miniconda/envs/testenv/lib -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lopenblas -lm -lpython3.4m -o sklearn/linear_model/sgd_fast.cpython-34m.so[0m
[39mbuilding 'sklearn.linear_model.sag_fast' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/linear_model/sag_fast.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/linear_model/sag_fast.c:451:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/linear_model/sag_fast.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lpython3.4m -o sklearn/linear_model/sag_fast.cpython-34m.so[0m
[39mbuilding 'sklearn.utils.sparsefuncs_fast' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/utils[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/sparsefuncs_fast.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/sparsefuncs_fast.c:448:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/sparsefuncs_fast.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/utils/sparsefuncs_fast.cpython-34m.so[0m
[39mbuilding 'sklearn.utils.arrayfuncs' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/arrayfuncs.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/arrayfuncs.c:466:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/arrayfuncs.o -L/home/travis/miniconda/envs/testenv/lib -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lopenblas -lm -lpython3.4m -o sklearn/utils/arrayfuncs.cpython-34m.so[0m
[39mbuilding 'sklearn.utils.murmurhash' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-Isklearn/utils/src -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/murmurhash.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/murmurhash.c:449:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mcompiling C++ sources[0m
[39mC compiler: /usr/lib/ccache/g++ -DNDEBUG -g -fwrapv -O3 -Wall -fPIC
[0m
[39mcreating build/temp.linux-x86_64-3.4/sklearn/utils/src[0m
[39mcompile options: '-Isklearn/utils/src -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mg++: sklearn/utils/src/MurmurHash3.cpp[0m
[39m/usr/lib/ccache/g++ -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/murmurhash.o build/temp.linux-x86_64-3.4/sklearn/utils/src/MurmurHash3.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lpython3.4m -o sklearn/utils/murmurhash.cpython-34m.so[0m
[39mbuilding 'sklearn.utils.lgamma' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-Isklearn/utils/src -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/lgamma.c[0m
[39mgcc: sklearn/utils/src/gamma.c[0m
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/lgamma.o build/temp.linux-x86_64-3.4/sklearn/utils/src/gamma.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/utils/lgamma.cpython-34m.so[0m
[39mbuilding 'sklearn.utils.graph_shortest_path' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/graph_shortest_path.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/graph_shortest_path.c:450:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/graph_shortest_path.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lpython3.4m -o sklearn/utils/graph_shortest_path.cpython-34m.so[0m
[39mbuilding 'sklearn.utils.fast_dict' extension[0m
[39mcompiling C++ sources[0m
[39mC compiler: /usr/lib/ccache/g++ -DNDEBUG -g -fwrapv -O3 -Wall -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mg++: sklearn/utils/fast_dict.cpp[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/fast_dict.cpp:474:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
sklearn/utils/fast_dict.cpp: In function ‘PyObject* __pyx_pw_7sklearn_5utils_9fast_dict_1argmin(PyObject*, PyObject*)’:
sklearn/utils/fast_dict.cpp:22206:48: warning: ‘__pyx_v_min_key’ may be used uninitialized in this function [-Wmaybe-uninitialized]
             return PyInt_FromLong((long) value);
                                                ^
sklearn/utils/fast_dict.cpp:3836:46: note: ‘__pyx_v_min_key’ was declared here
   __pyx_t_7sklearn_5utils_9fast_dict_ITYPE_t __pyx_v_min_key;
                                              ^
[39m/usr/lib/ccache/g++ -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/fast_dict.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/utils/fast_dict.cpython-34m.so[0m
[39mbuilding 'sklearn.utils.seq_dataset' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/seq_dataset.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/seq_dataset.c:450:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/seq_dataset.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lpython3.4m -o sklearn/utils/seq_dataset.cpython-34m.so[0m
[39mbuilding 'sklearn.utils.weight_vector' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-DHAVE_CBLAS -Isklearn/src/cblas -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/weight_vector.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/weight_vector.c:466:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/weight_vector.o -L/home/travis/miniconda/envs/testenv/lib -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lopenblas -lm -lpython3.4m -o sklearn/utils/weight_vector.cpython-34m.so[0m
[39mbuilding 'sklearn.utils._random' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/_random.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/_random.c:453:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/_random.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/utils/_random.cpython-34m.so[0m
[39mbuilding 'sklearn.utils._logistic_sigmoid' extension[0m
[39mcompiling C sources[0m
[39mC compiler: /usr/lib/ccache/gcc -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC
[0m
[39mcompile options: '-I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include -I/home/travis/miniconda/envs/testenv/include/python3.4m -I/home/travis/miniconda/envs/testenv/include/python3.4m -c'[0m
[39mgcc: sklearn/utils/_logistic_sigmoid.c[0m
In file included from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarraytypes.h:1781:0,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/ndarrayobject.h:18,
                 from /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/arrayobject.h:4,
                 from sklearn/utils/_logistic_sigmoid.c:454:
/home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: #warning "Using deprecated NumPy API, disable it by " "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION" [-Wcpp]
 #warning "Using deprecated NumPy API, disable it by " \
  ^
[39mgcc -pthread -shared -L/home/travis/miniconda/envs/testenv/lib -Wl,-rpath=/home/travis/miniconda/envs/testenv/lib,--no-as-needed build/temp.linux-x86_64-3.4/sklearn/utils/_logistic_sigmoid.o -L/home/travis/miniconda/envs/testenv/lib -Lbuild/temp.linux-x86_64-3.4 -lm -lpython3.4m -o sklearn/utils/_logistic_sigmoid.cpython-34m.so[0m
[39mCreating /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/scikit-learn.egg-link (link to .)[0m
[39mAdding scikit-learn 0.20.dev0 to easy-install.pth file[0m
[39m
Installed /home/travis/build/scikit-learn/scikit-learn[0m
[39mProcessing dependencies for scikit-learn==0.20.dev0[0m
[39mSearching for scipy==0.16.1[0m
[39mBest match: scipy 0.16.1[0m
[39mAdding scipy 0.16.1 to easy-install.pth file[0m
[39m
Using /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages[0m
[39mSearching for numpy==1.10.4[0m
[39mBest match: numpy 1.10.4[0m
[39mAdding numpy 1.10.4 to easy-install.pth file[0m
[39m
Using /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages[0m
[39mFinished processing dependencies for scikit-learn==0.20.dev0[0m
cache directory                     /home/travis/.ccache
cache hit (direct)                  8343
cache hit (preprocessed)            3775
cache miss                         11335
called for link                     3626
files in cache                        93
cache size                          40.6 Mbytes
max cache size                     100.0 Mbytes

travis_time:end:1c12e68b:start=1534811773822444781,finish=1534812002468254281,duration=228645809500[0Ktravis_fold:end:install[0Ktravis_time:start:18703e7c[0K$ bash build_tools/travis/test_script.sh
Python 3.4.5 :: Continuum Analytics, Inc.
numpy 1.10.4
scipy 0.16.1
48 CPUs
[1m============================= test session starts ==============================[0m
platform linux -- Python 3.4.5, pytest-3.5.0, py-1.5.4, pluggy-0.6.0
rootdir: /tmp/sklearn, inifile: setup.cfg
plugins: cov-2.3.1
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.
  _nan_object_mask = _nan_object_array != _nan_object_array
[1mcollecting 0 items                                                             [0m[1mcollecting 0 items                                                             [0m[1mcollecting 1 item                                                              [0m[1mcollecting 1 item                                                              [0m[1mcollecting 1 item                                                              [0m[1mcollecting 3 items                                                             [0m[1mcollecting 3 items                                                             [0m[1mcollecting 5 items                                                             [0m[1mcollecting 7 items                                                             [0m[1mcollecting 7 items                                                             [0m[1mcollecting 11 items                                                            [0m[1mcollecting 12 items                                                            [0m[1mcollecting 12 items                                                            [0m[1mcollecting 12 items                                                            [0m[1mcollecting 16 items                                                            [0m[1mcollecting 20 items                                                            [0m[1mcollecting 23 items                                                            [0m[1mcollecting 23 items                                                            [0m[1mcollecting 23 items                                                            [0m[1mcollecting 23 items                                                            [0m[1mcollecting 23 items                                                            [0m[1mcollecting 24 items                                                            [0m[1mcollecting 26 items                                                            [0m[1mcollecting 27 items                                                            [0m[1mcollecting 28 items                                                            [0m[1mcollecting 30 items                                                            [0m[1mcollecting 32 items                                                            [0m[1mcollecting 33 items                                                            [0m[1mcollecting 33 items                                                            [0m[1mcollecting 34 items                                                            [0m[1mcollecting 34 items                                                            [0m[1mcollecting 34 items                                                            [0m[1mcollecting 34 items                                                            [0m[1mcollecting 41 items                                                            [0m[1mcollecting 41 items                                                            [0m[1mcollecting 51 items                                                            [0m[1mcollecting 51 items                                                            [0m[1mcollecting 58 items                                                            [0m[1mcollecting 58 items                                                            [0m[1mcollecting 76 items                                                            [0m[1mcollecting 76 items                                                            [0m[1mcollecting 77 items                                                            [0m[1mcollecting 77 items                                                            [0m[1mcollecting 99 items                                                            [0m[1mcollecting 99 items                                                            [0m[1mcollecting 166 items                                                           [0m[1mcollecting 166 items                                                           [0m[1mcollecting 175 items                                                           [0m[1mcollecting 175 items                                                           [0m[1mcollecting 193 items                                                           [0m[1mcollecting 193 items                                                           [0m[1mcollecting 206 items                                                           [0m[1mcollecting 206 items                                                           [0m[1mcollecting 208 items                                                           [0m[1mcollecting 209 items                                                           [0m[1mcollecting 209 items                                                           [0m[1mcollecting 209 items                                                           [0m[1mcollecting 254 items                                                           [0m[1mcollecting 254 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 265 items                                                           [0m[1mcollecting 271 items                                                           [0m[1mcollecting 271 items                                                           [0m[1mcollecting 275 items                                                           [0m[1mcollecting 275 items                                                           [0m[1mcollecting 280 items                                                           [0m[1mcollecting 280 items                                                           [0m[1mcollecting 285 items                                                           [0m[1mcollecting 285 items                                                           [0m[1mcollecting 292 items                                                           [0m[1mcollecting 292 items                                                           [0m[1mcollecting 293 items                                                           [0m[1mcollecting 296 items                                                           [0m[1mcollecting 296 items                                                           [0m[1mcollecting 296 items                                                           [0m[1mcollecting 304 items                                                           [0m[1mcollecting 304 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 311 items                                                           [0m[1mcollecting 312 items                                                           [0m[1mcollecting 312 items                                                           [0m[1mcollecting 312 items                                                           [0m[1mcollecting 312 items                                                           [0m[1mcollecting 312 items                                                           [0m[1mcollecting 312 items                                                           [0m[1mcollecting 315 items                                                           [0m[1mcollecting 315 items                                                           [0m[1mcollecting 334 items                                                           [0m[1mcollecting 334 items                                                           [0m[1mcollecting 335 items                                                           [0m[1mcollecting 335 items                                                           [0m[1mcollecting 335 items                                                           [0m[1mcollecting 335 items                                                           [0m[1mcollecting 336 items                                                           [0m[1mcollecting 336 items                                                           [0m[1mcollecting 338 items                                                           [0m[1mcollecting 338 items                                                           [0m[1mcollecting 343 items                                                           [0m[1mcollecting 343 items                                                           [0m[1mcollecting 347 items                                                           [0m[1mcollecting 347 items                                                           [0m[1mcollecting 369 items                                                           [0m[1mcollecting 369 items                                                           [0m[1mcollecting 370 items                                                           [0m[1mcollecting 370 items                                                           [0m[1mcollecting 398 items                                                           [0m[1mcollecting 398 items                                                           [0m[1mcollecting 452 items                                                           [0m[1mcollecting 452 items                                                           [0m[1mcollecting 453 items                                                           [0m[1mcollecting 453 items                                                           [0m[1mcollecting 454 items                                                           [0m[1mcollecting 455 items                                                           [0m[1mcollecting 456 items                                                           [0m[1mcollecting 457 items                                                           [0m[1mcollecting 459 items                                                           [0m[1mcollecting 460 items                                                           [0m[1mcollecting 461 items                                                           [0m[1mcollecting 463 items                                                           [0m[1mcollecting 464 items                                                           [0m[1mcollecting 464 items                                                           [0m[1mcollecting 464 items                                                           [0m[1mcollecting 521 items                                                           [0m[1mcollecting 521 items                                                           [0m[1mcollecting 522 items                                                           [0m[1mcollecting 522 items                                                           [0m[1mcollecting 530 items                                                           [0m[1mcollecting 530 items                                                           [0m[1mcollecting 546 items                                                           [0m[1mcollecting 546 items                                                           [0m[1mcollecting 558 items                                                           [0m[1mcollecting 558 items                                                           [0m[1mcollecting 582 items                                                           [0m[1mcollecting 582 items                                                           [0m[1mcollecting 615 items                                                           [0m[1mcollecting 615 items                                                           [0m[1mcollecting 656 items                                                           [0m[1mcollecting 656 items                                                           [0m[1mcollecting 676 items                                                           [0m[1mcollecting 676 items                                                           [0m[1mcollecting 691 items                                                           [0m[1mcollecting 691 items                                                           [0m[1mcollecting 691 items                                                           [0m[1mcollecting 691 items                                                           [0m[1mcollecting 693 items                                                           [0m[1mcollecting 693 items                                                           [0m[1mcollecting 693 items                                                           [0m[1mcollecting 695 items                                                           [0m[1mcollecting 696 items                                                           [0m[1mcollecting 696 items                                                           [0m[1mcollecting 696 items                                                           [0m[1mcollecting 696 items                                                           [0m[1mcollecting 726 items                                                           [0m[1mcollecting 726 items                                                           [0m[1mcollecting 730 items                                                           [0m[1mcollecting 730 items                                                           [0m[1mcollecting 894 items                                                           [0m[1mcollecting 894 items                                                           [0m[1mcollecting 1011 items                                                          [0m[1mcollecting 1011 items                                                          [0m[1mcollecting 1020 items                                                          [0m[1mcollecting 1020 items                                                          [0m[1mcollecting 1034 items                                                          [0m[1mcollecting 1034 items                                                          [0m[1mcollecting 1041 items                                                          [0m[1mcollecting 1041 items                                                          [0m[1mcollecting 1058 items                                                          [0m[1mcollecting 1058 items                                                          [0m[1mcollecting 1074 items                                                          [0m[1mcollecting 1074 items                                                          [0m[1mcollecting 1076 items                                                          [0m[1mcollecting 1077 items                                                          [0m[1mcollecting 1078 items                                                          [0m[1mcollecting 1078 items                                                          [0m[1mcollecting 1081 items                                                          [0m[1mcollecting 1081 items                                                          [0m[1mcollecting 1081 items                                                          [0m[1mcollecting 1109 items                                                          [0m[1mcollecting 1109 items                                                          [0m[1mcollecting 1120 items                                                          [0m[1mcollecting 1120 items                                                          [0m[1mcollecting 1140 items                                                          [0m[1mcollecting 1140 items                                                          [0m[1mcollecting 1204 items                                                          [0m[1mcollecting 1204 items                                                          [0m[1mcollecting 1204 items                                                          [0m[1mcollecting 1204 items                                                          [0m[1mcollecting 1204 items                                                          [0m[1mcollecting 1206 items                                                          [0m[1mcollecting 1212 items                                                          [0m[1mcollecting 1213 items                                                          [0m[1mcollecting 1213 items                                                          [0m[1mcollecting 1213 items                                                          [0m[1mcollecting 1218 items                                                          [0m[1mcollecting 1218 items                                                          [0m[1mcollecting 1223 items                                                          [0m[1mcollecting 1223 items                                                          [0m[1mcollecting 1256 items                                                          [0m[1mcollecting 1256 items                                                          [0m[1mcollecting 1277 items                                                          [0m[1mcollecting 1277 items                                                          [0m[1mcollecting 1285 items                                                          [0m[1mcollecting 1285 items                                                          [0m[1mcollecting 1297 items                                                          [0m[1mcollecting 1297 items                                                          [0m[1mcollecting 1299 items                                                          [0m[1mcollecting 1299 items                                                          [0m[1mcollecting 1299 items                                                          [0m[1mcollecting 1300 items                                                          [0m[1mcollecting 1301 items                                                          [0m[1mcollecting 1301 items                                                          [0m[1mcollecting 1301 items                                                          [0m[1mcollecting 1301 items                                                          [0m[1mcollecting 1301 items                                                          [0m[1mcollecting 1331 items                                                          [0m[1mcollecting 1331 items                                                          [0m[1mcollecting 1411 items                                                          [0m[1mcollecting 1411 items                                                          [0m[1mcollecting 1663 items                                                          [0m[1mcollecting 1663 items                                                          [0m[1mcollecting 1663 items                                                          [0m[1mcollecting 1665 items                                                          [0m[1mcollecting 1672 items                                                          [0m[1mcollecting 1672 items                                                          [0m[1mcollecting 1675 items                                                          [0m[1mcollecting 1677 items                                                          [0m[1mcollecting 1677 items                                                          [0m[1mcollecting 1679 items                                                          [0m[1mcollecting 1680 items                                                          [0m[1mcollecting 1682 items                                                          [0m[1mcollecting 1682 items                                                          [0m[1mcollecting 1686 items                                                          [0m[1mcollecting 1687 items                                                          [0m[1mcollecting 1689 items                                                          [0m[1mcollecting 1689 items                                                          [0m[1mcollecting 1689 items                                                          [0m[1mcollecting 1689 items                                                          [0m[1mcollecting 1707 items                                                          [0m[1mcollecting 1707 items                                                          [0m[1mcollecting 1716 items                                                          [0m[1mcollecting 1716 items                                                          [0m[1mcollecting 1756 items                                                          [0m[1mcollecting 1756 items                                                          [0m[1mcollecting 1765 items                                                          [0m[1mcollecting 1765 items                                                          [0m[1mcollecting 1795 items                                                          [0m[1mcollecting 1795 items                                                          [0m[1mcollecting 1890 items                                                          [0m[1mcollecting 1890 items                                                          [0m[1mcollecting 1908 items                                                          [0m[1mcollecting 1908 items                                                          [0m[1mcollecting 1924 items                                                          [0m[1mcollecting 1924 items                                                          [0m[1mcollecting 1927 items                                                          [0m[1mcollecting 1927 items                                                          [0m[1mcollecting 1934 items                                                          [0m[1mcollecting 1934 items                                                          [0m[1mcollecting 1957 items                                                          [0m[1mcollecting 1957 items                                                          [0m[1mcollecting 2001 items                                                          [0m[1mcollecting 2001 items                                                          [0m[1mcollecting 2017 items                                                          [0m[1mcollecting 2017 items                                                          [0m[1mcollecting 2081 items                                                          [0m[1mcollecting 2145 items                                                          [0m[1mcollecting 2176 items                                                          [0m[1mcollecting 2207 items                                                          [0m[1mcollecting 2224 items                                                          [0m[1mcollecting 2224 items                                                          [0m[1mcollecting 2235 items                                                          [0m[1mcollecting 2235 items                                                          [0m[1mcollecting 2252 items                                                          [0m[1mcollecting 2252 items                                                          [0m[1mcollecting 2253 items                                                          [0m[1mcollecting 2254 items                                                          [0m[1mcollecting 2255 items                                                          [0m[1mcollecting 2256 items                                                          [0m[1mcollecting 2257 items                                                          [0m[1mcollecting 2257 items                                                          [0m[1mcollecting 2257 items                                                          [0m[1mcollecting 2263 items                                                          [0m[1mcollecting 2263 items                                                          [0m[1mcollecting 2270 items                                                          [0m[1mcollecting 2270 items                                                          [0m[1mcollecting 2273 items                                                          [0m[1mcollecting 2273 items                                                          [0m[1mcollecting 2285 items                                                          [0m[1mcollecting 2285 items                                                          [0m[1mcollecting 2333 items                                                          [0m[1mcollecting 2333 items                                                          [0m[1mcollecting 2333 items                                                          [0m[1mcollecting 2349 items                                                          [0m[1mcollecting 2353 items                                                          [0m[1mcollecting 2359 items                                                          [0m[1mcollecting 2365 items                                                          [0m[1mcollecting 2366 items                                                          [0m[1mcollecting 2366 items                                                          [0m[1mcollecting 2366 items                                                          [0m[1mcollecting 2373 items                                                          [0m[1mcollecting 2373 items                                                          [0m[1mcollecting 2373 items                                                          [0m[1mcollecting 2373 items                                                          [0m[1mcollecting 2376 items                                                          [0m[1mcollecting 2376 items                                                          [0m[1mcollecting 2416 items                                                          [0m[1mcollecting 2416 items                                                          [0m[1mcollecting 2435 items                                                          [0m[1mcollecting 2435 items                                                          [0m[1mcollecting 2443 items                                                          [0m[1mcollecting 2443 items                                                          [0m[1mcollecting 2443 items                                                          [0m[1mcollecting 2514 items                                                          [0m[1mcollecting 2514 items                                                          [0m[1mcollecting 2951 items                                                          [0m[1mcollecting 2951 items                                                          [0m[1mcollecting 3029 items                                                          [0m[1mcollecting 3029 items                                                          [0m[1mcollecting 3079 items                                                          [0m[1mcollecting 3079 items                                                          [0m[1mcollecting 3086 items                                                          [0m[1mcollecting 3086 items                                                          [0m[1mcollecting 3133 items                                                          [0m[1mcollecting 3133 items                                                          [0m[1mcollecting 3133 items                                                          [0m[1mcollecting 3133 items                                                          [0m[1mcollecting 3133 items                                                          [0m[1mcollecting 3133 items                                                          [0m[1mcollecting 3133 items                                                          [0m[1mcollecting 3148 items                                                          [0m[1mcollecting 3148 items                                                          [0m[1mcollecting 3179 items                                                          [0m[1mcollecting 3179 items                                                          [0m[1mcollecting 3181 items                                                          [0m[1mcollecting 3181 items                                                          [0m[1mcollecting 3184 items                                                          [0m[1mcollecting 3199 items                                                          [0m[1mcollecting 3203 items                                                          [0m[1mcollecting 3203 items                                                          [0m[1mcollecting 3203 items                                                          [0m[1mcollecting 3203 items                                                          [0m[1mcollecting 3258 items                                                          [0m[1mcollecting 3258 items                                                          [0m[1mcollecting 3305 items                                                          [0m[1mcollecting 3305 items                                                          [0m[1mcollecting 3352 items                                                          [0m[1mcollecting 3352 items                                                          [0m[1mcollecting 3353 items                                                          [0m[1mcollecting 3357 items                                                          [0m[1mcollecting 3359 items                                                          [0m[1mcollecting 3361 items                                                          [0m[1mcollecting 3361 items                                                          [0m[1mcollecting 3361 items                                                          [0m[1mcollecting 3362 items                                                          [0m[1mcollecting 3364 items                                                          [0m[1mcollecting 3365 items                                                          [0m[1mcollecting 3365 items                                                          [0m[1mcollecting 3365 items                                                          [0m[1mcollecting 3378 items                                                          [0m[1mcollecting 3378 items                                                          [0m[1mcollecting 3626 items                                                          [0m[1mcollecting 3626 items                                                          [0m[1mcollecting 3684 items                                                          [0m[1mcollecting 3684 items                                                          [0m[1mcollecting 3761 items                                                          [0m[1mcollecting 3761 items                                                          [0m[1mcollecting 3800 items                                                          [0m[1mcollecting 3800 items                                                          [0m[1mcollecting 3812 items                                                          [0m[1mcollecting 3812 items                                                          [0m[1mcollecting 3821 items                                                          [0m[1mcollecting 3821 items                                                          [0m[1mcollecting 3874 items                                                          [0m[1mcollecting 3874 items                                                          [0m[1mcollecting 3885 items                                                          [0m[1mcollecting 3885 items                                                          [0m[1mcollecting 3885 items                                                          [0m[1mcollecting 3885 items                                                          [0m[1mcollecting 3885 items                                                          [0m[1mcollecting 3886 items                                                          [0m[1mcollecting 3886 items                                                          [0m[1mcollecting 3886 items                                                          [0m[1mcollecting 3911 items                                                          [0m[1mcollecting 3911 items                                                          [0m[1mcollecting 3923 items                                                          [0m[1mcollecting 3923 items                                                          [0m[1mcollecting 3929 items                                                          [0m[1mcollecting 3929 items                                                          [0m[1mcollecting 3930 items                                                          [0m[1mcollecting 3932 items                                                          [0m[1mcollecting 3932 items                                                          [0m[1mcollecting 3932 items                                                          [0m[1mcollecting 3945 items                                                          [0m[1mcollecting 3945 items                                                          [0m[1mcollecting 3949 items                                                          [0m[1mcollecting 3949 items                                                          [0m[1mcollecting 3949 items                                                          [0m[1mcollecting 3968 items                                                          [0m[1mcollecting 3968 items                                                          [0m[1mcollecting 3977 items                                                          [0m[1mcollecting 3977 items                                                          [0m[1mcollecting 4127 items                                                          [0m[1mcollecting 4127 items                                                          [0m[1mcollecting 4161 items                                                          [0m[1mcollecting 4161 items                                                          [0m[1mcollecting 4204 items                                                          [0m[1mcollecting 4204 items                                                          [0m[1mcollecting 4215 items                                                          [0m[1mcollecting 4215 items                                                          [0m[1mcollecting 4223 items                                                          [0m[1mcollecting 4223 items                                                          [0m[1mcollecting 4255 items                                                          [0m[1mcollecting 4255 items                                                          [0m[1mcollecting 4258 items                                                          [0m[1mcollecting 4258 items                                                          [0m[1mcollecting 4258 items                                                          [0m[1mcollecting 4268 items                                                          [0m[1mcollecting 4268 items                                                          [0m[1mcollecting 4268 items                                                          [0m[1mcollecting 4268 items                                                          [0m[1mcollecting 4274 items                                                          [0m[1mcollecting 4274 items                                                          [0m[1mcollecting 4274 items                                                          [0m[1mcollecting 4293 items                                                          [0m[1mcollecting 4293 items                                                          [0m[1mcollecting 4309 items                                                          [0m[1mcollecting 4309 items                                                          [0m[1mcollecting 4356 items                                                          [0m[1mcollecting 4356 items                                                          [0m[1mcollecting 4356 items                                                          [0m[1mcollecting 4378 items                                                          [0m[1mcollecting 4378 items                                                          [0m[1mcollecting 4387 items                                                          [0m[1mcollecting 4387 items                                                          [0m[1mcollecting 4388 items                                                          [0m[1mcollecting 4388 items                                                          [0m[1mcollecting 9430 items                                                          [0m[1mcollecting 9430 items                                                          [0m[1mcollecting 9433 items                                                          [0m[1mcollecting 9433 items                                                          [0m[1mcollecting 9447 items                                                          [0m[1mcollecting 9447 items                                                          [0m[1mcollecting 9449 items                                                          [0m[1mcollecting 9449 items                                                          [0m[1mcollecting 9484 items                                                          [0m[1mcollecting 9484 items                                                          [0m[1mcollecting 9596 items                                                          [0m[1mcollecting 9596 items                                                          [0m[1mcollecting 9598 items                                                          [0m[1mcollecting 9598 items                                                          [0m[1mcollecting 9628 items                                                          [0m[1mcollecting 9628 items                                                          [0m[1mcollecting 9637 items                                                          [0m[1mcollecting 9637 items                                                          [0m[1mcollecting 9645 items                                                          [0m[1mcollecting 9645 items                                                          [0m[1mcollecting 9646 items                                                          [0m[1mcollecting 9646 items                                                          [0m[1mcollecting 9685 items                                                          [0m[1mcollecting 9685 items                                                          [0m[1mcollecting 9708 items                                                          [0m[1mcollecting 9708 items                                                          [0m[1mcollecting 9751 items                                                          [0m[1mcollecting 9751 items                                                          [0m[1mcollecting 9786 items                                                          [0m[1mcollecting 9786 items                                                          [0m[1mcollecting 9802 items                                                          [0m[1mcollecting 9802 items                                                          [0m[1mcollecting 9804 items                                                          [0m[1mcollecting 9804 items                                                          [0m[1mcollecting 9805 items                                                          [0m[1mcollecting 9807 items                                                          [0m[1mcollecting 9807 items                                                          [0m[1mcollecting 9807 items                                                          [0m[1mcollecting 9811 items                                                          [0m[1mcollecting 9811 items                                                          [0m[1mcollecting 9984 items                                                          [0m[1mcollecting 9991 items                                                          [0m[1mcollecting 9991 items                                                          [0m[1mcollecting 9991 items                                                          [0m[1mcollecting 9991 items                                                          [0m[1mcollecting 9991 items                                                          [0m[1mcollecting 9991 items                                                          [0m[1mcollecting 9991 items                                                          [0m[1mcollecting 9991 items                                                          [0m[1mcollecting 9992 items                                                          [0m[1mcollecting 9992 items                                                          [0m[1mcollecting 9995 items                                                          [0m[1mcollecting 9995 items                                                          [0m[1mcollecting 9996 items                                                          [0m[1mcollecting 9996 items                                                          [0m[1mcollecting 9996 items                                                          [0m[1mcollecting 9996 items                                                          [0m[1mcollecting 9999 items                                                          [0m[1mcollecting 9999 items                                                          [0m[1mcollecting 10000 items                                                         [0m[1mcollecting 10000 items                                                         [0m[1mcollecting 10000 items                                                         [0m[1mcollecting 10001 items                                                         [0m[1mcollecting 10003 items                                                         [0m[1mcollecting 10003 items                                                         [0m[1mcollecting 10003 items                                                         [0m[1mcollecting 10003 items                                                         [0m[1mcollecting 10003 items                                                         [0m[1mcollecting 10004 items                                                         [0m[1mcollecting 10004 items                                                         [0m[1mcollecting 10014 items                                                         [0m[1mcollecting 10014 items                                                         [0m[1mcollecting 10018 items                                                         [0m[1mcollecting 10018 items                                                         [0m[1mcollecting 10025 items                                                         [0m[1mcollecting 10025 items                                                         [0m[1mcollecting 10053 items                                                         [0m[1mcollecting 10053 items                                                         [0m[1mcollecting 10055 items                                                         [0m[1mcollecting 10055 items                                                         [0m[1mcollecting 10063 items                                                         [0m[1mcollecting 10063 items                                                         [0m[1mcollecting 10064 items                                                         [0m[1mcollecting 10064 items                                                         [0m[1mcollecting 10065 items                                                         [0m[1mcollecting 10065 items                                                         [0m[1mcollecting 10067 items                                                         [0m[1mcollecting 10067 items                                                         [0m[1mcollecting 10075 items                                                         [0m[1mcollecting 10075 items                                                         [0m[1mcollecting 10081 items                                                         [0m[1mcollecting 10081 items                                                         [0m[1mcollecting 10082 items                                                         [0m[1mcollecting 10082 items                                                         [0m[1mcollecting 10086 items                                                         [0m[1mcollecting 10086 items                                                         [0m[1mcollecting 10088 items                                                         [0m[1mcollecting 10088 items                                                         [0m[1mcollecting 10092 items                                                         [0m[1mcollecting 10092 items                                                         [0m[1mcollecting 10095 items                                                         [0m[1mcollecting 10095 items                                                         [0m[1mcollecting 10136 items                                                         [0m[1mcollecting 10136 items                                                         [0m[1mcollecting 10141 items                                                         [0m[1mcollecting 10141 items                                                         [0m[1mcollecting 10143 items                                                         [0m[1mcollecting 10155 items                                                         [0m[1mcollecting 10155 items                                                         [0m[1mcollecting 10189 items                                                         [0m[1mcollecting 10189 items                                                         [0m[1mcollecting 10242 items                                                         [0m[1mcollected 10242 items                                                          [0m

sklearn/_config.py s[36m                                                     [  0%][0m
sklearn/discriminant_analysis.py ss[36m                                      [  0%][0m
sklearn/exceptions.py ss[36m                                                 [  0%][0m
sklearn/impute.py ss[36m                                                     [  0%][0m
sklearn/kernel_approximation.py ssss[36m                                     [  0%][0m
sklearn/kernel_ridge.py s[36m                                                [  0%][0m
sklearn/naive_bayes.py ssss[36m                                              [  0%][0m
sklearn/pipeline.py ssss[36m                                                 [  0%][0m
sklearn/random_projection.py sss[36m                                         [  0%][0m
sklearn/cluster/affinity_propagation_.py s[36m                               [  0%][0m
sklearn/cluster/bicluster.py ss[36m                                          [  0%][0m
sklearn/cluster/birch.py s[36m                                               [  0%][0m
sklearn/cluster/dbscan_.py s[36m                                             [  0%][0m
sklearn/cluster/hierarchical.py ss[36m                                       [  0%][0m
sklearn/cluster/k_means_.py ss[36m                                           [  0%][0m
sklearn/cluster/mean_shift_.py s[36m                                         [  0%][0m
sklearn/cluster/spectral.py s[36m                                            [  0%][0m
sklearn/cluster/tests/test_affinity_propagation.py .......[36m               [  0%][0m
sklearn/cluster/tests/test_bicluster.py ........s.[36m                       [  0%][0m
sklearn/cluster/tests/test_birch.py .......[36m                              [  0%][0m
sklearn/cluster/tests/test_dbscan.py ..................[36m                  [  0%][0m
sklearn/cluster/tests/test_feature_agglomeration.py .[36m                    [  0%][0m
sklearn/cluster/tests/test_hierarchical.py ......................[36m        [  0%][0m
sklearn/cluster/tests/test_k_means.py ..................................[36m [  1%]
[0m.................................[36m                                        [  1%][0m
sklearn/cluster/tests/test_mean_shift.py .........[36m                       [  1%][0m
sklearn/cluster/tests/test_optics.py ..................[36m                  [  1%][0m
sklearn/cluster/tests/test_spectral.py .............[36m                     [  2%][0m
sklearn/compose/_column_transformer.py ss[36m                                [  2%][0m
sklearn/compose/_target.py s[36m                                             [  2%][0m
sklearn/compose/tests/test_column_transformer.py .s.....s...............[36m [  2%]
[0m.sssssssss...........s[36m                                                   [  2%][0m
sklearn/compose/tests/test_target.py ...........[36m                         [  2%][0m
sklearn/covariance/tests/test_covariance.py ......[36m                       [  2%][0m
sklearn/covariance/tests/test_elliptic_envelope.py ....[36m                  [  2%][0m
sklearn/covariance/tests/test_graph_lasso.py .....[36m                       [  2%][0m
sklearn/covariance/tests/test_graphical_lasso.py .....[36m                   [  2%][0m
sklearn/covariance/tests/test_robust_covariance.py .......[36m               [  2%][0m
sklearn/cross_decomposition/cca_.py s[36m                                    [  2%][0m
sklearn/cross_decomposition/pls_.py sss[36m                                  [  2%][0m
sklearn/cross_decomposition/tests/test_pls.py ........[36m                   [  2%][0m
sklearn/datasets/base.py sssssss[36m                                         [  3%][0m
sklearn/datasets/samples_generator.py s[36m                                  [  3%][0m
sklearn/datasets/tests/test_20news.py sss[36m                                [  3%][0m
sklearn/datasets/tests/test_base.py ...................[36m                  [  3%][0m
sklearn/datasets/tests/test_california_housing.py s[36m                      [  3%][0m
sklearn/datasets/tests/test_covtype.py s[36m                                 [  3%][0m
sklearn/datasets/tests/test_kddcup99.py ss[36m                               [  3%][0m
sklearn/datasets/tests/test_lfw.py .....[36m                                 [  3%][0m
sklearn/datasets/tests/test_mldata.py ....[36m                               [  3%][0m
sklearn/datasets/tests/test_openml.py ......................[36m             [  3%][0m
sklearn/datasets/tests/test_rcv1.py s[36m                                    [  3%][0m
sklearn/datasets/tests/test_samples_generator.py .......................[36m [  3%]
[0m.....[36m                                                                    [  3%][0m
sklearn/datasets/tests/test_svmlight_format.py .........................[36m [  4%]
[0m.............................[36m                                            [  4%][0m
sklearn/decomposition/base.py s[36m                                          [  4%][0m
sklearn/decomposition/factor_analysis.py s[36m                               [  4%][0m
sklearn/decomposition/fastica_.py s[36m                                      [  4%][0m
sklearn/decomposition/incremental_pca.py s[36m                               [  4%][0m
sklearn/decomposition/kernel_pca.py s[36m                                    [  4%][0m
sklearn/decomposition/nmf.py ss[36m                                          [  4%][0m
sklearn/decomposition/online_lda.py s[36m                                    [  4%][0m
sklearn/decomposition/pca.py s[36m                                           [  4%][0m
sklearn/decomposition/sparse_pca.py ss[36m                                   [  4%][0m
sklearn/decomposition/truncated_svd.py s[36m                                 [  4%][0m
sklearn/decomposition/tests/test_dict_learning.py ......................[36m [  4%]
[0m...................................[36m                                      [  5%][0m
sklearn/decomposition/tests/test_factor_analysis.py .[36m                    [  5%][0m
sklearn/decomposition/tests/test_fastica.py ........[36m                     [  5%][0m
sklearn/decomposition/tests/test_incremental_pca.py ................[36m     [  5%][0m
sklearn/decomposition/tests/test_kernel_pca.py ............[36m              [  5%][0m
sklearn/decomposition/tests/test_nmf.py ........................[36m         [  5%][0m
sklearn/decomposition/tests/test_online_lda.py .........................[36m [  5%]
[0m........[36m                                                                 [  6%][0m
sklearn/decomposition/tests/test_pca.py ................................[36m [  6%]
[0m.........[36m                                                                [  6%][0m
sklearn/decomposition/tests/test_sparse_pca.py ..............ss....[36m      [  6%][0m
sklearn/decomposition/tests/test_truncated_svd.py ...............[36m        [  6%][0m
sklearn/ensemble/forest.py ss[36m                                            [  6%][0m
sklearn/ensemble/partial_dependence.py ss[36m                                [  6%][0m
sklearn/ensemble/voting_classifier.py s[36m                                  [  6%][0m
sklearn/ensemble/tests/test_bagging.py ..............................[36m    [  7%][0m
sklearn/ensemble/tests/test_base.py ....[36m                                 [  7%][0m
sklearn/ensemble/tests/test_forest.py ..................................[36m [  7%]
[0m........................................................................[36m [  8%]
[0m..........................................................[36m               [  8%][0m
sklearn/ensemble/tests/test_gradient_boosting.py FFFFFFF..FFFFFFFFFFFFFF[36m [  8%]
[0mFFFFFFFFFFFFFFFFFFFFFFFFF.FFFFFFFFFFFFFFF.FFFFFFFFFFFFFFFFFFFFFFFFFFFFFF[36m [  9%]
[0mFFFFFF..FFFF..FFFFFFFF[36m                                                   [  9%][0m
sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py ........[36m [  9%]
[0m.[36m                                                                        [  9%][0m
sklearn/ensemble/tests/test_iforest.py ..............[36m                    [ 10%][0m
sklearn/ensemble/tests/test_partial_dependence.py FFFFsss[36m                [ 10%][0m
sklearn/ensemble/tests/test_voting_classifier.py .................[36m       [ 10%][0m
sklearn/ensemble/tests/test_weight_boosting.py ................[36m          [ 10%][0m
sklearn/feature_extraction/dict_vectorizer.py ss[36m                         [ 10%][0m
sklearn/feature_extraction/hashing.py s[36m                                  [ 10%][0m
sklearn/feature_extraction/image.py s[36m                                    [ 10%][0m
sklearn/feature_extraction/text.py sss[36m                                   [ 10%][0m
sklearn/feature_extraction/tests/test_dict_vectorizer.py ...............[36m [ 10%]
[0m.............[36m                                                            [ 10%][0m
sklearn/feature_extraction/tests/test_feature_hasher.py ...........[36m      [ 10%][0m
sklearn/feature_extraction/tests/test_image.py ....................[36m      [ 11%][0m
sklearn/feature_extraction/tests/test_text.py ..........................[36m [ 11%]
[0m......................................[36m                                   [ 11%][0m
sklearn/feature_selection/rfe.py ss[36m                                      [ 11%][0m
sklearn/feature_selection/univariate_selection.py ssssss[36m                 [ 11%][0m
sklearn/feature_selection/variance_threshold.py s[36m                        [ 11%][0m
sklearn/feature_selection/tests/test_base.py .....[36m                       [ 11%][0m
sklearn/feature_selection/tests/test_chi2.py .....[36m                       [ 11%][0m
sklearn/feature_selection/tests/test_feature_select.py .................[36m [ 12%]
[0m................[36m                                                         [ 12%][0m
sklearn/feature_selection/tests/test_from_model.py .....................[36m [ 12%]
[0m[36m                                                                         [ 12%][0m
sklearn/feature_selection/tests/test_mutual_info.py ........[36m             [ 12%][0m
sklearn/feature_selection/tests/test_rfe.py ............[36m                 [ 12%][0m
sklearn/feature_selection/tests/test_variance_threshold.py ..[36m            [ 12%][0m
sklearn/gaussian_process/gpc.py s[36m                                        [ 12%][0m
sklearn/gaussian_process/gpr.py s[36m                                        [ 12%][0m
sklearn/gaussian_process/tests/test_gpc.py .............................[36m [ 12%]
[0m.[36m                                                                        [ 12%][0m
sklearn/gaussian_process/tests/test_gpr.py .............................[36m [ 13%]
[0m...................................................[36m                      [ 13%][0m
sklearn/gaussian_process/tests/test_kernels.py .........................[36m [ 14%]
[0m........................................................................[36m [ 14%]
[0m........................................................................[36m [ 15%]
[0m........................................................................[36m [ 16%]
[0m...........[36m                                                              [ 16%][0m
sklearn/linear_model/bayes.py ss[36m                                         [ 16%][0m
sklearn/linear_model/coordinate_descent.py sssssss[36m                       [ 16%][0m
sklearn/linear_model/least_angle.py sss[36m                                  [ 16%][0m
sklearn/linear_model/logistic.py ss[36m                                      [ 16%][0m
sklearn/linear_model/passive_aggressive.py ss[36m                            [ 16%][0m
sklearn/linear_model/perceptron.py s[36m                                     [ 16%][0m
sklearn/linear_model/randomized_l1.py ss[36m                                 [ 16%][0m
sklearn/linear_model/ridge.py ssss[36m                                       [ 16%][0m
sklearn/linear_model/sag.py s[36m                                            [ 16%][0m
sklearn/linear_model/stochastic_gradient.py ss[36m                           [ 16%][0m
sklearn/linear_model/tests/test_base.py ..................[36m               [ 16%][0m
sklearn/linear_model/tests/test_bayes.py s........[36m                       [ 16%][0m
sklearn/linear_model/tests/test_coordinate_descent.py ..................[36m [ 16%]
[0m......................[36m                                                   [ 17%][0m
sklearn/linear_model/tests/test_huber.py .........[36m                       [ 17%][0m
sklearn/linear_model/tests/test_least_angle.py .........................[36m [ 17%]
[0m.....[36m                                                                    [ 17%][0m
sklearn/linear_model/tests/test_logistic.py ............................[36m [ 17%]
[0m...................................................................[36m      [ 18%][0m
sklearn/linear_model/tests/test_omp.py ..................[36m                [ 18%][0m
sklearn/linear_model/tests/test_passive_aggressive.py ................[36m   [ 18%][0m
sklearn/linear_model/tests/test_perceptron.py ...[36m                        [ 18%][0m
sklearn/linear_model/tests/test_randomized_l1.py .......[36m                 [ 18%][0m
sklearn/linear_model/tests/test_ransac.py .......................[36m        [ 19%][0m
sklearn/linear_model/tests/test_ridge.py ...............................[36m [ 19%]
[0m.............[36m                                                            [ 19%][0m
sklearn/linear_model/tests/test_sag.py ................[36m                  [ 19%][0m
sklearn/linear_model/tests/test_sgd.py .................................[36m [ 20%]
[0m........................................................................[36m [ 20%]
[0m........................................................................[36m [ 21%]
[0m..............................[36m                                           [ 21%][0m
sklearn/linear_model/tests/test_sparse_coordinate_descent.py ...........[36m [ 21%]
[0m[36m                                                                         [ 21%][0m
sklearn/linear_model/tests/test_theil_sen.py .................[36m           [ 21%][0m
sklearn/manifold/isomap.py s[36m                                             [ 21%][0m
sklearn/manifold/locally_linear.py s[36m                                     [ 22%][0m
sklearn/manifold/mds.py s[36m                                                [ 22%][0m
sklearn/manifold/spectral_embedding_.py s[36m                                [ 22%][0m
sklearn/manifold/t_sne.py s[36m                                              [ 22%][0m
sklearn/manifold/tests/test_isomap.py ......[36m                             [ 22%][0m
sklearn/manifold/tests/test_locally_linear.py .......[36m                    [ 22%][0m
sklearn/manifold/tests/test_mds.py ...[36m                                   [ 22%][0m
sklearn/manifold/tests/test_spectral_embedding.py ....s.......[36m           [ 22%][0m
sklearn/manifold/tests/test_t_sne.py ...................................[36m [ 22%]
[0m.............[36m                                                            [ 22%][0m
sklearn/metrics/classification.py ssssssssssssssss[36m                       [ 22%][0m
sklearn/metrics/pairwise.py ssss[36m                                         [ 22%][0m
sklearn/metrics/ranking.py ssssss[36m                                        [ 23%][0m
sklearn/metrics/regression.py ssssss[36m                                     [ 23%][0m
sklearn/metrics/scorer.py s[36m                                              [ 23%][0m
sklearn/metrics/cluster/supervised.py sssssss[36m                            [ 23%][0m
sklearn/metrics/cluster/tests/test_bicluster.py ...[36m                      [ 23%][0m
sklearn/metrics/cluster/tests/test_common.py ...........................[36m [ 23%]
[0m.............[36m                                                            [ 23%][0m
sklearn/metrics/cluster/tests/test_supervised.py ...................[36m     [ 23%][0m
sklearn/metrics/cluster/tests/test_unsupervised.py ........[36m              [ 23%][0m
sklearn/metrics/tests/test_classification.py ...........................[36m [ 24%]
[0m............................................[36m                             [ 24%][0m
sklearn/metrics/tests/test_common.py ...................................[36m [ 24%]
[0m........................................................................[36m [ 25%]
[0m........................................................................[36m [ 26%]
[0m........................................................................[36m [ 26%]
[0m........................................................................[36m [ 27%]
[0m........................................................................[36m [ 28%]
[0m..........................................[36m                               [ 28%][0m
sklearn/metrics/tests/test_pairwise.py .................................[36m [ 29%]
[0m.............................................[36m                            [ 29%][0m
sklearn/metrics/tests/test_ranking.py ..................................[36m [ 29%]
[0m................[36m                                                         [ 30%][0m
sklearn/metrics/tests/test_regression.py .......[36m                         [ 30%][0m
sklearn/metrics/tests/test_score_objects.py ............................[36m [ 30%]
[0m...................[36m                                                      [ 30%][0m
sklearn/mixture/tests/test_bayesian_mixture.py ...............[36m           [ 30%][0m
sklearn/mixture/tests/test_gaussian_mixture.py .........................[36m [ 30%]
[0m......[36m                                                                   [ 31%][0m
sklearn/mixture/tests/test_mixture.py ..[36m                                 [ 31%][0m
sklearn/model_selection/_search.py sss[36m                                   [ 31%][0m
sklearn/model_selection/_split.py sssssssssssssss[36m                        [ 31%][0m
sklearn/model_selection/_validation.py ssss[36m                              [ 31%][0m
sklearn/model_selection/tests/test_search.py ...........................[36m [ 31%]
[0m............................[36m                                             [ 31%][0m
sklearn/model_selection/tests/test_split.py ............................[36m [ 32%]
[0m...................[36m                                                      [ 32%][0m
sklearn/model_selection/tests/test_validation.py .......................[36m [ 32%]
[0m........................[36m                                                 [ 32%][0m
sklearn/neighbors/approximate.py s[36m                                       [ 32%][0m
sklearn/neighbors/base.py ssss[36m                                           [ 32%][0m
sklearn/neighbors/classification.py ss[36m                                   [ 32%][0m
sklearn/neighbors/graph.py ss[36m                                            [ 32%][0m
sklearn/neighbors/nearest_centroid.py s[36m                                  [ 32%][0m
sklearn/neighbors/regression.py ss[36m                                       [ 32%][0m
sklearn/neighbors/unsupervised.py s[36m                                      [ 32%][0m
sklearn/neighbors/tests/test_approximate.py .............[36m                [ 32%][0m
sklearn/neighbors/tests/test_ball_tree.py ..............................[36m [ 33%]
[0m........................................................................[36m [ 33%]
[0m........................................................................[36m [ 34%]
[0m........................................................................[36m [ 35%]
[0m..[36m                                                                       [ 35%][0m
sklearn/neighbors/tests/test_dist_metrics.py ...........................[36m [ 35%]
[0m...............................[36m                                          [ 35%][0m
sklearn/neighbors/tests/test_kd_tree.py ................................[36m [ 36%]
[0m.............................................[36m                            [ 36%][0m
sklearn/neighbors/tests/test_kde.py ....................................[36m [ 37%]
[0m...[36m                                                                      [ 37%][0m
sklearn/neighbors/tests/test_lof.py ............[36m                         [ 37%][0m
sklearn/neighbors/tests/test_nearest_centroid.py .........[36m               [ 37%][0m
sklearn/neighbors/tests/test_neighbors.py ..............................[36m [ 37%]
[0m.......................[36m                                                  [ 37%][0m
sklearn/neighbors/tests/test_quad_tree.py ...........[36m                    [ 37%][0m
sklearn/neural_network/rbm.py s[36m                                          [ 37%][0m
sklearn/neural_network/tests/test_mlp.py .........................[36m       [ 38%][0m
sklearn/neural_network/tests/test_rbm.py ............[36m                    [ 38%][0m
sklearn/neural_network/tests/test_stochastic_optimizers.py ......[36m        [ 38%][0m
sklearn/preprocessing/_discretization.py s[36m                               [ 38%][0m
sklearn/preprocessing/_encoders.py ss[36m                                    [ 38%][0m
sklearn/preprocessing/data.py sssssssssssss[36m                              [ 38%][0m
sklearn/preprocessing/label.py ssss[36m                                      [ 38%][0m
sklearn/preprocessing/tests/test_base.py ...................[36m             [ 38%][0m
sklearn/preprocessing/tests/test_common.py .........[36m                     [ 38%][0m
sklearn/preprocessing/tests/test_data.py ...........................s.s.[36m [ 39%]
[0ms.s...........s.s.......................................................[36m [ 39%]
[0m...............................................[36m                          [ 40%][0m
sklearn/preprocessing/tests/test_discretization.py .....................[36m [ 40%]
[0m.............[36m                                                            [ 40%][0m
sklearn/preprocessing/tests/test_encoders.py ...............sss.........[36m [ 40%]
[0m.....s.......s..[36m                                                         [ 41%][0m
sklearn/preprocessing/tests/test_function_transformer.py ..........s[36m     [ 41%][0m
sklearn/preprocessing/tests/test_imputation.py ........[36m                  [ 41%][0m
sklearn/preprocessing/tests/test_label.py ..............................[36m [ 41%]
[0m..[36m                                                                       [ 41%][0m
sklearn/semi_supervised/label_propagation.py sss[36m                         [ 41%][0m
sklearn/semi_supervised/tests/test_label_propagation.py ..........[36m       [ 41%][0m
sklearn/svm/classes.py ssssss[36m                                            [ 41%][0m
sklearn/svm/tests/test_bounds.py ...................[36m                     [ 41%][0m
sklearn/svm/tests/test_sparse.py ................[36m                        [ 42%][0m
sklearn/svm/tests/test_svm.py ..........................................[36m [ 42%]
[0m.....[36m                                                                    [ 42%][0m
sklearn/tests/test_base.py ......................[36m                        [ 42%][0m
sklearn/tests/test_calibration.py .........[36m                              [ 42%][0m
sklearn/tests/test_check_build.py .[36m                                      [ 42%][0m
sklearn/tests/test_common.py ...........................................[36m [ 43%]
[0m........................................................................[36m [ 43%]
[0m........................................................................[36m [ 44%]
[0m..................s...................................s.................[36m [ 45%]
[0m........................................................................[36m [ 46%]
[0m...........................s...................................s........[36m [ 46%]
[0m.................................................s......................[36m [ 47%]
[0m...........s.....................................................s......[36m [ 48%]
[0m........................................................................[36m [ 48%]
[0m....s......................s...................................s........[36m [ 49%]
[0m..........................s............................s................[36m [ 50%]
[0m...................s....................................................[36m [ 50%]
[0m........................................................................[36m [ 51%]
[0m...........................................................s............[36m [ 52%]
[0m.......................s.................................s..............[36m [ 53%]
[0m......................s.................................................[36m [ 53%]
[0m........................................................................[36m [ 54%]
[0m...................................................s....................[36m [ 55%]
[0m........................................................................[36m [ 55%]
[0m......................................................................s.[36m [ 56%]
[0m..................................s.....................................[36m [ 57%]
[0m........................................................................[36m [ 58%]
[0m....................s...................................................[36m [ 58%]
[0m.......................................s................................[36m [ 59%]
[0m.....................................................s..................[36m [ 60%]
[0m........................................................................[36m [ 60%]
[0m..........................................s.............................[36m [ 61%]
[0m......................s.................................................[36m [ 62%]
[0m........................................................................[36m [ 62%]
[0m........................................................................[36m [ 63%]
[0m........................................................................[36m [ 64%]
[0m........................................................................[36m [ 65%]
[0m........................................................................[36m [ 65%]
[0m..................................................s.....................[36m [ 66%]
[0m............s.....................................s.....................[36m [ 67%]
[0m.................................................................s......[36m [ 67%]
[0m...............................s........................................[36m [ 68%]
[0m........................................................................[36m [ 69%]
[0m........................................................................[36m [ 69%]
[0m........................................................................[36m [ 70%]
[0m.............s...................................................s......[36m [ 71%]
[0m........................................................................[36m [ 72%]
[0m........................................................................[36m [ 72%]
[0m........................s...............................................[36m [ 73%]
[0m........................................................................[36m [ 74%]
[0m.............................s.....................s..............s.....[36m [ 74%]
[0m........................................................................[36m [ 75%]
[0m......................................s.................................[36m [ 76%]
[0m........................................................................[36m [ 77%]
[0m.................................s....................................s.[36m [ 77%]
[0m........................................................................[36m [ 78%]
[0m..................................................s.....................[36m [ 79%]
[0m........................................................................[36m [ 79%]
[0m....................................s...................................[36m [ 80%]
[0m........................................................................[36m [ 81%]
[0m.........................s....................................s.........[36m [ 81%]
[0m........................................................................[36m [ 82%]
[0m.........s.................................s............................[36m [ 83%]
[0m.....s....................................s.............................[36m [ 84%]
[0m...................................s....................................[36m [ 84%]
[0m.s..................................s.........................s.........[36m [ 85%]
[0m.s......................................................................[36m [ 86%]
[0m........................................................................[36m [ 86%]
[0m........................................................................[36m [ 87%]
[0m...............................................................s........[36m [ 88%]
[0m........................................................................[36m [ 88%]
[0m........................................................................[36m [ 89%]
[0m.........................s..............................................[36m [ 90%]
[0m........................................................................[36m [ 91%]
[0m........................................................................[36m [ 91%]
[0m...............................[36m                                          [ 92%][0m
sklearn/tests/test_config.py ...[36m                                         [ 92%][0m
sklearn/tests/test_discriminant_analysis.py ..............[36m               [ 92%][0m
sklearn/tests/test_docstring_parameters.py s.[36m                            [ 92%][0m
sklearn/tests/test_dummy.py ...................................[36m          [ 92%][0m
sklearn/tests/test_impute.py ...............................ss..........[36m [ 93%]
[0mss...................................................................[36m    [ 93%][0m
sklearn/tests/test_init.py .s[36m                                            [ 93%][0m
sklearn/tests/test_isotonic.py ..............................[36m            [ 94%][0m
sklearn/tests/test_kernel_approximation.py .........[36m                     [ 94%][0m
sklearn/tests/test_kernel_ridge.py ........[36m                              [ 94%][0m
sklearn/tests/test_metaestimators.py .[36m                                   [ 94%][0m
sklearn/tests/test_multiclass.py .......................................[36m [ 94%]
[0m[36m                                                                         [ 94%][0m
sklearn/tests/test_multioutput.py F...F.F................[36m                [ 94%][0m
sklearn/tests/test_naive_bayes.py ......................................[36m [ 95%]
[0m.....[36m                                                                    [ 95%][0m
sklearn/tests/test_pipeline.py ...................................[36m       [ 95%][0m
sklearn/tests/test_random_projection.py ................[36m                 [ 95%][0m
sklearn/tests/test_site_joblib.py ..[36m                                     [ 95%][0m
sklearn/tree/export.py s[36m                                                 [ 95%][0m
sklearn/tree/tree.py ss[36m                                                  [ 95%][0m
sklearn/tree/tests/test_export.py ..F.[36m                                   [ 95%][0m
sklearn/tree/tests/test_tree.py ........................................[36m [ 96%]
[0m........................................................................[36m [ 96%]
[0m.............................................................[36m            [ 97%][0m
sklearn/utils/__init__.py sssssss[36m                                        [ 97%][0m
sklearn/utils/deprecation.py s[36m                                           [ 97%][0m
sklearn/utils/extmath.py sss[36m                                             [ 97%][0m
sklearn/utils/graph.py s[36m                                                 [ 97%][0m
sklearn/utils/multiclass.py sss[36m                                          [ 97%][0m
sklearn/utils/random.py s[36m                                                [ 97%][0m
sklearn/utils/testing.py s[36m                                               [ 97%][0m
sklearn/utils/validation.py ss[36m                                           [ 97%][0m
sklearn/utils/tests/test_bench.py .[36m                                      [ 97%][0m
sklearn/utils/tests/test_class_weight.py ..........[36m                      [ 97%][0m
sklearn/utils/tests/test_deprecation.py .s..[36m                             [ 97%][0m
sklearn/utils/tests/test_estimator_checks.py .......[36m                     [ 97%][0m
sklearn/utils/tests/test_extmath.py ............................[36m         [ 98%][0m
sklearn/utils/tests/test_fast_dict.py ..[36m                                 [ 98%][0m
sklearn/utils/tests/test_fixes.py ........[36m                               [ 98%][0m
sklearn/utils/tests/test_graph.py .[36m                                      [ 98%][0m
sklearn/utils/tests/test_linear_assignment.py .[36m                          [ 98%][0m
sklearn/utils/tests/test_metaestimators.py ..[36m                            [ 98%][0m
sklearn/utils/tests/test_multiclass.py .....s..[36m                          [ 98%][0m
sklearn/utils/tests/test_murmurhash.py ......[36m                            [ 98%][0m
sklearn/utils/tests/test_optimize.py .[36m                                   [ 98%][0m
sklearn/utils/tests/test_random.py ....[36m                                  [ 98%][0m
sklearn/utils/tests/test_seq_dataset.py ..[36m                               [ 98%][0m
sklearn/utils/tests/test_shortest_path.py ....[36m                           [ 98%][0m
sklearn/utils/tests/test_show_versions.py ...[36m                            [ 98%][0m
sklearn/utils/tests/test_sparsefuncs.py ................................[36m [ 98%]
[0m.........[36m                                                                [ 98%][0m
sklearn/utils/tests/test_stats.py .....[36m                                  [ 99%][0m
sklearn/utils/tests/test_testing.py ...........s..[36m                       [ 99%][0m
sklearn/utils/tests/test_utils.py ..........s.......................[36m     [ 99%][0m
sklearn/utils/tests/test_validation.py .................................[36m [ 99%]
[0m..............s.s...[36m                                                     [100%][0m

=================================== FAILURES ===================================
[31m[1m____________________ test_classification_toy[deviance-auto] ____________________[0m

presort = 'auto', loss = 'deviance'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_toy(presort, loss):[0m
[1m>       check_classification_toy(presort, loss)[0m

loss       = 'deviance'
presort    = 'auto'

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:70: in check_classification_toy
[1m    clf.fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46df8d6550>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46df205208>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46df8d6550>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________ test_classification_toy[deviance-True] ____________________[0m

presort = True, loss = 'deviance'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_toy(presort, loss):[0m
[1m>       check_classification_toy(presort, loss)[0m

loss       = 'deviance'
presort    = True

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:70: in check_classification_toy
[1m    clf.fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46df205828>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46df205080>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46df205828>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________ test_classification_toy[deviance-False] ____________________[0m

presort = False, loss = 'deviance'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_toy(presort, loss):[0m
[1m>       check_classification_toy(presort, loss)[0m

loss       = 'deviance'
presort    = False

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:70: in check_classification_toy
[1m    clf.fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46df1da7f0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46df1da5c0>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46df1da7f0>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________ test_classification_toy[exponential-auto] ___________________[0m

presort = 'auto', loss = 'exponential'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_toy(presort, loss):[0m
[1m>       check_classification_toy(presort, loss)[0m

loss       = 'exponential'
presort    = 'auto'

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:70: in check_classification_toy
[1m    clf.fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e0355e48>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.ExponentialLoss object at 0x7f46e0355048>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e0355e48>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________ test_classification_toy[exponential-True] ___________________[0m

presort = True, loss = 'exponential'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_toy(presort, loss):[0m
[1m>       check_classification_toy(presort, loss)[0m

loss       = 'exponential'
presort    = True

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:70: in check_classification_toy
[1m    clf.fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e031c320>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.ExponentialLoss object at 0x7f46e031c4a8>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e031c320>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________ test_classification_toy[exponential-False] __________________[0m

presort = False, loss = 'exponential'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_toy(presort, loss):[0m
[1m>       check_classification_toy(presort, loss)[0m

loss       = 'exponential'
presort    = False

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:70: in check_classification_toy
[1m    clf.fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e2600668>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.ExponentialLoss object at 0x7f46e26004a8>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e2600668>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________________ test_classifier_parameter_checks _______________________[0m

[1m    def test_classifier_parameter_checks():[0m
[1m        # Check input parameter validation for GradientBoostingClassifier.[0m
[1m        assert_raises(ValueError,[0m
[1m                      GradientBoostingClassifier(n_estimators=0).fit, X, y)[0m
[1m        assert_raises(ValueError,[0m
[1m                      GradientBoostingClassifier(n_estimators=-1).fit, X, y)[0m
[1m    [0m
[1m        assert_raises(ValueError,[0m
[1m                      GradientBoostingClassifier(learning_rate=0.0).fit, X, y)[0m
[1m        assert_raises(ValueError,[0m
[1m                      GradientBoostingClassifier(learning_rate=-1.0).fit, X, y)[0m
[1m    [0m
[1m        assert_raises(ValueError,[0m
[1m                      GradientBoostingClassifier(loss='foobar').fit, X, y)[0m
[1m    [0m
[1m        assert_raises(ValueError,[0m
[1m>                     GradientBoostingClassifier(min_samples_split=0.0).fit, X, y)[0m


[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:103: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/_unittest_backport.py[0m:204: in assertRaises
[1m    return context.handle('assertRaises', args, kwargs)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/_unittest_backport.py[0m:113: in handle
[1m    callable_obj(*args, **kwargs)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eeb6a860>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________ test_classification_synthetic[deviance-auto] _________________[0m

presort = 'auto', loss = 'deviance'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_synthetic(presort, loss):[0m
[1m>       check_classification_synthetic(presort, loss)[0m

loss       = 'deviance'
presort    = 'auto'

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:234: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:217: in check_classification_synthetic
[1m    gbrt.fit(X_train, y_train)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
y = array([1, 1, 0, ..., 0, 0, 1])
y_pred = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eeb6f400>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eeb6f780>
monitor    = None
n_inbag    = 2000
n_samples  = 2000
random_state = <mtrand.RandomState object at 0x7f46eeb6f400>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, ..., 0, 0, 1])
y_pred     = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________ test_classification_synthetic[deviance-True] _________________[0m

presort = True, loss = 'deviance'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_synthetic(presort, loss):[0m
[1m>       check_classification_synthetic(presort, loss)[0m

loss       = 'deviance'
presort    = True

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:234: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:217: in check_classification_synthetic
[1m    gbrt.fit(X_train, y_train)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
y = array([1, 1, 0, ..., 0, 0, 1])
y_pred = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eedfd550>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eedfd3c8>
monitor    = None
n_inbag    = 2000
n_samples  = 2000
random_state = <mtrand.RandomState object at 0x7f46eedfd550>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, ..., 0, 0, 1])
y_pred     = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m________________ test_classification_synthetic[deviance-False] _________________[0m

presort = False, loss = 'deviance'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_synthetic(presort, loss):[0m
[1m>       check_classification_synthetic(presort, loss)[0m

loss       = 'deviance'
presort    = False

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:234: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:217: in check_classification_synthetic
[1m    gbrt.fit(X_train, y_train)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
y = array([1, 1, 0, ..., 0, 0, 1])
y_pred = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee0d550>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eedfa630>
monitor    = None
n_inbag    = 2000
n_samples  = 2000
random_state = <mtrand.RandomState object at 0x7f46eee0d550>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, ..., 0, 0, 1])
y_pred     = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________ test_classification_synthetic[exponential-auto] ________________[0m

presort = 'auto', loss = 'exponential'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_synthetic(presort, loss):[0m
[1m>       check_classification_synthetic(presort, loss)[0m

loss       = 'exponential'
presort    = 'auto'

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:234: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:217: in check_classification_synthetic
[1m    gbrt.fit(X_train, y_train)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='exponential', m...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
y = array([1, 1, 0, ..., 0, 0, 1])
y_pred = array([[ 0.00300001],
       [ 0.00300001],
       [ 0.00300001],
       ..., 
       [ 0.00300001],
       [ 0.00300001],
       [ 0.00300001]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee25240>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.ExponentialLoss object at 0x7f46eee73550>
monitor    = None
n_inbag    = 2000
n_samples  = 2000
random_state = <mtrand.RandomState object at 0x7f46eee25240>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='exponential', m...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, ..., 0, 0, 1])
y_pred     = array([[ 0.00300001],
       [ 0.00300001],
       [ 0.00300001],
       ..., 
       [ 0.00300001],
       [ 0.00300001],
       [ 0.00300001]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________ test_classification_synthetic[exponential-True] ________________[0m

presort = True, loss = 'exponential'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_synthetic(presort, loss):[0m
[1m>       check_classification_synthetic(presort, loss)[0m

loss       = 'exponential'
presort    = True

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:234: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:217: in check_classification_synthetic
[1m    gbrt.fit(X_train, y_train)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='exponential', m...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
y = array([1, 1, 0, ..., 0, 0, 1])
y_pred = array([[ 0.00300001],
       [ 0.00300001],
       [ 0.00300001],
       ..., 
       [ 0.00300001],
       [ 0.00300001],
       [ 0.00300001]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee80ef0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.ExponentialLoss object at 0x7f46eee80f60>
monitor    = None
n_inbag    = 2000
n_samples  = 2000
random_state = <mtrand.RandomState object at 0x7f46eee80ef0>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='exponential', m...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, ..., 0, 0, 1])
y_pred     = array([[ 0.00300001],
       [ 0.00300001],
       [ 0.00300001],
       ..., 
       [ 0.00300001],
       [ 0.00300001],
       [ 0.00300001]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________ test_classification_synthetic[exponential-False] _______________[0m

presort = False, loss = 'exponential'

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('deviance', 'exponential'))[0m
[1m    def test_classification_synthetic(presort, loss):[0m
[1m>       check_classification_synthetic(presort, loss)[0m

loss       = 'exponential'
presort    = False

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:234: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:217: in check_classification_synthetic
[1m    gbrt.fit(X_train, y_train)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='exponential', m...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
y = array([1, 1, 0, ..., 0, 0, 1])
y_pred = array([[ 0.00300001],
       [ 0.00300001],
       [ 0.00300001],
       ..., 
       [ 0.00300001],
       [ 0.00300001],
       [ 0.00300001]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee97cc0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.ExponentialLoss object at 0x7f46eee979e8>
monitor    = None
n_inbag    = 2000
n_samples  = 2000
random_state = <mtrand.RandomState object at 0x7f46eee97cc0>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=1.0, loss='exponential', m...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, ..., 0, 0, 1])
y_pred     = array([[ 0.00300001],
       [ 0.00300001],
       [ 0.00300001],
       ..., 
       [ 0.00300001],
       [ 0.00300001],
       [ 0.00300001]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_boston[1.0-ls-auto] ___________________________[0m

presort = 'auto', loss = 'ls', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'ls'
presort    = 'auto'
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee5cb00>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee5cf60>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46eee5cb00>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_boston[1.0-ls-True] ___________________________[0m

presort = True, loss = 'ls', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'ls'
presort    = True
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e257d278>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e257d3c8>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e257d278>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_boston[1.0-ls-False] ___________________________[0m

presort = False, loss = 'ls', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'ls'
presort    = False
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e271f2e8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e271f780>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e271f2e8>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_boston[1.0-lad-auto] ___________________________[0m

presort = 'auto', loss = 'lad', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'lad'
presort    = 'auto'
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e274cef0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastAbsoluteError object at 0x7f46e274cb70>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e274cef0>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_boston[1.0-lad-True] ___________________________[0m

presort = True, loss = 'lad', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'lad'
presort    = True
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eeb820b8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastAbsoluteError object at 0x7f46eeb82550>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46eeb820b8>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_boston[1.0-lad-False] __________________________[0m

presort = False, loss = 'lad', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'lad'
presort    = False
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e2411390>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastAbsoluteError object at 0x7f46e24112e8>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e2411390>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_boston[1.0-huber-auto] __________________________[0m

presort = 'auto', loss = 'huber', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'huber'
presort    = 'auto'
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46df9c7240>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.HuberLossFunction object at 0x7f46df9c72e8>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46df9c7240>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_boston[1.0-huber-True] __________________________[0m

presort = True, loss = 'huber', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'huber'
presort    = True
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e272e208>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.HuberLossFunction object at 0x7f46e272e0b8>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e272e208>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_boston[1.0-huber-False] _________________________[0m

presort = False, loss = 'huber', subsample = 1.0

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'huber'
presort    = False
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e23c65c0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.HuberLossFunction object at 0x7f46e23c6160>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e23c65c0>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_boston[0.5-ls-auto] ___________________________[0m

presort = 'auto', loss = 'ls', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'ls'
presort    = 'auto'
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee55b00>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee55a20>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46eee55b00>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_boston[0.5-ls-True] ___________________________[0m

presort = True, loss = 'ls', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'ls'
presort    = True
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e2402e10>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e2402ba8>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e2402e10>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_boston[0.5-ls-False] ___________________________[0m

presort = False, loss = 'ls', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'ls'
presort    = False
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e240f550>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e240f3c8>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e240f550>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_boston[0.5-lad-auto] ___________________________[0m

presort = 'auto', loss = 'lad', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'lad'
presort    = 'auto'
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eeb7ca20>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastAbsoluteError object at 0x7f46eeb7c518>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46eeb7ca20>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_boston[0.5-lad-True] ___________________________[0m

presort = True, loss = 'lad', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'lad'
presort    = True
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eeb8e828>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastAbsoluteError object at 0x7f46eeb8e0b8>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46eeb8e828>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_boston[0.5-lad-False] __________________________[0m

presort = False, loss = 'lad', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'lad'
presort    = False
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46dfb74d68>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastAbsoluteError object at 0x7f46dfb74438>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46dfb74d68>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='lad', ...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_boston[0.5-huber-auto] __________________________[0m

presort = 'auto', loss = 'huber', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'huber'
presort    = 'auto'
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=0.5, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e0153390>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.HuberLossFunction object at 0x7f46e01530f0>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e0153390>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=0.5, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_boston[0.5-huber-True] __________________________[0m

presort = True, loss = 'huber', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'huber'
presort    = True
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=0.5, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e25a1780>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.HuberLossFunction object at 0x7f46e25a1d68>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e25a1780>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=0.5, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_boston[0.5-huber-False] _________________________[0m

presort = False, loss = 'huber', subsample = 0.5

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('loss', ('ls', 'lad', 'huber'))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    def test_boston(presort, loss, subsample):[0m
[1m>       check_boston(presort, loss, subsample)[0m

loss       = 'huber'
presort    = False
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:271: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:253: in check_boston
[1m    sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=0.5, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46dfb48358>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.HuberLossFunction object at 0x7f46dfb48be0>
monitor    = None
n_inbag    = 253
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46dfb48358>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=1,
             subsample=0.5, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_iris[None-1.0-auto] ___________________________[0m

presort = 'auto', subsample = 1.0, sample_weight = None

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = 'auto'
sample_weight = None
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eeb70828>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46eeb70f28>
monitor    = None
n_inbag    = 150
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46eeb70828>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_iris[None-1.0-True] ___________________________[0m

presort = True, subsample = 1.0, sample_weight = None

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = True
sample_weight = None
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e0111208>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e01112e8>
monitor    = None
n_inbag    = 150
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e0111208>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_iris[None-1.0-False] ___________________________[0m

presort = False, subsample = 1.0, sample_weight = None

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = False
sample_weight = None
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e00b1390>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e00b16a0>
monitor    = None
n_inbag    = 150
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e00b1390>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_iris[None-0.5-auto] ___________________________[0m

presort = 'auto', subsample = 0.5, sample_weight = None

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = 'auto'
sample_weight = None
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e00a08d0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e00a0278>
monitor    = None
n_inbag    = 75
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e00a08d0>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_iris[None-0.5-True] ___________________________[0m

presort = True, subsample = 0.5, sample_weight = None

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = True
sample_weight = None
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eeb6a240>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46eeb6a400>
monitor    = None
n_inbag    = 75
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46eeb6a240>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_iris[None-0.5-False] ___________________________[0m

presort = False, subsample = 0.5, sample_weight = None

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = False
sample_weight = None
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee25d30>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46eee25400>
monitor    = None
n_inbag    = 75
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46eee25d30>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________________ test_iris[1-1.0-auto] _____________________________[0m

presort = 'auto', subsample = 1.0
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = 'auto'
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
random_state = <mtrand.RandomState object at 0x7f46eee1ed30>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46df931048>
monitor    = None
n_inbag    = 150
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46eee1ed30>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________________ test_iris[1-1.0-True] _____________________________[0m

presort = True, subsample = 1.0
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = True
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
random_state = <mtrand.RandomState object at 0x7f46e253dfd0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e253dd68>
monitor    = None
n_inbag    = 150
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e253dfd0>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________________ test_iris[1-1.0-False] ____________________________[0m

presort = False, subsample = 1.0
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = False
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
subsample  = 1.0

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
random_state = <mtrand.RandomState object at 0x7f46e0095a90>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e00953c8>
monitor    = None
n_inbag    = 150
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e0095a90>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________________ test_iris[1-0.5-auto] _____________________________[0m

presort = 'auto', subsample = 0.5
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = 'auto'
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
random_state = <mtrand.RandomState object at 0x7f46e02121d0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e0212128>
monitor    = None
n_inbag    = 75
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e02121d0>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________________ test_iris[1-0.5-True] _____________________________[0m

presort = True, subsample = 0.5
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = True
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
random_state = <mtrand.RandomState object at 0x7f46df8e6978>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46df8e6160>
monitor    = None
n_inbag    = 75
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46df8e6978>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________________ test_iris[1-0.5-False] ____________________________[0m

presort = False, subsample = 0.5
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])

[1m    @pytest.mark.parametrize('presort', ('auto', True, False))[0m
[1m    @pytest.mark.parametrize('subsample', (1.0, 0.5))[0m
[1m    @pytest.mark.parametrize('sample_weight', (None, 1))[0m
[1m    def test_iris(presort, subsample, sample_weight):[0m
[1m        if sample_weight == 1:[0m
[1m            sample_weight = np.ones(len(iris.target))[0m
[1m>       check_iris(presort, subsample, sample_weight)[0m

presort    = False
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
subsample  = 0.5

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:295: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:281: in check_iris
[1m    clf.fit(iris.data, iris.target, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
random_state = <mtrand.RandomState object at 0x7f46e01cf710>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = None

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = None
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e01cf5f8>
monitor    = None
n_inbag    = 75
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e01cf710>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,... 1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.])
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_regression_synthetic ___________________________[0m

[1m    def test_regression_synthetic():[0m
[1m        # Test on synthetic regression datasets used in Leo Breiman,[0m
[1m        # `Bagging Predictors?. Machine Learning 24(2): 123-140 (1996).[0m
[1m        random_state = check_random_state(1)[0m
[1m        regression_params = {'n_estimators': 100, 'max_depth': 4,[0m
[1m                             'min_samples_split': 2, 'learning_rate': 0.1,[0m
[1m                             'loss': 'ls'}[0m
[1m    [0m
[1m        # Friedman1[0m
[1m        X, y = datasets.make_friedman1(n_samples=1200,[0m
[1m                                       random_state=random_state,[0m
[1m                                       noise=1.0)[0m
[1m        X_train, y_train = X[:200], y[:200][0m
[1m        X_test, y_test = X[200:], y[200:][0m
[1m    [0m
[1m        for presort in True, False:[0m
[1m            clf = GradientBoostingRegressor(presort=presort)[0m
[1m>           clf.fit(X_train, y_train)[0m

X          = array([[  4.17022005e-01,   7.20324493e-01,   1.14374817e-04, ...,
          3.45560727e-01,   3.96767474e-01,   5.388....96046772e-01,   2.95963082e-01,   2.99945331e-01, ...,
          3.23980416e-01,   3.26597613e-01,   6.86266528e-01]])
X_test     = array([[ 0.57697785,  0.87538874,  0.60856544, ...,  0.1844956 ,
         0.50989838,  0.3437881 ],
       [ 0.7697253...114,  0.81966456],
       [ 0.49604677,  0.29596308,  0.29994533, ...,  0.32398042,
         0.32659761,  0.68626653]])
X_train    = array([[  4.17022005e-01,   7.20324493e-01,   1.14374817e-04, ...,
          3.45560727e-01,   3.96767474e-01,   5.388....64144095e-01,   4.47898916e-01,   5.61786261e-01, ...,
          8.28732852e-01,   3.09979598e-02,   9.46728270e-01]])
clf        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
presort    = True
random_state = <mtrand.RandomState object at 0x7f46e257dc18>
regression_params = {'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 4, 'min_samples_split': 2, ...}
y          = array([ 16.94688724,  17.97503416,  18.09898529, ...,  21.417506  ,
        24.0456661 ,  11.03134585])
y_test     = array([ 14.9304222 ,  11.42752625,  23.59558797,  16.77821051,
        16.68660701,  16.18482429,   3.51605112,   6.98... 21.09621828,   9.23474961,  16.24493898,  12.77876724,
        22.21991454,  21.417506  ,  24.0456661 ,  11.03134585])
y_train    = array([ 16.94688724,  17.97503416,  18.09898529,  13.67329165,
        16.27066519,   7.38545428,   5.42617075,  15.99... 15.69964998,  16.75243671,   6.30805517,  12.74429545,
        22.37167885,   9.42836419,  18.35192473,  21.57225672])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:315: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  4.17021990e-01,   7.20324516e-01,   1.14374816e-04, ...,
          3.45560730e-01,   3.96767467e-01,   5.388...  4.47898924e-01,   5.61786234e-01, ...,
          8.28732848e-01,   3.09979599e-02,   9.46728289e-01]], dtype=float32)
y = array([ 16.94688724,  17.97503416,  18.09898529,  13.67329165,
        16.27066519,   7.38545428,   5.42617075,  15.99... 15.69964998,  16.75243671,   6.30805517,  12.74429545,
        22.37167885,   9.42836419,  18.35192473,  21.57225672])
y_pred = array([[ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
   ...7],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 25,  44,   0, ...,  67,   9, 123],
       [160, 112, 110, ...,  48,  53,  56],
       [ 69,  12, 189, ..., 14..., 121,  15, 120],
       [ 96,  25, 151, ..., 120,  29,  51],
       [137,  17, 112, ..., 189,  54,  10]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  4.17021990e-01,   7.20324516e-01,   1.14374816e-04, ...,
          3.45560730e-01,   3.96767467e-01,   5.388...  4.47898924e-01,   5.61786234e-01, ...,
          8.28732848e-01,   3.09979599e-02,   9.46728289e-01]], dtype=float32)
X_idx_sorted = array([[ 25,  44,   0, ...,  67,   9, 123],
       [160, 112, 110, ...,  48,  53,  56],
       [ 69,  12, 189, ..., 14..., 121,  15, 120],
       [ 96,  25, 151, ..., 120,  29,  51],
       [137,  17, 112, ..., 189,  54,  10]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eeb8bba8>
monitor    = None
n_inbag    = 200
n_samples  = 200
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 16.94688724,  17.97503416,  18.09898529,  13.67329165,
        16.27066519,   7.38545428,   5.42617075,  15.99... 15.69964998,  16.75243671,   6.30805517,  12.74429545,
        22.37167885,   9.42836419,  18.35192473,  21.57225672])
y_pred     = array([[ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
   ...7],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_feature_importances ___________________________[0m

[1m    def test_feature_importances():[0m
[1m        X = np.array(boston.data, dtype=np.float32)[0m
[1m        y = np.array(boston.target, dtype=np.float32)[0m
[1m    [0m
[1m        for presort in True, False:[0m
[1m            clf = GradientBoostingRegressor(n_estimators=100, max_depth=5,[0m
[1m                                            min_samples_split=2, random_state=1,[0m
[1m                                            presort=presort)[0m
[1m>           clf.fit(X, y)[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
clf        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
presort    = True
y          = array([ 22.60000038,  50.        ,  23.        ,   8.30000019,
        21.20000076,  19.89999962,  20.60000038,  18.70...    ,
        24.79999924,  18.5       ,  36.40000153,  19.20000076,
        16.60000038,  23.10000038], dtype=float32)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:352: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.60000038,  50.        ,  23.        ,   8.30000019,
        21.20000076,  19.89999962,  20.60000038,  18.70...    ,
        24.79999924,  18.5       ,  36.40000153,  19.20000076,
        16.60000038,  23.10000038], dtype=float32)
y_pred = array([[ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [...328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e47f95f8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e47f9470>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e47f95f8>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.60000038,  50.        ,  23.        ,   8.30000019,
        21.20000076,  19.89999962,  20.60000038,  18.70...    ,
        24.79999924,  18.5       ,  36.40000153,  19.20000076,
        16.60000038,  23.10000038], dtype=float32)
y_pred     = array([[ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [...328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064],
       [ 22.5328064]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_____________________________ test_probability_log _____________________________[0m

[1m    def test_probability_log():[0m
[1m        # Predict probabilities.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1)[0m
[1m    [0m
[1m        assert_raises(ValueError, clf.predict_proba, T)[0m
[1m    [0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e02891d0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e0289668>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e02891d0>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_check_inputs_predict ___________________________[0m

[1m    def test_check_inputs_predict():[0m
[1m        # X has wrong shape[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1)[0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:394: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e00a7828>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e00a7a20>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e00a7828>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________________ test_check_inputs_predict_stages _______________________[0m

[1m    def test_check_inputs_predict_stages():[0m
[1m        # check that predict_stages through an error if the type of X is not[0m
[1m        # supported[0m
[1m        x, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        x_sparse_csc = csc_matrix(x)[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1)[0m
[1m>       clf.fit(x, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
x          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
x_sparse_csc = <100x10 sparse matrix of type '<class 'numpy.float64'>'
	with 1000 stored elements in Compressed Sparse Column format>
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:424: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46df900eb8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46df900ba8>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46df900eb8>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_check_max_features ____________________________[0m

[1m    def test_check_max_features():[0m
[1m        # test if max_features is valid.[0m
[1m        clf = GradientBoostingRegressor(n_estimators=100, random_state=1,[0m
[1m                                        max_features=0)[0m
[1m>       assert_raises(ValueError, clf.fit, X, y)[0m

clf        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:441: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/_unittest_backport.py[0m:204: in assertRaises
[1m    return context.handle('assertRaises', args, kwargs)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/_unittest_backport.py[0m:113: in handle
[1m    callable_obj(*args, **kwargs)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([-1, -1, -1,  1,  1,  1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee5ce80>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46df8f0438>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46eee5ce80>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([-1, -1, -1,  1,  1,  1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_max_feature_regression __________________________[0m

[1m    def test_max_feature_regression():[0m
[1m        # Test to make sure random state is set properly.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)[0m
[1m    [0m
[1m        X_train, X_test = X[:2000], X[2000:][0m
[1m        y_train, y_test = y[:2000], y[2000:][0m
[1m    [0m
[1m        gbrt = GradientBoostingClassifier(n_estimators=100, min_samples_split=5,[0m
[1m                                          max_depth=2, learning_rate=.1,[0m
[1m                                          max_features=2, random_state=1)[0m
[1m>       gbrt.fit(X_train, y_train)[0m

X          = array([[ 1.62434536, -0.61175641, -0.52817175, ..., -0.7612069 ,
         0.3190391 , -0.24937038],
       [ 1.4621079...785,  2.51028547],
       [ 0.82776805,  2.04855517,  2.77822335, ...,  0.12579842,
        -0.1916412 ,  0.67553921]])
X_test     = array([[-1.93156389, -0.16042612, -0.83221954, ..., -0.11040249,
        -0.60073243,  1.13414588],
       [-1.1072485...785,  2.51028547],
       [ 0.82776805,  2.04855517,  2.77822335, ...,  0.12579842,
        -0.1916412 ,  0.67553921]])
X_train    = array([[ 1.62434536, -0.61175641, -0.52817175, ..., -0.7612069 ,
         0.3190391 , -0.24937038],
       [ 1.4621079...731, -0.55719058],
       [ 0.38396418, -0.81877813, -2.12462153, ...,  2.4084338 ,
         0.88278555, -0.09959631]])
gbrt       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., ..., -1.,  1.,  1.])
y_test     = array([-1., -1.,  1., ..., -1.,  1.,  1.])
y_train    = array([ 1.,  1., -1., ..., -1., -1.,  1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:462: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
y = array([1, 1, 0, ..., 0, 0, 1])
y_pred = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e032f5c0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e032ff28>
monitor    = None
n_inbag    = 2000
n_samples  = 2000
random_state = <mtrand.RandomState object at 0x7f46e032f5c0>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, ..., 0, 0, 1])
y_pred     = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________________ test_feature_importance_regression ______________________[0m

[1m    def test_feature_importance_regression():[0m
[1m        """Test that Gini importance is calculated correctly.[0m
[1m    [0m
[1m        This test follows the example from [1]_ (pg. 373).[0m
[1m    [0m
[1m        .. [1] Friedman, J., Hastie, T., & Tibshirani, R. (2001). The elements[0m
[1m           of statistical learning. New York: Springer series in statistics.[0m
[1m        """[0m
[1m        california = fetch_california_housing()[0m
[1m        X, y = california.data, california.target[0m
[1m        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)[0m
[1m    [0m
[1m        reg = GradientBoostingRegressor(loss='huber', learning_rate=0.1,[0m
[1m                                        max_leaf_nodes=6, n_estimators=100,[0m
[1m                                        random_state=0)[0m
[1m>       reg.fit(X_train, y_train)[0m

X          = array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,
          37.88      , -122.23      ],
      ...    ],
       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,
          39.37      , -121.24      ]])
X_test     = array([[   4.1518    ,   22.        ,    5.66307278, ...,    4.18059299,
          32.58      , -117.05      ],
      ...    ],
       [   4.7639    ,   36.        ,    5.26181818, ...,    2.90545455,
          37.66      , -122.44      ]])
X_train    = array([[   7.3003    ,   19.        ,    7.9616    , ...,    3.0816    ,
          38.46      , -122.68      ],
      ...    ],
       [   1.3882    ,   15.        ,    3.9295302 , ...,    3.43624161,
          32.8       , -115.56      ]])
california = {'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,
          37.88      , -122.23      ...), 'feature_names': ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']}
reg        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=0,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 4.526,  3.585,  3.521, ...,  0.923,  0.847,  0.894])
y_test     = array([ 1.369,  2.413,  2.007, ...,  1.609,  2.273,  2.656])
y_train    = array([ 3.813,  3.293,  1.854, ...,  2.893,  4.846,  0.694])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:482: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=0,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[   7.30030012,   19.        ,    7.96159983, ...,    3.08159995,
          38.45999908, -122.68000031],
      ...   1.38820004,   15.        ,    3.92953014, ...,    3.43624163,
          32.79999924, -115.55999756]], dtype=float32)
y = array([ 3.813,  3.293,  1.854, ...,  2.893,  4.846,  0.694])
y_pred = array([[ 1.808],
       [ 1.808],
       [ 1.808],
       ..., 
       [ 1.808],
       [ 1.808],
       [ 1.808]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e2549a58>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 2895,  5434,  2400, ...,  6012,  1263,  6747],
       [ 9247, 11056, 12808, ...,  8737,  4207, 13570],
      ... [  298,  2470, 10831, ...,  3615,   660, 10060],
       [12775,  4268,   488, ...,  9006, 10296,  7749]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[   7.30030012,   19.        ,    7.96159983, ...,    3.08159995,
          38.45999908, -122.68000031],
      ...   1.38820004,   15.        ,    3.92953014, ...,    3.43624163,
          32.79999924, -115.55999756]], dtype=float32)
X_idx_sorted = array([[ 2895,  5434,  2400, ...,  6012,  1263,  6747],
       [ 9247, 11056, 12808, ...,  8737,  4207, 13570],
      ... [  298,  2470, 10831, ...,  3615,   660, 10060],
       [12775,  4268,   488, ...,  9006, 10296,  7749]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.HuberLossFunction object at 0x7f46e2413550>
monitor    = None
n_inbag    = 15480
n_samples  = 15480
random_state = <mtrand.RandomState object at 0x7f46e2549a58>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='huber'...dom_state=0,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 3.813,  3.293,  1.854, ...,  2.893,  4.846,  0.694])
y_pred     = array([[ 1.808],
       [ 1.808],
       [ 1.808],
       ..., 
       [ 1.808],
       [ 1.808],
       [ 1.808]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
----------------------------- Captured stderr call -----------------------------
Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/travis/scikit_learn_data
------------------------------ Captured log call -------------------------------
california_housing.py      114 INFO     Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /home/travis/scikit_learn_data
[31m[1m____________________________ test_max_feature_auto _____________________________[0m

[1m    def test_max_feature_auto():[0m
[1m        # Test if max features is set properly for floats and str.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)[0m
[1m        _, n_features = X.shape[0m
[1m    [0m
[1m        X_train = X[:2000][0m
[1m        y_train = y[:2000][0m
[1m    [0m
[1m        gbrt = GradientBoostingClassifier(n_estimators=1, max_features='auto')[0m
[1m>       gbrt.fit(X_train, y_train)[0m

X          = array([[ 1.62434536, -0.61175641, -0.52817175, ..., -0.7612069 ,
         0.3190391 , -0.24937038],
       [ 1.4621079...785,  2.51028547],
       [ 0.82776805,  2.04855517,  2.77822335, ...,  0.12579842,
        -0.1916412 ,  0.67553921]])
X_train    = array([[ 1.62434536, -0.61175641, -0.52817175, ..., -0.7612069 ,
         0.3190391 , -0.24937038],
       [ 1.4621079...731, -0.55719058],
       [ 0.38396418, -0.81877813, -2.12462153, ...,  2.4084338 ,
         0.88278555, -0.09959631]])
_          = 12000
gbrt       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
n_features = 10
y          = array([ 1.,  1., -1., ..., -1.,  1.,  1.])
y_train    = array([ 1.,  1., -1., ..., -1., -1.,  1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:504: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
y = array([1, 1, 0, ..., 0, 0, 1])
y_pred = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079... ],
       [ 0.38396418, -0.81877816, -2.12462163, ...,  2.40843391,
         0.88278556, -0.09959631]], dtype=float32)
X_idx_sorted = array([[ 955, 1312, 1406, ..., 1738,  740,  800],
       [1521, 1050,   89, ..., 1173,  226, 1099],
       [1037, 1240...292],
       [ 506, 1728,   73, ..., 1731,  534,  122],
       [ 807, 1106, 1943, ...,  895,  304, 1420]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e02da0f0>
monitor    = None
n_inbag    = 2000
n_samples  = 2000
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True, ...,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1., ...,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, ..., 0, 0, 1])
y_pred     = array([[ 0.00600002],
       [ 0.00600002],
       [ 0.00600002],
       ..., 
       [ 0.00600002],
       [ 0.00600002],
       [ 0.00600002]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_____________________________ test_staged_predict ______________________________[0m

[1m    def test_staged_predict():[0m
[1m        # Test whether staged decision function eventually gives[0m
[1m        # the same prediction.[0m
[1m        X, y = datasets.make_friedman1(n_samples=1200,[0m
[1m                                       random_state=1, noise=1.0)[0m
[1m        X_train, y_train = X[:200], y[:200][0m
[1m        X_test = X[200:][0m
[1m        clf = GradientBoostingRegressor()[0m
[1m        # test raise ValueError if not fitted[0m
[1m        assert_raises(ValueError, lambda X: np.fromiter([0m
[1m            clf.staged_predict(X), dtype=np.float64), X_test)[0m
[1m    [0m
[1m>       clf.fit(X_train, y_train)[0m

X          = array([[  4.17022005e-01,   7.20324493e-01,   1.14374817e-04, ...,
          3.45560727e-01,   3.96767474e-01,   5.388....96046772e-01,   2.95963082e-01,   2.99945331e-01, ...,
          3.23980416e-01,   3.26597613e-01,   6.86266528e-01]])
X_test     = array([[ 0.57697785,  0.87538874,  0.60856544, ...,  0.1844956 ,
         0.50989838,  0.3437881 ],
       [ 0.7697253...114,  0.81966456],
       [ 0.49604677,  0.29596308,  0.29994533, ...,  0.32398042,
         0.32659761,  0.68626653]])
X_train    = array([[  4.17022005e-01,   7.20324493e-01,   1.14374817e-04, ...,
          3.45560727e-01,   3.96767474e-01,   5.388....64144095e-01,   4.47898916e-01,   5.61786261e-01, ...,
          8.28732852e-01,   3.09979598e-02,   9.46728270e-01]])
clf        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 16.94688724,  17.97503416,  18.09898529, ...,  21.417506  ,
        24.0456661 ,  11.03134585])
y_train    = array([ 16.94688724,  17.97503416,  18.09898529,  13.67329165,
        16.27066519,   7.38545428,   5.42617075,  15.99... 15.69964998,  16.75243671,   6.30805517,  12.74429545,
        22.37167885,   9.42836419,  18.35192473,  21.57225672])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:541: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  4.17021990e-01,   7.20324516e-01,   1.14374816e-04, ...,
          3.45560730e-01,   3.96767467e-01,   5.388...  4.47898924e-01,   5.61786234e-01, ...,
          8.28732848e-01,   3.09979599e-02,   9.46728289e-01]], dtype=float32)
y = array([ 16.94688724,  17.97503416,  18.09898529,  13.67329165,
        16.27066519,   7.38545428,   5.42617075,  15.99... 15.69964998,  16.75243671,   6.30805517,  12.74429545,
        22.37167885,   9.42836419,  18.35192473,  21.57225672])
y_pred = array([[ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
   ...7],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 25,  44,   0, ...,  67,   9, 123],
       [160, 112, 110, ...,  48,  53,  56],
       [ 69,  12, 189, ..., 14..., 121,  15, 120],
       [ 96,  25, 151, ..., 120,  29,  51],
       [137,  17, 112, ..., 189,  54,  10]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  4.17021990e-01,   7.20324516e-01,   1.14374816e-04, ...,
          3.45560730e-01,   3.96767467e-01,   5.388...  4.47898924e-01,   5.61786234e-01, ...,
          8.28732848e-01,   3.09979599e-02,   9.46728289e-01]], dtype=float32)
X_idx_sorted = array([[ 25,  44,   0, ...,  67,   9, 123],
       [160, 112, 110, ...,  48,  53,  56],
       [ 69,  12, 189, ..., 14..., 121,  15, 120],
       [ 96,  25, 151, ..., 120,  29,  51],
       [137,  17, 112, ..., 189,  54,  10]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e008fb38>
monitor    = None
n_inbag    = 200
n_samples  = 200
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 16.94688724,  17.97503416,  18.09898529,  13.67329165,
        16.27066519,   7.38545428,   5.42617075,  15.99... 15.69964998,  16.75243671,   6.30805517,  12.74429545,
        22.37167885,   9.42836419,  18.35192473,  21.57225672])
y_pred     = array([[ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
   ...7],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757],
       [ 14.57129757]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_staged_predict_proba ___________________________[0m

[1m    def test_staged_predict_proba():[0m
[1m        # Test whether staged predict proba eventually gives[0m
[1m        # the same prediction.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=1200,[0m
[1m                                         random_state=1)[0m
[1m        X_train, y_train = X[:200], y[:200][0m
[1m        X_test, y_test = X[200:], y[200:][0m
[1m        clf = GradientBoostingClassifier(n_estimators=20)[0m
[1m        # test raise NotFittedError if not fitted[0m
[1m        assert_raises(NotFittedError, lambda X: np.fromiter([0m
[1m            clf.staged_predict_proba(X), dtype=np.float64), X_test)[0m
[1m    [0m
[1m>       clf.fit(X_train, y_train)[0m

X          = array([[ 1.62434536, -0.61175641, -0.52817175, ..., -0.7612069 ,
         0.3190391 , -0.24937038],
       [ 1.4621079...531, -1.6012152 ],
       [ 1.01535137, -0.37024721, -1.21949605, ..., -1.15020874,
         0.62787077, -0.77325477]])
X_test     = array([[ 0.48951662,  0.23879586, -0.44811181, ...,  0.15258149,
         0.50127485, -0.78640277],
       [ 1.0169956...531, -1.6012152 ],
       [ 1.01535137, -0.37024721, -1.21949605, ..., -1.15020874,
         0.62787077, -0.77325477]])
X_train    = array([[ 1.62434536, -0.61175641, -0.52817175, ..., -0.7612069 ,
         0.3190391 , -0.24937038],
       [ 1.4621079...992, -0.63379205],
       [-0.43076974,  0.94428473, -0.47287556, ..., -0.92165905,
         0.64737512,  1.38682559]])
clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., ...,  1., -1., -1.])
y_test     = array([-1.,  1.,  1., -1.,  1., -1.,  1., -1.,  1., -1.,  1., -1., -1.,
        1.,  1.,  1., -1., -1., -1., -1., -1.,..., -1.,  1., -1., -1.,  1.,  1.,  1., -1., -1., -1.,
        1., -1., -1.,  1., -1., -1.,  1.,  1.,  1.,  1., -1., -1.])
y_train    = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,... 1.,  1., -1.,
       -1., -1., -1., -1.,  1., -1., -1., -1.,  1., -1., -1., -1.,  1.,
        1.,  1.,  1.,  1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:563: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079...4],
       [-0.43076974,  0.94428474, -0.47287557, ..., -0.92165905,
         0.64737511,  1.38682556]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0])
y_pred = array([[ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.20...[ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104,  66,  89, ...,  36, 170, 148],
       [157, 158,  25, ...,  49,  73, 146],
       [ 79,  15, 153, ..., 19..., 102,   6, 170],
       [ 30,  41,  17, ...,   4,  16,  39],
       [144, 198,  73, ..., 141, 174, 122]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.62434542, -0.61175638, -0.52817178, ..., -0.76120692,
         0.31903911, -0.24937038],
       [ 1.4621079...4],
       [-0.43076974,  0.94428474, -0.47287557, ..., -0.92165905,
         0.64737511,  1.38682556]], dtype=float32)
X_idx_sorted = array([[104,  66,  89, ...,  36, 170, 148],
       [157, 158,  25, ...,  49,  73, 146],
       [ 79,  15, 153, ..., 19..., 102,   6, 170],
       [ 30,  41,  17, ...,   4,  16,  39],
       [144, 198,  73, ..., 141, 174, 122]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46dfb444e0>
monitor    = None
n_inbag    = 200
n_samples  = 200
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,... 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0])
y_pred     = array([[ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.20...[ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707],
       [ 0.2006707]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________ test_staged_functions_defensive[GradientBoostingClassifier] __________[0m

Estimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Estimator', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_staged_functions_defensive(Estimator):[0m
[1m        # test that staged_functions make defensive copies[0m
[1m        rng = np.random.RandomState(0)[0m
[1m        X = rng.uniform(size=(10, 3))[0m
[1m        y = (4 * X[:, 0]).astype(np.int) + 1  # don't predict zeros[0m
[1m        estimator = Estimator()[0m
[1m>       estimator.fit(X, y)[0m

Estimator  = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[ 0.5488135 ,  0.71518937,  0.60276338],
       [ 0.54488318,  0.4236548 ,  0.64589411],
       [ 0.43758721,  ...6147936,  0.78052918],
       [ 0.11827443,  0.63992102,  0.14335329],
       [ 0.94466892,  0.52184832,  0.41466194]])
estimator  = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
rng        = <mtrand.RandomState object at 0x7f46e00b0208>
y          = array([3, 3, 2, 2, 3, 1, 4, 4, 1, 4])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:586: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 0.54881352,  0.71518934,  0.60276335],
       [ 0.54488319,  0.42365479,  0.64589411],
       [ 0.4375872 ,  ...5292 ],
       [ 0.11827443,  0.63992101,  0.14335328],
       [ 0.94466889,  0.52184832,  0.41466194]], dtype=float32)
y = array([2, 2, 1, 1, 2, 0, 3, 3, 0, 3])
y_pred = array([[ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,...  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[5, 5, 4],
       [8, 1, 8],
       [3, 7, 9],
       [2, 9, 3],
       [1, 8, 0],
       [0, 0, 1],
       [4, 3, 7],
       [6, 6, 5],
       [7, 2, 2],
       [9, 4, 6]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.54881352,  0.71518934,  0.60276335],
       [ 0.54488319,  0.42365479,  0.64589411],
       [ 0.4375872 ,  ...5292 ],
       [ 0.11827443,  0.63992101,  0.14335328],
       [ 0.94466889,  0.52184832,  0.41466194]], dtype=float32)
X_idx_sorted = array([[5, 5, 4],
       [8, 1, 8],
       [3, 7, 9],
       [2, 9, 3],
       [1, 8, 0],
       [0, 0, 1],
       [4, 3, 7],
       [6, 6, 5],
       [7, 2, 2],
       [9, 4, 6]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e00b0438>
monitor    = None
n_inbag    = 10
n_samples  = 10
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([2, 2, 1, 1, 2, 0, 3, 3, 0, 3])
y_pred     = array([[ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,...  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3],
       [ 0.2,  0.2,  0.3,  0.3]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________ test_staged_functions_defensive[GradientBoostingRegressor] __________[0m

Estimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Estimator', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_staged_functions_defensive(Estimator):[0m
[1m        # test that staged_functions make defensive copies[0m
[1m        rng = np.random.RandomState(0)[0m
[1m        X = rng.uniform(size=(10, 3))[0m
[1m        y = (4 * X[:, 0]).astype(np.int) + 1  # don't predict zeros[0m
[1m        estimator = Estimator()[0m
[1m>       estimator.fit(X, y)[0m

Estimator  = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[ 0.5488135 ,  0.71518937,  0.60276338],
       [ 0.54488318,  0.4236548 ,  0.64589411],
       [ 0.43758721,  ...6147936,  0.78052918],
       [ 0.11827443,  0.63992102,  0.14335329],
       [ 0.94466892,  0.52184832,  0.41466194]])
estimator  = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
rng        = <mtrand.RandomState object at 0x7f46e2411048>
y          = array([3, 3, 2, 2, 3, 1, 4, 4, 1, 4])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:586: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[ 0.54881352,  0.71518934,  0.60276335],
       [ 0.54488319,  0.42365479,  0.64589411],
       [ 0.4375872 ,  ...5292 ],
       [ 0.11827443,  0.63992101,  0.14335328],
       [ 0.94466889,  0.52184832,  0.41466194]], dtype=float32)
y = array([3, 3, 2, 2, 3, 1, 4, 4, 1, 4])
y_pred = array([[ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[5, 5, 4],
       [8, 1, 8],
       [3, 7, 9],
       [2, 9, 3],
       [1, 8, 0],
       [0, 0, 1],
       [4, 3, 7],
       [6, 6, 5],
       [7, 2, 2],
       [9, 4, 6]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.54881352,  0.71518934,  0.60276335],
       [ 0.54488319,  0.42365479,  0.64589411],
       [ 0.4375872 ,  ...5292 ],
       [ 0.11827443,  0.63992101,  0.14335328],
       [ 0.94466889,  0.52184832,  0.41466194]], dtype=float32)
X_idx_sorted = array([[5, 5, 4],
       [8, 1, 8],
       [3, 7, 9],
       [2, 9, 3],
       [1, 8, 0],
       [0, 0, 1],
       [4, 3, 7],
       [6, 6, 5],
       [7, 2, 2],
       [9, 4, 6]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e24111d0>
monitor    = None
n_inbag    = 10
n_samples  = 10
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([3, 3, 2, 2, 3, 1, 4, 4, 1, 4])
y_pred     = array([[ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7],
       [ 2.7]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________________________ test_serialization ______________________________[0m

[1m    def test_serialization():[0m
[1m        # Check model serialization.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1)[0m
[1m    [0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:602: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee72c50>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eee72438>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46eee72c50>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_degenerate_targets ____________________________[0m

[1m    def test_degenerate_targets():[0m
[1m        # Check if we can fit even though all targets are equal.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1)[0m
[1m    [0m
[1m        # classifier should raise exception[0m
[1m        assert_raises(ValueError, clf.fit, X, np.ones(len(X)))[0m
[1m    [0m
[1m        clf = GradientBoostingRegressor(n_estimators=100, random_state=1)[0m
[1m>       clf.fit(X, np.ones(len(X)))[0m

clf        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:626: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([ 1.,  1.,  1.,  1.,  1.,  1.])
y_pred = array([[ 1.],
       [ 1.],
       [ 1.],
       [ 1.],
       [ 1.],
       [ 1.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee488d0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee48080>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46eee488d0>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1.,  1.,  1.,  1.,  1.])
y_pred     = array([[ 1.],
       [ 1.],
       [ 1.],
       [ 1.],
       [ 1.],
       [ 1.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________________________ test_quantile_loss ______________________________[0m

[1m    def test_quantile_loss():[0m
[1m        # Check if quantile loss with alpha=0.5 equals lad.[0m
[1m        clf_quantile = GradientBoostingRegressor(n_estimators=100, loss='quantile',[0m
[1m                                                 max_depth=4, alpha=0.5,[0m
[1m                                                 random_state=7)[0m
[1m    [0m
[1m>       clf_quantile.fit(boston.data, boston.target)[0m

clf_quantile = GradientBoostingRegressor(alpha=0.5, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='quanti...dom_state=7,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:638: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.5, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='quanti...dom_state=7,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e0207a20>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.QuantileLossFunction object at 0x7f46e0207e80>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e0207a20>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.5, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='quanti...dom_state=7,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
      ...21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2],
       [ 21.2]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________________________ test_symbol_labels ______________________________[0m

[1m    def test_symbol_labels():[0m
[1m        # Test with non-integer class labels.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1)[0m
[1m    [0m
[1m        symbol_y = tosequence(map(str, y))[0m
[1m    [0m
[1m>       clf.fit(X, symbol_y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
symbol_y   = ['-1', '-1', '-1', '1', '1', '1']

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:655: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e00ae550>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e00aeda0>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e00ae550>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_float_class_labels ____________________________[0m

[1m    def test_float_class_labels():[0m
[1m        # Test with float class labels.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1)[0m
[1m    [0m
[1m        float_y = np.asarray(y, dtype=np.float32)[0m
[1m    [0m
[1m>       clf.fit(X, float_y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
float_y    = array([-1., -1., -1.,  1.,  1.,  1.], dtype=float32)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:666: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e030aac8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e030aeb8>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e030aac8>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________________________ test_mem_layout ________________________________[0m

[1m    def test_mem_layout():[0m
[1m        # Test with different memory layouts of X and y[0m
[1m        X_ = np.asfortranarray(X)[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1)[0m
[1m>       clf.fit(X_, y)[0m

X_         = array([[-2, -1],
       [-1, -1],
       [-1, -2],
       [ 1,  1],
       [ 1,  2],
       [ 2,  1]])
clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:691: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e00bdfd0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e00bd470>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e00bdfd0>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_____________________________ test_oob_improvement _____________________________[0m

[1m    def test_oob_improvement():[0m
[1m        # Test if oob improvement has correct shape and regression test.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1,[0m
[1m                                         subsample=0.5)[0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:720: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46dfb3ed68>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46dfb3ec50>
monitor    = None
n_inbag    = 3
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46dfb3ed68>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________________ test_oob_improvement_raise __________________________[0m

[1m    def test_oob_improvement_raise():[0m
[1m        # Test if oob improvement has correct shape.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1,[0m
[1m                                         subsample=1.0)[0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:732: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e00870f0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e0087fd0>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e00870f0>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_oob_multilcass_iris ___________________________[0m

[1m    def test_oob_multilcass_iris():[0m
[1m        # Check OOB improvement on multi-class dataset.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, loss='deviance',[0m
[1m                                         random_state=1, subsample=0.5)[0m
[1m>       clf.fit(iris.data, iris.target)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:740: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e031c160>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e031c3c8>
monitor    = None
n_inbag    = 75
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e031c160>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_____________________________ test_verbose_output ______________________________[0m

[1m    def test_verbose_output():[0m
[1m        # Check verbose=1 does not cause error.[0m
[1m        from sklearn.externals.six.moves import cStringIO as StringIO[0m
[1m        import sys[0m
[1m        old_stdout = sys.stdout[0m
[1m        sys.stdout = StringIO()[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1,[0m
[1m                                         verbose=1, subsample=0.8)[0m
[1m>       clf.fit(X, y)[0m

StringIO   = <class '_io.StringIO'>
clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.8, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)
old_stdout = <_pytest.capture.EncodedFile object at 0x7f46fe43d470>
sys        = <module 'sys' (built-in)>

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:759: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.8, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46dfb71550>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46dfb71c18>
monitor    = None
n_inbag    = 4
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46dfb71550>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.8, tol=0.0001, validation_fraction=0.1,
              verbose=1, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_more_verbose_output ___________________________[0m

[1m    def test_more_verbose_output():[0m
[1m        # Check verbose=2 does not cause error.[0m
[1m        from sklearn.externals.six.moves import cStringIO as StringIO[0m
[1m        import sys[0m
[1m        old_stdout = sys.stdout[0m
[1m        sys.stdout = StringIO()[0m
[1m        clf = GradientBoostingClassifier(n_estimators=100, random_state=1,[0m
[1m                                         verbose=2)[0m
[1m>       clf.fit(X, y)[0m

StringIO   = <class '_io.StringIO'>
clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=2, warm_start=False)
old_stdout = <_pytest.capture.EncodedFile object at 0x7f46fe43d470>
sys        = <module 'sys' (built-in)>

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:784: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=2, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e0217470>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e0217b70>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e0217470>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=2, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________ test_warm_start[GradientBoostingClassifier] __________________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start(Cls):[0m
[1m        # Test if warm start equals fit.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=200, max_depth=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:806: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eeb7ca20>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________ test_warm_start[GradientBoostingRegressor] __________________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start(Cls):[0m
[1m        # Test if warm start equals fit.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=200, max_depth=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:806: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eeb6fbe0>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_warm_start_n_estimators[GradientBoostingClassifier] ___________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_n_estimators(Cls):[0m
[1m        # Test if warm start equals fit - set n_estimators.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=300, max_depth=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:828: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46dfb450f0>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_warm_start_n_estimators[GradientBoostingRegressor] ____________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_n_estimators(Cls):[0m
[1m        # Test if warm start equals fit - set n_estimators.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=300, max_depth=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:828: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee22ba8>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________ test_warm_start_max_depth[GradientBoostingClassifier] _____________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_max_depth(Cls):[0m
[1m        # Test if possible to fit trees of different depth in ensemble.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1, warm_start=True)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:843: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eeea3358>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_____________ test_warm_start_max_depth[GradientBoostingRegressor] _____________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_max_depth(Cls):[0m
[1m        # Test if possible to fit trees of different depth in ensemble.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1, warm_start=True)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:843: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee8a198>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________ test_warm_start_clear[GradientBoostingClassifier] _______________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_clear(Cls):[0m
[1m        # Test if fit clears state.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:858: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46dfb618d0>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________ test_warm_start_clear[GradientBoostingRegressor] _______________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_clear(Cls):[0m
[1m        # Test if fit clears state.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:858: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e008ba90>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m________ test_warm_start_zero_n_estimators[GradientBoostingClassifier] _________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_zero_n_estimators(Cls):[0m
[1m        # Test if warm start with zero n_estimators raises error[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1, warm_start=True)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:873: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e00b3860>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________ test_warm_start_zero_n_estimators[GradientBoostingRegressor] _________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_zero_n_estimators(Cls):[0m
[1m        # Test if warm start with zero n_estimators raises error[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1, warm_start=True)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:873: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46ef3ec588>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______ test_warm_start_smaller_n_estimators[GradientBoostingClassifier] _______[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_smaller_n_estimators(Cls):[0m
[1m        # Test if warm start with smaller n_estimators raises error[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1, warm_start=True)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:883: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eeb70780>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______ test_warm_start_smaller_n_estimators[GradientBoostingRegressor] ________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_smaller_n_estimators(Cls):[0m
[1m        # Test if warm start with smaller n_estimators raises error[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1, warm_start=True)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:883: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee40588>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m________ test_warm_start_equal_n_estimators[GradientBoostingClassifier] ________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_equal_n_estimators(Cls):[0m
[1m        # Test if warm start with equal n_estimators does nothing[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:893: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e5a45780>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m________ test_warm_start_equal_n_estimators[GradientBoostingRegressor] _________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_equal_n_estimators(Cls):[0m
[1m        # Test if warm start with equal n_estimators does nothing[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:893: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e0142ac8>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________ test_warm_start_oob_switch[GradientBoostingClassifier] ____________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_oob_switch(Cls):[0m
[1m        # Test if oob can be turned on during warm start.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1, warm_start=True)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e0092748>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________ test_warm_start_oob_switch[GradientBoostingRegressor] _____________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_oob_switch(Cls):[0m
[1m        # Test if oob can be turned on during warm start.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=100, max_depth=1, warm_start=True)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:907: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e0319be0>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...        random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________ test_warm_start_oob[GradientBoostingClassifier] ________________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_oob(Cls):[0m
[1m        # Test if warm start OOB equals fit.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=200, max_depth=1, subsample=0.5,[0m
[1m                  random_state=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:923: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46dfb69898>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46dfb69b00>
monitor    = None
n_inbag    = 50
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46dfb69898>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m________________ test_warm_start_oob[GradientBoostingRegressor] ________________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_oob(Cls):[0m
[1m        # Test if warm start OOB equals fit.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est = Cls(n_estimators=200, max_depth=1, subsample=0.5,[0m
[1m                  random_state=1)[0m
[1m>       est.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:923: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e0272eb8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e0272c50>
monitor    = None
n_inbag    = 50
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46e0272eb8>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________ test_warm_start_sparse[GradientBoostingClassifier] ______________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_sparse(Cls):[0m
[1m        # Test that all sparse matrix types are supported[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        sparse_matrix_type = [csr_matrix, csc_matrix, coo_matrix][0m
[1m        est_dense = Cls(n_estimators=100, max_depth=1, subsample=0.5,[0m
[1m                        random_state=1, warm_start=True)[0m
[1m>       est_dense.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est_dense  = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...om_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
sparse_matrix_type = [<class 'scipy.sparse.csr.csr_matrix'>, <class 'scipy.sparse.csc.csc_matrix'>, <class 'scipy.sparse.coo.coo_matrix'>]
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:942: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...om_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e009db70>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e009d8d0>
monitor    = None
n_inbag    = 50
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46e009db70>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...om_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________ test_warm_start_sparse[GradientBoostingRegressor] _______________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_sparse(Cls):[0m
[1m        # Test that all sparse matrix types are supported[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        sparse_matrix_type = [csr_matrix, csc_matrix, coo_matrix][0m
[1m        est_dense = Cls(n_estimators=100, max_depth=1, subsample=0.5,[0m
[1m                        random_state=1, warm_start=True)[0m
[1m>       est_dense.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est_dense  = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...           random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
sparse_matrix_type = [<class 'scipy.sparse.csr.csr_matrix'>, <class 'scipy.sparse.csc.csc_matrix'>, <class 'scipy.sparse.coo.coo_matrix'>]
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:942: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...           random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee69d30>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee69b00>
monitor    = None
n_inbag    = 50
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46eee69d30>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...           random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_____________ test_warm_start_fortran[GradientBoostingClassifier] ______________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_fortran(Cls):[0m
[1m        # Test that feeding a X in Fortran-ordered is giving the same results as[0m
[1m        # in C-ordered[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est_c = Cls(n_estimators=1, random_state=1, warm_start=True)[0m
[1m        est_fortran = Cls(n_estimators=1, random_state=1, warm_start=True)[0m
[1m    [0m
[1m>       est_c.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est_c      = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...om_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
est_fortran = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...om_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:972: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...om_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46df8d7390>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46df8d7e48>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46df8d7390>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...om_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________ test_warm_start_fortran[GradientBoostingRegressor] ______________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_warm_start_fortran(Cls):[0m
[1m        # Test that feeding a X in Fortran-ordered is giving the same results as[0m
[1m        # in C-ordered[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        est_c = Cls(n_estimators=1, random_state=1, warm_start=True)[0m
[1m        est_fortran = Cls(n_estimators=1, random_state=1, warm_start=True)[0m
[1m    [0m
[1m>       est_c.fit(X, y)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est_c      = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...           random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
est_fortran = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...           random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:972: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...           random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46df8f8b38>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46df8f8710>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46df8f8b38>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...           random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=True)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_monitor_early_stopping[GradientBoostingClassifier] ____________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_monitor_early_stopping(Cls):[0m
[1m        # Test if monitor return value works.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m    [0m
[1m        est = Cls(n_estimators=20, max_depth=1, random_state=1, subsample=0.5)[0m
[1m>       est.fit(X, y, monitor=early_stopping_monitor)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:998: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e032b0b8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0
monitor = <function early_stopping_monitor at 0x7f46e4bb2840>
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e032b4e0>
monitor    = <function early_stopping_monitor at 0x7f46e4bb2840>
n_inbag    = 50
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46e032b0b8>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=0.5, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________ test_monitor_early_stopping[GradientBoostingRegressor] ____________[0m

Cls = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('Cls', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_monitor_early_stopping(Cls):[0m
[1m        # Test if monitor return value works.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m    [0m
[1m        est = Cls(n_estimators=20, max_depth=1, random_state=1, subsample=0.5)[0m
[1m>       est.fit(X, y, monitor=early_stopping_monitor)[0m

Cls        = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:998: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e272fb70>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0
monitor = <function early_stopping_monitor at 0x7f46e4bb2840>
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = True
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e272fda0>
monitor    = <function early_stopping_monitor at 0x7f46e4bb2840>
n_inbag    = 50
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46e272fb70>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=0.5, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_complete_classification _________________________[0m

[1m    def test_complete_classification():[0m
[1m        # Test greedy trees with max_depth + 1 leafs.[0m
[1m        from sklearn.tree._tree import TREE_LEAF[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m        k = 4[0m
[1m    [0m
[1m        est = GradientBoostingClassifier(n_estimators=20, max_depth=None,[0m
[1m                                         random_state=1, max_leaf_nodes=k + 1)[0m
[1m>       est.fit(X, y)[0m

TREE_LEAF  = -1
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
k          = 4
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1036: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46dfb62080>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46dfb62b70>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46dfb62080>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_complete_regression ___________________________[0m

[1m    def test_complete_regression():[0m
[1m        # Test greedy trees with max_depth + 1 leafs.[0m
[1m        from sklearn.tree._tree import TREE_LEAF[0m
[1m        k = 4[0m
[1m    [0m
[1m        est = GradientBoostingRegressor(n_estimators=20, max_depth=None,[0m
[1m                                        random_state=1, max_leaf_nodes=k + 1)[0m
[1m>       est.fit(boston.data, boston.target)[0m

TREE_LEAF  = -1
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
k          = 4

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1051: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e0321b70>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e253da58>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46e0321b70>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...dom_state=1,
             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,
             warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_zero_estimator_reg ____________________________[0m

[1m    def test_zero_estimator_reg():[0m
[1m        # Test if ZeroEstimator works for regression.[0m
[1m        est = GradientBoostingRegressor(n_estimators=20, max_depth=1,[0m
[1m                                        random_state=1, init=ZeroEstimator())[0m
[1m>       est.fit(boston.data, boston.target)[0m

est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse',
             init=<sklearn.ensemble.gradient_boosting.Z...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1062: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse',
             init=<sklearn.ensemble.gradient_boosting.Z...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
y = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eeb8e470>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.72399998e-02,   0.00000000e+00,   3.24000001e+00, ...,
          1.68999996e+01,   3.75209991e+02,   7.340...  0.00000000e+00,   4.05000019e+00, ...,
          1.66000004e+01,   3.96899994e+02,   1.46899996e+01]], dtype=float32)
X_idx_sorted = array([[377,   0, 499, ..., 203, 437, 310],
       [248, 314,  97, ..., 471, 459,  14],
       [463, 313, 119, ...,  1..., 117, 351, 397],
       [ 10, 412, 484, ..., 280, 413, 307],
       [344, 426, 415, ..., 282, 505,  96]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eeb8e828>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46eeb8e470>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse',
             init=<sklearn.ensemble.gradient_boosting.Z...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 22.6,  50. ,  23. ,   8.3,  21.2,  19.9,  20.6,  18.7,  16.1,
        18.6,   8.8,  17.2,  14.9,  10.5,  50. ,... 24.2,  22.2,  27.9,  22.2,
        33.1,  19.3,  18.9,  22.6,  50. ,  24.8,  18.5,  36.4,  19.2,
        16.6,  23.1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________________________ test_zero_estimator_clf ____________________________[0m

[1m    def test_zero_estimator_clf():[0m
[1m        # Test if ZeroEstimator works for classification.[0m
[1m        X = iris.data[0m
[1m        y = np.array(iris.target)[0m
[1m        est = GradientBoostingClassifier(n_estimators=20, max_depth=1,[0m
[1m                                         random_state=1, init=ZeroEstimator())[0m
[1m>       est.fit(X, y)[0m

X          = array([[ 6.7,  3.1,  4.7,  1.5],
       [ 6.1,  2.9,  4.7,  1.4],
       [ 6.3,  2.5,  5. ,  1.9],
       [ 4.7,  3.2,...  2.9,  4.3,  1.3],
       [ 5.5,  2.5,  4. ,  1.3],
       [ 4.4,  3. ,  1.3,  0.2],
       [ 6.5,  3. ,  5.8,  2.2]])
est        = GradientBoostingClassifier(criterion='friedman_mse',
              init=<sklearn.ensemble.gradient_boosting.ZeroEstima...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1085: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse',
              init=<sklearn.ensemble.gradient_boosting.ZeroEstima...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
y = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred = array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]...      [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee55a90>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 6.69999981,  3.0999999 ,  4.69999981,  1.5       ],
       [ 6.0999999 ,  2.9000001 ,  4.69999981,  1.3999999... ,  3.        ,  1.29999995,  0.2       ],
       [ 6.5       ,  3.        ,  5.80000019,  2.20000005]], dtype=float32)
X_idx_sorted = array([[104, 110, 120,  70],
       [148, 132, 104, 104],
       [  6,   7, 126,  83],
       [ 55,  53,  39,   5],
  ...1,  39, 131,  52],
       [121, 140,  41,  57],
       [  8,  84,   8, 128],
       [ 48,  98, 121, 129]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46eee55b00>
monitor    = None
n_inbag    = 150
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46eee55a90>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse',
              init=<sklearn.ensemble.gradient_boosting.ZeroEstima...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 2, 0, 2, 0, 0, 1, 2, 0, 2, 0, 1, 2, 0, 1, 1, 1, 0, 2, 2, 0, 2,
       1, 2, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0,...       1, 2, 2, 1, 1, 0, 2, 2, 1, 1, 1, 0, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1,
       1, 2, 0, 0, 1, 1, 1, 2, 1, 1, 0, 2])
y_pred     = array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]...      [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________ test_max_leaf_nodes_max_depth[GradientBoostingClassifier] ___________[0m

GBEstimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('GBEstimator', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_max_leaf_nodes_max_depth(GBEstimator):[0m
[1m        # Test precedence of max_leaf_nodes over max_depth.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m    [0m
[1m        k = 4[0m
[1m    [0m
[1m>       est = GBEstimator(max_depth=1, max_leaf_nodes=k).fit(X, y)[0m

GBEstimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
k          = 4
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1116: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e01a1160>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_max_leaf_nodes_max_depth[GradientBoostingRegressor] ___________[0m

GBEstimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('GBEstimator', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_max_leaf_nodes_max_depth(GBEstimator):[0m
[1m        # Test precedence of max_leaf_nodes over max_depth.[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m    [0m
[1m        k = 4[0m
[1m    [0m
[1m>       est = GBEstimator(max_depth=1, max_leaf_nodes=k).fit(X, y)[0m

GBEstimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
k          = 4
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1116: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e030d9b0>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________ test_min_impurity_decrease[GradientBoostingClassifier] ____________[0m

GBEstimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>

[1m    @pytest.mark.parametrize('GBEstimator', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_min_impurity_decrease(GBEstimator):[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m    [0m
[1m        est = GBEstimator(min_impurity_decrease=0.1)[0m
[1m>       est.fit(X, y)[0m

GBEstimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1143: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46eedfd438>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...tate=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,...1, 1, 0, 1,
       0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 0, 1, 0, 0])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________ test_min_impurity_decrease[GradientBoostingRegressor] _____________[0m

GBEstimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>

[1m    @pytest.mark.parametrize('GBEstimator', GRADIENT_BOOSTING_ESTIMATORS)[0m
[1m    def test_min_impurity_decrease(GBEstimator):[0m
[1m        X, y = datasets.make_hastie_10_2(n_samples=100, random_state=1)[0m
[1m    [0m
[1m        est = GBEstimator(min_impurity_decrease=0.1)[0m
[1m>       est.fit(X, y)[0m

GBEstimator = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[  1.62434536e+00,  -6.11756414e-01,  -5.28171752e-01,
         -1.07296862e+00,   8.65407629e-01,  -2.30153870...118890e+00,  -1.16444148e-01,
         -2.27729800e+00,  -6.96245395e-02,   3.53870427e-01,
         -1.86955017e-01]])
est        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1143: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
y = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  1.62434542e+00,  -6.11756384e-01,  -5.28171778e-01,
         -1.07296860e+00,   8.65407646e-01,  -2.30153871....16444148e-01,
         -2.27729797e+00,  -6.96245357e-02,   3.53870422e-01,
         -1.86955020e-01]], dtype=float32)
X_idx_sorted = array([[79, 66, 89, 97, 41,  0, 97, 36, 73, 58],
       [72, 15, 25, 55, 54,  7, 99, 49, 33, 33],
       [67, 71, 36, ...4, 43],
       [51, 39, 17, 15, 81, 22, 25, 80,  6, 20],
       [30, 41, 73, 68, 31, 56, 37,  4, 16, 39]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee112b0>
monitor    = None
n_inbag    = 100
n_samples  = 100
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...       random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 1.,  1., -1., -1.,  1., -1.,  1., -1., -1., -1., -1., -1.,  1.,
       -1., -1.,  1.,  1.,  1., -1.,  1.,  1.,...  -1.,  1.,  1., -1., -1., -1.,  1.,  1., -1.,  1., -1.,  1.,  1.,
       -1., -1.,  1.,  1., -1., -1.,  1., -1., -1.])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
      ...[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________ test_warm_start_wo_nestimators_change _____________________[0m

[1m    def test_warm_start_wo_nestimators_change():[0m
[1m        # Test if warm_start does nothing if n_estimators is not changed.[0m
[1m        # Regression test for #3513.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=10, warm_start=True)[0m
[1m>       clf.fit([[0, 1], [2, 3]], [0, 1])[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1154: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
X = array([[ 0.,  1.],
       [ 2.,  3.]], dtype=float32), y = array([0, 1])
y_pred = array([[ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46f43ef588>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 0],
       [1, 1]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.,  1.],
       [ 2.,  3.]], dtype=float32)
X_idx_sorted = array([[0, 0],
       [1, 1]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e0212048>
monitor    = None
n_inbag    = 2
n_samples  = 2
random_state = <mtrand.RandomState object at 0x7f46f43ef588>
sample_mask = array([ True,  True], dtype=bool)
sample_weight = array([ 1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=True)
y          = array([0, 1])
y_pred     = array([[ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_probability_exponential _________________________[0m

[1m    def test_probability_exponential():[0m
[1m        # Predict probabilities.[0m
[1m        clf = GradientBoostingClassifier(loss='exponential',[0m
[1m                                         n_estimators=100, random_state=1)[0m
[1m    [0m
[1m        assert_raises(ValueError, clf.predict_proba, T)[0m
[1m    [0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1167: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e5a45780>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.ExponentialLoss object at 0x7f46eeb760b8>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46e5a45780>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='exponential', m...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_sparse_input[csr_matrix-GradientBoostingClassifier] ___________[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
sparse_matrix = <class 'scipy.sparse.csr.csr_matrix'>

[1m    @skip_if_32bit[0m
[1m    @pytest.mark.parametrize([0m
[1m            'EstimatorClass',[0m
[1m            (GradientBoostingClassifier, GradientBoostingRegressor))[0m
[1m    @pytest.mark.parametrize('sparse_matrix', (csr_matrix, csc_matrix, coo_matrix))[0m
[1m    def test_sparse_input(EstimatorClass, sparse_matrix):[0m
[1m        y, X = datasets.make_multilabel_classification(random_state=0,[0m
[1m                                                       n_samples=50,[0m
[1m                                                       n_features=1,[0m
[1m                                                       n_classes=20)[0m
[1m        y = y[:, 0][0m
[1m    [0m
[1m>       check_sparse_input(EstimatorClass, X, sparse_matrix(X), y)[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,... 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
sparse_matrix = <class 'scipy.sparse.csr.csr_matrix'>
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1214: in check_sparse_input
[1m    max_depth=2).fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
y = array([16, 11,  6, 16, 11,  1, 10,  7,  9, 12, 11, 10, 13,  8, 19, 12,  5,
       16, 18, 22,  5,  9,  0,  4,  6,  2, 13, 13, 12,  6, 13, 20,  1, 13,
        3, 15, 12, 17, 11, 14, 13, 10, 14,  9, 21,  7,  9, 12,  6,  7])
y_pred = array([[ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
      ...0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee3fdd8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46eee3f4a8>
monitor    = None
n_inbag    = 50
n_samples  = 50
random_state = <mtrand.RandomState object at 0x7f46eee3fdd8>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...   True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([16, 11,  6, 16, 11,  1, 10,  7,  9, 12, 11, 10, 13,  8, 19, 12,  5,
       16, 18, 22,  5,  9,  0,  4,  6,  2, 13, 13, 12,  6, 13, 20,  1, 13,
        3, 15, 12, 17, 11, 14, 13, 10, 14,  9, 21,  7,  9, 12,  6,  7])
y_pred     = array([[ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
      ...0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_sparse_input[csr_matrix-GradientBoostingRegressor] ____________[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
sparse_matrix = <class 'scipy.sparse.csr.csr_matrix'>

[1m    @skip_if_32bit[0m
[1m    @pytest.mark.parametrize([0m
[1m            'EstimatorClass',[0m
[1m            (GradientBoostingClassifier, GradientBoostingRegressor))[0m
[1m    @pytest.mark.parametrize('sparse_matrix', (csr_matrix, csc_matrix, coo_matrix))[0m
[1m    def test_sparse_input(EstimatorClass, sparse_matrix):[0m
[1m        y, X = datasets.make_multilabel_classification(random_state=0,[0m
[1m                                                       n_samples=50,[0m
[1m                                                       n_features=1,[0m
[1m                                                       n_classes=20)[0m
[1m        y = y[:, 0][0m
[1m    [0m
[1m>       check_sparse_input(EstimatorClass, X, sparse_matrix(X), y)[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,... 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
sparse_matrix = <class 'scipy.sparse.csr.csr_matrix'>
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1214: in check_sparse_input
[1m    max_depth=2).fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
y = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])
y_pred = array([[ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],...       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e00b5668>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e00b57f0>
monitor    = None
n_inbag    = 50
n_samples  = 50
random_state = <mtrand.RandomState object at 0x7f46e00b5668>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...   True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])
y_pred     = array([[ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],...       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_sparse_input[csc_matrix-GradientBoostingClassifier] ___________[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
sparse_matrix = <class 'scipy.sparse.csc.csc_matrix'>

[1m    @skip_if_32bit[0m
[1m    @pytest.mark.parametrize([0m
[1m            'EstimatorClass',[0m
[1m            (GradientBoostingClassifier, GradientBoostingRegressor))[0m
[1m    @pytest.mark.parametrize('sparse_matrix', (csr_matrix, csc_matrix, coo_matrix))[0m
[1m    def test_sparse_input(EstimatorClass, sparse_matrix):[0m
[1m        y, X = datasets.make_multilabel_classification(random_state=0,[0m
[1m                                                       n_samples=50,[0m
[1m                                                       n_features=1,[0m
[1m                                                       n_classes=20)[0m
[1m        y = y[:, 0][0m
[1m    [0m
[1m>       check_sparse_input(EstimatorClass, X, sparse_matrix(X), y)[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,... 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
sparse_matrix = <class 'scipy.sparse.csc.csc_matrix'>
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1214: in check_sparse_input
[1m    max_depth=2).fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
y = array([16, 11,  6, 16, 11,  1, 10,  7,  9, 12, 11, 10, 13,  8, 19, 12,  5,
       16, 18, 22,  5,  9,  0,  4,  6,  2, 13, 13, 12,  6, 13, 20,  1, 13,
        3, 15, 12, 17, 11, 14, 13, 10, 14,  9, 21,  7,  9, 12,  6,  7])
y_pred = array([[ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
      ...0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e274ce48>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e274ccc0>
monitor    = None
n_inbag    = 50
n_samples  = 50
random_state = <mtrand.RandomState object at 0x7f46e274ce48>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...   True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([16, 11,  6, 16, 11,  1, 10,  7,  9, 12, 11, 10, 13,  8, 19, 12,  5,
       16, 18, 22,  5,  9,  0,  4,  6,  2, 13, 13, 12,  6, 13, 20,  1, 13,
        3, 15, 12, 17, 11, 14, 13, 10, 14,  9, 21,  7,  9, 12,  6,  7])
y_pred     = array([[ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
      ...0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_sparse_input[csc_matrix-GradientBoostingRegressor] ____________[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
sparse_matrix = <class 'scipy.sparse.csc.csc_matrix'>

[1m    @skip_if_32bit[0m
[1m    @pytest.mark.parametrize([0m
[1m            'EstimatorClass',[0m
[1m            (GradientBoostingClassifier, GradientBoostingRegressor))[0m
[1m    @pytest.mark.parametrize('sparse_matrix', (csr_matrix, csc_matrix, coo_matrix))[0m
[1m    def test_sparse_input(EstimatorClass, sparse_matrix):[0m
[1m        y, X = datasets.make_multilabel_classification(random_state=0,[0m
[1m                                                       n_samples=50,[0m
[1m                                                       n_features=1,[0m
[1m                                                       n_classes=20)[0m
[1m        y = y[:, 0][0m
[1m    [0m
[1m>       check_sparse_input(EstimatorClass, X, sparse_matrix(X), y)[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,... 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
sparse_matrix = <class 'scipy.sparse.csc.csc_matrix'>
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1214: in check_sparse_input
[1m    max_depth=2).fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
y = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])
y_pred = array([[ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],...       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46dfb47eb8>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46dfb47cf8>
monitor    = None
n_inbag    = 50
n_samples  = 50
random_state = <mtrand.RandomState object at 0x7f46dfb47eb8>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...   True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])
y_pred     = array([[ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],...       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_sparse_input[coo_matrix-GradientBoostingClassifier] ___________[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
sparse_matrix = <class 'scipy.sparse.coo.coo_matrix'>

[1m    @skip_if_32bit[0m
[1m    @pytest.mark.parametrize([0m
[1m            'EstimatorClass',[0m
[1m            (GradientBoostingClassifier, GradientBoostingRegressor))[0m
[1m    @pytest.mark.parametrize('sparse_matrix', (csr_matrix, csc_matrix, coo_matrix))[0m
[1m    def test_sparse_input(EstimatorClass, sparse_matrix):[0m
[1m        y, X = datasets.make_multilabel_classification(random_state=0,[0m
[1m                                                       n_samples=50,[0m
[1m                                                       n_features=1,[0m
[1m                                                       n_classes=20)[0m
[1m        y = y[:, 0][0m
[1m    [0m
[1m>       check_sparse_input(EstimatorClass, X, sparse_matrix(X), y)[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingClassifier'>
X          = array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,... 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
sparse_matrix = <class 'scipy.sparse.coo.coo_matrix'>
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1214: in check_sparse_input
[1m    max_depth=2).fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
y = array([16, 11,  6, 16, 11,  1, 10,  7,  9, 12, 11, 10, 13,  8, 19, 12,  5,
       16, 18, 22,  5,  9,  0,  4,  6,  2, 13, 13, 12,  6, 13, 20,  1, 13,
        3, 15, 12, 17, 11, 14, 13, 10, 14,  9, 21,  7,  9, 12,  6,  7])
y_pred = array([[ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
      ...0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e03555c0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e0355668>
monitor    = None
n_inbag    = 50
n_samples  = 50
random_state = <mtrand.RandomState object at 0x7f46e03555c0>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...   True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([16, 11,  6, 16, 11,  1, 10,  7,  9, 12, 11, 10, 13,  8, 19, 12,  5,
       16, 18, 22,  5,  9,  0,  4,  6,  2, 13, 13, 12,  6, 13, 20,  1, 13,
        3, 15, 12, 17, 11, 14, 13, 10, 14,  9, 21,  7,  9, 12,  6,  7])
y_pred     = array([[ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
      ...0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02],
       [ 0.02,  0.04,  0.02, ...,  0.02,  0.02,  0.02]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m___________ test_sparse_input[coo_matrix-GradientBoostingRegressor] ____________[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
sparse_matrix = <class 'scipy.sparse.coo.coo_matrix'>

[1m    @skip_if_32bit[0m
[1m    @pytest.mark.parametrize([0m
[1m            'EstimatorClass',[0m
[1m            (GradientBoostingClassifier, GradientBoostingRegressor))[0m
[1m    @pytest.mark.parametrize('sparse_matrix', (csr_matrix, csc_matrix, coo_matrix))[0m
[1m    def test_sparse_input(EstimatorClass, sparse_matrix):[0m
[1m        y, X = datasets.make_multilabel_classification(random_state=0,[0m
[1m                                                       n_samples=50,[0m
[1m                                                       n_features=1,[0m
[1m                                                       n_classes=20)[0m
[1m        y = y[:, 0][0m
[1m    [0m
[1m>       check_sparse_input(EstimatorClass, X, sparse_matrix(X), y)[0m

EstimatorClass = <class 'sklearn.ensemble.gradient_boosting.GradientBoostingRegressor'>
X          = array([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],
       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,... 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
sparse_matrix = <class 'scipy.sparse.coo.coo_matrix'>
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1214: in check_sparse_input
[1m    max_depth=2).fit(X, y)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
y = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])
y_pred = array([[ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],...       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e02dac50>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.],
 ... 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32)
X_idx_sorted = array([[ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  0,  0,  0,  0,
         0,  0,  0],
       [26, 23, 25, 2...42,  1],
       [22, 49, 49, 13,  1, 11, 29, 35, 36, 36,  1, 48,  0,  2, 32, 49, 49,
        22,  3, 22]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46e02da550>
monitor    = None
n_inbag    = 50
n_samples  = 50
random_state = <mtrand.RandomState object at 0x7f46e02dac50>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...   True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 54.,  49.,  43.,  54.,  49.,  37.,  48.,  44.,  47.,  50.,  49.,
        48.,  51.,  45.,  63.,  50.,  42.,  5....,
        51.,  40.,  53.,  50.,  55.,  49.,  52.,  51.,  48.,  52.,  47.,
        66.,  44.,  47.,  50.,  43.,  44.])
y_pred     = array([[ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],...       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82],
       [ 48.82]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m____________________ test_gradient_boosting_early_stopping _____________________[0m

[1m    def test_gradient_boosting_early_stopping():[0m
[1m        X, y = make_classification(n_samples=1000, random_state=0)[0m
[1m    [0m
[1m        gbc = GradientBoostingClassifier(n_estimators=1000,[0m
[1m                                         n_iter_no_change=10,[0m
[1m                                         learning_rate=0.1, max_depth=3,[0m
[1m                                         random_state=42)[0m
[1m    [0m
[1m        gbr = GradientBoostingRegressor(n_estimators=1000, n_iter_no_change=10,[0m
[1m                                        learning_rate=0.1, max_depth=3,[0m
[1m                                        random_state=42)[0m
[1m    [0m
[1m        X_train, X_test, y_train, y_test = train_test_split(X, y,[0m
[1m                                                            random_state=42)[0m
[1m        # Check if early_stopping works as expected[0m
[1m        for est, tol, early_stop_n_estimators in ((gbc, 1e-1, 24), (gbr, 1e-1, 13),[0m
[1m                                                  (gbc, 1e-3, 36),[0m
[1m                                                  (gbr, 1e-3, 28)):[0m
[1m            est.set_params(tol=tol)[0m
[1m>           est.fit(X_train, y_train)[0m

X          = array([[-1.06377997,  0.67640868,  1.06935647, ..., -0.35562842,
         1.05721416, -0.90259159],
       [ 0.0708476...096,  2.78960064],
       [ 0.54272502,  0.30821962,  0.20152656, ...,  1.09537374,
         0.12630589, -1.42647315]])
X_test     = array([[ 0.59223168, -1.55812414,  0.13098171, ...,  1.66715922,
        -0.86986548, -1.45517835],
       [-0.7397205...73 ,  1.09586334],
       [-1.76244223, -0.9092681 , -0.69026159, ..., -0.28518422,
        -1.43484527, -2.09840956]])
X_train    = array([[ 1.9103425 , -1.83956679,  1.07731488, ...,  0.81108968,
        -0.21142861, -0.64679696],
       [-1.5249018...498, -1.02742501],
       [ 0.18671911,  0.36472003,  0.06453274, ..., -0.22949441,
        -0.49828831, -1.7114733 ]])
early_stop_n_estimators = 24
est        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...dom_state=42,
              subsample=1.0, tol=0.1, validation_fraction=0.1, verbose=0,
              warm_start=False)
gbc        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...dom_state=42,
              subsample=1.0, tol=0.1, validation_fraction=0.1, verbose=0,
              warm_start=False)
gbr        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...         random_state=42, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
tol        = 0.1
y          = array([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
       1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,...1,
       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1])
y_test     = array([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,... 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
       0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1])
y_train    = array([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,... 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
       1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1288: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...dom_state=42,
              subsample=1.0, tol=0.1, validation_fraction=0.1, verbose=0,
              warm_start=False)
X = array([[-1.20076382, -0.11121846,  0.44762212, ..., -0.24343619,
        -1.14133441,  1.54311144],
       [-0.8413016...3],
       [ 0.04236114, -0.47705629,  0.13317989, ..., -0.54167837,
         0.17059821, -0.55996537]], dtype=float32)
y = array([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,...1, 0, 0, 1,
       1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
       0, 1, 1, 1, 1, 0, 1, 0])
y_pred = array([[-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [...191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e008f128>
X_val = array([[ 0.65757084, -1.28698123, -1.24171269, ...,  0.84443384,
         0.31882566,  0.13279839],
       [ 1.7806649...8],
       [-1.10028315,  1.11037886,  1.48527563, ..., -0.6047641 ,
        -0.0425556 ,  0.53374386]], dtype=float32)
y_val = array([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,...0, 0, 1, 0, 1, 0,
       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
       0, 1, 1, 1, 1, 0])
sample_weight_val = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
begin_at_stage = 0, monitor = None
X_idx_sorted = array([[670, 482, 144, ..., 484, 606, 111],
       [441, 547, 536, ..., 635, 444, 211],
       [154, 449, 518, ..., 55..., 348, 351, 558],
       [316, 231,  47, ..., 251, 207, 447],
       [164, 575, 608, ..., 255,  59, 575]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-1.20076382, -0.11121846,  0.44762212, ..., -0.24343619,
        -1.14133441,  1.54311144],
       [-0.8413016...3],
       [ 0.04236114, -0.47705629,  0.13317989, ..., -0.54167837,
         0.17059821, -0.55996537]], dtype=float32)
X_idx_sorted = array([[670, 482, 144, ..., 484, 606, 111],
       [441, 547, 536, ..., 635, 444, 211],
       [154, 449, 518, ..., 55..., 348, 351, 558],
       [316, 231,  47, ..., 251, 207, 447],
       [164, 575, 608, ..., 255,  59, 575]], dtype=int32)
X_val      = array([[ 0.65757084, -1.28698123, -1.24171269, ...,  0.84443384,
         0.31882566,  0.13279839],
       [ 1.7806649...8],
       [-1.10028315,  1.11037886,  1.48527563, ..., -0.6047641 ,
        -0.0425556 ,  0.53374386]], dtype=float32)
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e008f588>
monitor    = None
n_inbag    = 675
n_samples  = 675
random_state = <mtrand.RandomState object at 0x7f46e008f128>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...dom_state=42,
              subsample=1.0, tol=0.1, validation_fraction=0.1, verbose=0,
              warm_start=False)
y          = array([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,...1, 0, 0, 1,
       1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
       0, 1, 1, 1, 1, 0, 1, 0])
y_pred     = array([[-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [...191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651]])
y_val      = array([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,...0, 0, 1, 0, 1, 0,
       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
       0, 1, 1, 1, 1, 0])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m__________________ test_gradient_boosting_validation_fraction __________________[0m

[1m    def test_gradient_boosting_validation_fraction():[0m
[1m        X, y = make_classification(n_samples=1000, random_state=0)[0m
[1m    [0m
[1m        gbc = GradientBoostingClassifier(n_estimators=100,[0m
[1m                                         n_iter_no_change=10,[0m
[1m                                         validation_fraction=0.1,[0m
[1m                                         learning_rate=0.1, max_depth=3,[0m
[1m                                         random_state=42)[0m
[1m        gbc2 = clone(gbc).set_params(validation_fraction=0.3)[0m
[1m        gbc3 = clone(gbc).set_params(n_iter_no_change=20)[0m
[1m    [0m
[1m        gbr = GradientBoostingRegressor(n_estimators=100, n_iter_no_change=10,[0m
[1m                                        learning_rate=0.1, max_depth=3,[0m
[1m                                        validation_fraction=0.1,[0m
[1m                                        random_state=42)[0m
[1m        gbr2 = clone(gbr).set_params(validation_fraction=0.3)[0m
[1m        gbr3 = clone(gbr).set_params(n_iter_no_change=20)[0m
[1m    [0m
[1m        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)[0m
[1m        # Check if validation_fraction has an effect[0m
[1m>       gbc.fit(X_train, y_train)[0m

X          = array([[-1.06377997,  0.67640868,  1.06935647, ..., -0.35562842,
         1.05721416, -0.90259159],
       [ 0.0708476...096,  2.78960064],
       [ 0.54272502,  0.30821962,  0.20152656, ...,  1.09537374,
         0.12630589, -1.42647315]])
X_test     = array([[ 0.59223168, -1.55812414,  0.13098171, ...,  1.66715922,
        -0.86986548, -1.45517835],
       [-0.7397205...73 ,  1.09586334],
       [-1.76244223, -0.9092681 , -0.69026159, ..., -0.28518422,
        -1.43484527, -2.09840956]])
X_train    = array([[ 1.9103425 , -1.83956679,  1.07731488, ...,  0.81108968,
        -0.21142861, -0.64679696],
       [-1.5249018...498, -1.02742501],
       [ 0.18671911,  0.36472003,  0.06453274, ..., -0.22949441,
        -0.49828831, -1.7114733 ]])
gbc        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_..._state=42,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
gbc2       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_..._state=42,
              subsample=1.0, tol=0.0001, validation_fraction=0.3,
              verbose=0, warm_start=False)
gbc3       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_..._state=42,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
gbr        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...         random_state=42, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
gbr2       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...         random_state=42, subsample=1.0, tol=0.0001,
             validation_fraction=0.3, verbose=0, warm_start=False)
gbr3       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...         random_state=42, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1,
       1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0,...1,
       1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,
       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1])
y_test     = array([0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,
       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,... 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,
       0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1])
y_train    = array([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,
       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,... 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,
       1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py[0m:1324: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_..._state=42,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-1.20076382, -0.11121846,  0.44762212, ..., -0.24343619,
        -1.14133441,  1.54311144],
       [-0.8413016...3],
       [ 0.04236114, -0.47705629,  0.13317989, ..., -0.54167837,
         0.17059821, -0.55996537]], dtype=float32)
y = array([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,...1, 0, 0, 1,
       1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
       0, 1, 1, 1, 1, 0, 1, 0])
y_pred = array([[-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [...191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e00b0630>
X_val = array([[ 0.65757084, -1.28698123, -1.24171269, ...,  0.84443384,
         0.31882566,  0.13279839],
       [ 1.7806649...8],
       [-1.10028315,  1.11037886,  1.48527563, ..., -0.6047641 ,
        -0.0425556 ,  0.53374386]], dtype=float32)
y_val = array([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,...0, 0, 1, 0, 1, 0,
       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
       0, 1, 1, 1, 1, 0])
sample_weight_val = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
begin_at_stage = 0, monitor = None
X_idx_sorted = array([[670, 482, 144, ..., 484, 606, 111],
       [441, 547, 536, ..., 635, 444, 211],
       [154, 449, 518, ..., 55..., 348, 351, 558],
       [316, 231,  47, ..., 251, 207, 447],
       [164, 575, 608, ..., 255,  59, 575]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-1.20076382, -0.11121846,  0.44762212, ..., -0.24343619,
        -1.14133441,  1.54311144],
       [-0.8413016...3],
       [ 0.04236114, -0.47705629,  0.13317989, ..., -0.54167837,
         0.17059821, -0.55996537]], dtype=float32)
X_idx_sorted = array([[670, 482, 144, ..., 484, 606, 111],
       [441, 547, 536, ..., 635, 444, 211],
       [154, 449, 518, ..., 55..., 348, 351, 558],
       [316, 231,  47, ..., 251, 207, 447],
       [164, 575, 608, ..., 255,  59, 575]], dtype=int32)
X_val      = array([[ 0.65757084, -1.28698123, -1.24171269, ...,  0.84443384,
         0.31882566,  0.13279839],
       [ 1.7806649...8],
       [-1.10028315,  1.11037886,  1.48527563, ..., -0.6047641 ,
        -0.0425556 ,  0.53374386]], dtype=float32)
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46e00b0d68>
monitor    = None
n_inbag    = 675
n_samples  = 675
random_state = <mtrand.RandomState object at 0x7f46e00b0630>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_..._state=42,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,
       0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,...1, 0, 0, 1,
       1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0,
       0, 1, 1, 1, 1, 0, 1, 0])
y_pred     = array([[-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [...191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651],
       [-0.09191651]])
y_val      = array([1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,
       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,...0, 0, 1, 0, 1, 0,
       0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,
       0, 1, 1, 1, 1, 0])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________________ test_partial_dependence_classifier ______________________[0m

[1m    def test_partial_dependence_classifier():[0m
[1m        # Test partial dependence for classifier[0m
[1m        clf = GradientBoostingClassifier(n_estimators=10, random_state=1)[0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_partial_dependence.py[0m:34: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46df8e6518>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46df8e6668>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46df8e6518>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________________ test_partial_dependence_multiclass ______________________[0m

[1m    def test_partial_dependence_multiclass():[0m
[1m        # Test partial dependence for multi-class classifier[0m
[1m        clf = GradientBoostingClassifier(n_estimators=10, random_state=1)[0m
[1m>       clf.fit(iris.data, iris.target)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_partial_dependence.py[0m:54: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[ 5.0999999 ,  3.5       ,  1.39999998,  0.2       ],
       [ 4.9000001 ,  3.        ,  1.39999998,  0.2      ...1,  3.4000001 ,  5.4000001 ,  2.29999995],
       [ 5.9000001 ,  3.        ,  5.0999999 ,  1.79999995]], dtype=float32)
y = array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
y_pred = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46e009fa58>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[ 13,  60,  22,  32],
       [ 42,  62,  13,  13],
       [ 38, 119,  14,  37],
       [  8,  68,  35,   9],
  ...8,  14, 105, 114],
       [117,  32, 117, 100],
       [135,  33, 122, 144],
       [131,  15, 118, 109]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 5.0999999 ,  3.5       ,  1.39999998,  0.2       ],
       [ 4.9000001 ,  3.        ,  1.39999998,  0.2      ...1,  3.4000001 ,  5.4000001 ,  2.29999995],
       [ 5.9000001 ,  3.        ,  5.0999999 ,  1.79999995]], dtype=float32)
X_idx_sorted = array([[ 13,  60,  22,  32],
       [ 42,  62,  13,  13],
       [ 38, 119,  14,  37],
       [  8,  68,  35,   9],
  ...8,  14, 105, 114],
       [117,  32, 117, 100],
       [135,  33, 122, 144],
       [131,  15, 118, 109]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.MultinomialDeviance object at 0x7f46e009fcc0>
monitor    = None
n_inbag    = 150
n_samples  = 150
random_state = <mtrand.RandomState object at 0x7f46e009fa58>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
y_pred     = array([[ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  ...3333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333],
       [ 0.33333333,  0.33333333,  0.33333333]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m______________________ test_partial_dependence_regressor _______________________[0m

[1m    def test_partial_dependence_regressor():[0m
[1m        # Test partial dependence for regressor[0m
[1m        clf = GradientBoostingRegressor(n_estimators=10, random_state=1)[0m
[1m>       clf.fit(boston.data, boston.target)[0m

clf        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_partial_dependence.py[0m:69: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[  6.32000016e-03,   1.80000000e+01,   2.30999994e+00, ...,
          1.53000002e+01,   3.96899994e+02,   4.980...  0.00000000e+00,   1.19300003e+01, ...,
          2.10000000e+01,   3.96899994e+02,   7.88000011e+00]], dtype=float32)
y = array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,
        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,... 13.6,  20.1,  21.8,  24.5,
        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,
        22. ,  11.9])
y_pred = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46eee80be0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[  0, 505, 195, ..., 197, 450, 161],
       [284, 439,  56, ..., 198, 423, 162],
       [285, 440, 283, ..., 19..., 134, 324, 373],
       [418, 204, 492, ..., 355, 295, 414],
       [380,  57, 491, ..., 354, 505, 374]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[  6.32000016e-03,   1.80000000e+01,   2.30999994e+00, ...,
          1.53000002e+01,   3.96899994e+02,   4.980...  0.00000000e+00,   1.19300003e+01, ...,
          2.10000000e+01,   3.96899994e+02,   7.88000011e+00]], dtype=float32)
X_idx_sorted = array([[  0, 505, 195, ..., 197, 450, 161],
       [284, 439,  56, ..., 198, 423, 162],
       [285, 440, 283, ..., 19..., 134, 324, 373],
       [418, 204, 492, ..., 355, 295, 414],
       [380,  57, 491, ..., 354, 505, 374]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46eee80550>
monitor    = None
n_inbag    = 506
n_samples  = 506
random_state = <mtrand.RandomState object at 0x7f46eee80be0>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=1, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,
        18.9,  15. ,  18.9,  21.7,  20.4,  18.2,... 13.6,  20.1,  21.8,  24.5,
        23.1,  19.7,  18.3,  21.2,  17.5,  16.8,  22.4,  20.6,  23.9,
        22. ,  11.9])
y_pred     = array([[ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
   ...2],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632],
       [ 22.53280632]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_partial_dependecy_input _________________________[0m

[1m    def test_partial_dependecy_input():[0m
[1m        # Test input validation of partial dependence.[0m
[1m        clf = GradientBoostingClassifier(n_estimators=10, random_state=1)[0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_partial_dependence.py[0m:82: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46dfb74710>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46dfb74940>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46dfb74710>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=1,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_________________________ test_multi_target_regression _________________________[0m

[1m    def test_multi_target_regression():[0m
[1m        X, y = datasets.make_regression(n_targets=3)[0m
[1m        X_train, y_train = X[:50], y[:50][0m
[1m        X_test, y_test = X[50:], y[50:][0m
[1m    [0m
[1m        references = np.zeros_like(y_test)[0m
[1m        for n in range(3):[0m
[1m            rgr = GradientBoostingRegressor(random_state=0)[0m
[1m>           rgr.fit(X_train, y_train[:, n])[0m

X          = array([[ 0.68293442, -1.64956248,  0.83223163, ...,  1.13265246,
         0.65175606,  1.12326675],
       [ 0.7797444...94 ,  2.10910625],
       [ 1.19822674,  0.17851589,  0.0953019 , ...,  2.07032596,
         0.24634712,  0.65280465]])
X_test     = array([[ 1.03120695, -0.6865286 , -1.31478308, ..., -1.01802926,
         0.79139879, -1.02248633],
       [ 0.1885603...94 ,  2.10910625],
       [ 1.19822674,  0.17851589,  0.0953019 , ...,  2.07032596,
         0.24634712,  0.65280465]])
X_train    = array([[ 0.68293442, -1.64956248,  0.83223163, ...,  1.13265246,
         0.65175606,  1.12326675],
       [ 0.7797444...651, -0.18970984],
       [-1.04959687, -1.59500679,  0.44208115, ...,  0.90063525,
        -0.40444322, -1.93625278]])
n          = 0
references = array([[ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]...      [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.],
       [ 0.,  0.,  0.]])
rgr        = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([[  97.64540873,   -5.1380316 ,  200.09733846],
       [ -39.45978948, -132.51296147, -201.63706704],
       [-2...73769236],
       [-243.41890159, -242.24359415, -217.42698076],
       [  -4.59937164,  136.44033668,   91.54564692]])
y_test     = array([[  68.11972012, -116.52381424,   43.85032884],
       [ 197.65322265,   86.02800373,  192.12541124],
       [-1...73769236],
       [-243.41890159, -242.24359415, -217.42698076],
       [  -4.59937164,  136.44033668,   91.54564692]])
y_train    = array([[  97.64540873,   -5.1380316 ,  200.09733846],
       [ -39.45978948, -132.51296147, -201.63706704],
       [-2...57484889],
       [ 114.71730256, -152.35455411, -203.4325072 ],
       [ -86.61908203, -344.20772352, -111.92233925]])

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_multioutput.py[0m:45: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[ 0.6829344 , -1.64956248,  0.83223164, ...,  1.1326524 ,
         0.65175605,  1.1232667 ],
       [ 0.7797444...4],
       [-1.04959691, -1.59500682,  0.44208115, ...,  0.90063524,
        -0.40444323, -1.93625283]], dtype=float32)
y = array([  97.64540873,  -39.45978948, -222.55571728,  206.16912576,
        286.54900101,  240.47851917, -155.31214702,...  73.24057808,
        -78.96906479,  -58.46556792,  -32.94114062, -185.89148132,
        114.71730256,  -86.61908203])
y_pred = array([[-31.36032091],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091],
   ...1],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46d7eaf0f0>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[36, 45, 27, ..., 27, 41, 39],
       [ 8,  0,  4, ..., 46, 38, 15],
       [48, 49, 44, ..., 40, 24, 49],
    ...38, 32, 12, ...,  9, 47, 18],
       [41, 31, 26, ..., 22,  2, 16],
       [20, 22, 28, ..., 19, 45, 11]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 0.6829344 , -1.64956248,  0.83223164, ...,  1.1326524 ,
         0.65175605,  1.1232667 ],
       [ 0.7797444...4],
       [-1.04959691, -1.59500682,  0.44208115, ...,  0.90063524,
        -0.40444323, -1.93625283]], dtype=float32)
X_idx_sorted = array([[36, 45, 27, ..., 27, 41, 39],
       [ 8,  0,  4, ..., 46, 38, 15],
       [48, 49, 44, ..., 40, 24, 49],
    ...38, 32, 12, ...,  9, 47, 18],
       [41, 31, 26, ..., 22,  2, 16],
       [20, 22, 28, ..., 19, 45, 11]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46d7e9e080>
monitor    = None
n_inbag    = 50
n_samples  = 50
random_state = <mtrand.RandomState object at 0x7f46d7eaf0f0>
sample_mask = array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,...   True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,...,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,
        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([  97.64540873,  -39.45978948, -222.55571728,  206.16912576,
        286.54900101,  240.47851917, -155.31214702,...  73.24057808,
        -78.96906479,  -58.46556792,  -32.94114062, -185.89148132,
        114.71730256,  -86.61908203])
y_pred     = array([[-31.36032091],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091],
   ...1],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091],
       [-31.36032091]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_____________________ test_multi_target_sample_weights_api _____________________[0m

[1m    def test_multi_target_sample_weights_api():[0m
[1m        X = [[1, 2, 3], [4, 5, 6]][0m
[1m        y = [[3.141, 2.718], [2.718, 3.141]][0m
[1m        w = [0.8, 0.6][0m
[1m    [0m
[1m        rgr = MultiOutputRegressor(Lasso())[0m
[1m        assert_raises_regex(ValueError, "does not support sample weights",[0m
[1m                            rgr.fit, X, y, w)[0m
[1m    [0m
[1m        # no exception should be raised if the base estimator supports weights[0m
[1m        rgr = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))[0m
[1m>       rgr.fit(X, y, w)[0m

X          = [[1, 2, 3], [4, 5, 6]]
rgr        = MultiOutputRegressor(estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             ... subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False),
           n_jobs=None)
w          = [0.8, 0.6]
y          = [[3.141, 2.718], [2.718, 3.141]]

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_multioutput.py[0m:113: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/multioutput.py[0m:169: in fit
[1m    for i in range(y.shape[1]))[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:981: in __call__
[1m    if self.dispatch_one_batch(iterator):[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:823: in dispatch_one_batch
[1m    self._dispatch(tasks)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:780: in _dispatch
[1m    job = self._backend.apply_async(batch, callback=cb)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_parallel_backends.py[0m:183: in apply_async
[1m    result = ImmediateResult(func)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_parallel_backends.py[0m:543: in __init__
[1m    self.results = batch()[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:261: in __call__
[1m    for func, args, kwargs in self.items][0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:261: in <listcomp>
[1m    for func, args, kwargs in self.items][0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/multioutput.py[0m:38: in _fit_estimator
[1m    estimator.fit(X, y, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[ 1.,  2.,  3.],
       [ 4.,  5.,  6.]], dtype=float32)
y = array([ 3.141,  2.718])
y_pred = array([[ 2.95971429],
       [ 2.95971429]])
sample_weight = array([ 0.8,  0.6])
random_state = <mtrand.RandomState object at 0x7f46d7e9e048>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 0, 0],
       [1, 1, 1]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.,  2.,  3.],
       [ 4.,  5.,  6.]], dtype=float32)
X_idx_sorted = array([[0, 0, 0],
       [1, 1, 1]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46d7e9e4a8>
monitor    = None
n_inbag    = 2
n_samples  = 2
random_state = <mtrand.RandomState object at 0x7f46d7e9e048>
sample_mask = array([ True,  True], dtype=bool)
sample_weight = array([ 0.8,  0.6])
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 3.141,  2.718])
y_pred     = array([[ 2.95971429],
       [ 2.95971429]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m_______________________ test_multi_target_sample_weights _______________________[0m

[1m    def test_multi_target_sample_weights():[0m
[1m        # weighted regressor[0m
[1m        Xw = [[1, 2, 3], [4, 5, 6]][0m
[1m        yw = [[3.141, 2.718], [2.718, 3.141]][0m
[1m        w = [2., 1.][0m
[1m        rgr_w = MultiOutputRegressor(GradientBoostingRegressor(random_state=0))[0m
[1m>       rgr_w.fit(Xw, yw, w)[0m

Xw         = [[1, 2, 3], [4, 5, 6]]
rgr_w      = MultiOutputRegressor(estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             ... subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False),
           n_jobs=None)
w          = [2.0, 1.0]
yw         = [[3.141, 2.718], [2.718, 3.141]]

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_multioutput.py[0m:138: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/multioutput.py[0m:169: in fit
[1m    for i in range(y.shape[1]))[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:981: in __call__
[1m    if self.dispatch_one_batch(iterator):[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:823: in dispatch_one_batch
[1m    self._dispatch(tasks)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:780: in _dispatch
[1m    job = self._backend.apply_async(batch, callback=cb)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_parallel_backends.py[0m:183: in apply_async
[1m    result = ImmediateResult(func)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_parallel_backends.py[0m:543: in __init__
[1m    self.results = batch()[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:261: in __call__
[1m    for func, args, kwargs in self.items][0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py[0m:261: in <listcomp>
[1m    for func, args, kwargs in self.items][0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/multioutput.py[0m:38: in _fit_estimator
[1m    estimator.fit(X, y, sample_weight=sample_weight)[0m
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
X = array([[ 1.,  2.,  3.],
       [ 4.,  5.,  6.]], dtype=float32)
y = array([ 3.141,  2.718]), y_pred = array([[ 3.],
       [ 3.]])
sample_weight = array([ 2.,  1.])
random_state = <mtrand.RandomState object at 0x7f46dc139710>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 0, 0],
       [1, 1, 1]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[ 1.,  2.,  3.],
       [ 4.,  5.,  6.]], dtype=float32)
X_idx_sorted = array([[0, 0, 0],
       [1, 1, 1]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.LeastSquaresError object at 0x7f46dc1396a0>
monitor    = None
n_inbag    = 2
n_samples  = 2
random_state = <mtrand.RandomState object at 0x7f46dc139710>
sample_mask = array([ True,  True], dtype=bool)
sample_weight = array([ 2.,  1.])
sample_weight_val = None
self       = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', m...          random_state=0, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
y          = array([ 3.141,  2.718])
y_pred     = array([[ 3.],
       [ 3.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning
[31m[1m________________________ test_friedman_mse_in_graphviz _________________________[0m

[1m    def test_friedman_mse_in_graphviz():[0m
[1m        clf = DecisionTreeRegressor(criterion="friedman_mse", random_state=0)[0m
[1m        clf.fit(X, y)[0m
[1m        dot_data = StringIO()[0m
[1m        export_graphviz(clf, out_file=dot_data)[0m
[1m    [0m
[1m        clf = GradientBoostingClassifier(n_estimators=2, random_state=0)[0m
[1m>       clf.fit(X, y)[0m

clf        = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
dot_data   = <_io.StringIO object at 0x7f46dc1330d8>

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/tree/tests/test_export.py[0m:260: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1472: in fit
[1m    begin_at_stage, monitor, X_idx_sorted)[0m
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
X = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
y = array([0, 0, 0, 1, 1, 1])
y_pred = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
random_state = <mtrand.RandomState object at 0x7f46d7e62438>, X_val = None
y_val = None, sample_weight_val = None, begin_at_stage = 0, monitor = None
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)

[1m    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,[0m
[1m                    X_val, y_val, sample_weight_val,[0m
[1m                    begin_at_stage=0, monitor=None, X_idx_sorted=None):[0m
[1m        """Iteratively fits the stages.[0m
[1m    [0m
[1m            For each stage it computes the progress (OOB, train score)[0m
[1m            and delegates to ``_fit_stage``.[0m
[1m            Returns the number of stages fit; might differ from ``n_estimators``[0m
[1m            due to early stopping.[0m
[1m            """[0m
[1m        n_samples = X.shape[0][0m
[1m        do_oob = self.subsample < 1.0[0m
[1m        sample_mask = np.ones((n_samples, ), dtype=np.bool)[0m
[1m        n_inbag = max(1, int(self.subsample * n_samples))[0m
[1m        loss_ = self.loss_[0m
[1m    [0m
[1m        # Set min_weight_leaf from min_weight_fraction_leaf[0m
[1m        if self.min_weight_fraction_leaf != 0. and sample_weight is not None:[0m
[1m            min_weight_leaf = (self.min_weight_fraction_leaf *[0m
[1m>                              np.sum(sample_weight))[0m
[31m[1mE           DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future[0m

X          = array([[-2., -1.],
       [-1., -1.],
       [-1., -2.],
       [ 1.,  1.],
       [ 1.,  2.],
       [ 2.,  1.]], dtype=float32)
X_idx_sorted = array([[0, 2],
       [1, 0],
       [2, 1],
       [3, 3],
       [4, 5],
       [5, 4]], dtype=int32)
X_val      = None
begin_at_stage = 0
do_oob     = False
loss_      = <sklearn.ensemble.gradient_boosting.BinomialDeviance object at 0x7f46df92d668>
monitor    = None
n_inbag    = 6
n_samples  = 6
random_state = <mtrand.RandomState object at 0x7f46d7e62438>
sample_mask = array([ True,  True,  True,  True,  True,  True], dtype=bool)
sample_weight = array([ 1.,  1.,  1.,  1.,  1.,  1.], dtype=float32)
sample_weight_val = None
self       = GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_...m_state=0,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
y          = array([0, 0, 0, 1, 1, 1])
y_pred     = array([[ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.],
       [ 0.]])
y_val      = None

[31m[1m/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py[0m:1503: DeprecationWarning

----------- coverage: platform linux, python 3.4.5-final-0 -----------
Name                                                                                                                Stmts   Miss  Cover
---------------------------------------------------------------------------------------------------------------------------------------
/home/travis/build/scikit-learn/scikit-learn/sklearn/__check_build/__init__.py                                         18      3    83%
/home/travis/build/scikit-learn/scikit-learn/sklearn/__check_build/setup.py                                             9      2    78%
/home/travis/build/scikit-learn/scikit-learn/sklearn/__init__.py                                                       32     11    66%
/home/travis/build/scikit-learn/scikit-learn/sklearn/_build_utils/__init__.py                                          49     22    55%
/home/travis/build/scikit-learn/scikit-learn/sklearn/_config.py                                                        16      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/base.py                                                          163      5    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/calibration.py                                                   178      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/__init__.py                                               10      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/_feature_agglomeration.py                                 26      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/affinity_propagation_.py                                 121      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/bicluster.py                                             175      9    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/birch.py                                                 231      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/dbscan_.py                                                54      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/hierarchical.py                                          307      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/k_means_.py                                              445     18    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/mean_shift_.py                                           104      5    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/optics_.py                                               199      3    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/setup.py                                                  22      2    91%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/spectral.py                                              101      5    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/__init__.py                                          0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/common.py                                            8      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_affinity_propagation.py                        79      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_bicluster.py                                  151     10    93%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_birch.py                                      105      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_dbscan.py                                     196      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_feature_agglomeration.py                       27      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_hierarchical.py                               332      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_k_means.py                                    507      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_mean_shift.py                                  76      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_optics.py                                      93      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/tests/test_spectral.py                                   108      4    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/compose/__init__.py                                                3      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/compose/_column_transformer.py                                   215     19    91%
/home/travis/build/scikit-learn/scikit-learn/sklearn/compose/_target.py                                                56      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/compose/tests/__init__.py                                          0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/compose/tests/test_column_transformer.py                         475     89    81%
/home/travis/build/scikit-learn/scikit-learn/sklearn/compose/tests/test_target.py                                     156      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/__init__.py                                             6      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/elliptic_envelope.py                                   37      1    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/empirical_covariance_.py                               70      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/graph_lasso_.py                                       232     13    94%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/robust_covariance.py                                  218     16    93%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/shrunk_covariance_.py                                 127     10    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/tests/__init__.py                                       0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/tests/test_covariance.py                              184      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/tests/test_elliptic_envelope.py                        36      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/tests/test_graph_lasso.py                              93      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/tests/test_graphical_lasso.py                          91      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/covariance/tests/test_robust_covariance.py                        67      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cross_decomposition/__init__.py                                    2      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cross_decomposition/cca_.py                                        5      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cross_decomposition/pls_.py                                      215      5    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cross_decomposition/tests/__init__.py                              0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/cross_decomposition/tests/test_pls.py                            189      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/__init__.py                                              49      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/base.py                                                 200      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/california_housing.py                                    42      3    93%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/covtype.py                                               51     27    47%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/kddcup99.py                                             115     80    30%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/lfw.py                                                  162     22    86%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/mlcomp.py                                                51     42    18%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/mldata.py                                                82      9    89%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/olivetti_faces.py                                        41     29    29%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/openml.py                                               210     13    94%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/rcv1.py                                                 115     81    30%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/samples_generator.py                                    401     22    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/setup.py                                                 17      2    88%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/species_distributions.py                                 75     54    28%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/svmlight_format.py                                      137      7    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/__init__.py                                         0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_20news.py                                     54     31    43%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_base.py                                      175      5    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_california_housing.py                         15      4    73%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_common.py                                      5      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_covtype.py                                    21     10    52%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_kddcup99.py                                   34     20    41%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_lfw.py                                        92      3    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_mldata.py                                     90      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_openml.py                                    271      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_rcv1.py                                       50     33    34%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_samples_generator.py                         259      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/tests/test_svmlight_format.py                           314      5    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/twenty_newsgroups.py                                    162    121    25%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/__init__.py                                         12      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/base.py                                             49      1    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/dict_learning.py                                   331     26    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/factor_analysis.py                                 112      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/fastica_.py                                        182      7    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/incremental_pca.py                                  74      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/kernel_pca.py                                       92      2    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/nmf.py                                             399     11    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/online_lda.py                                      241      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/pca.py                                             160      4    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/setup.py                                            15      2    87%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/sparse_pca.py                                       81      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/__init__.py                                    0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_dict_learning.py                        243      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_factor_analysis.py                       52      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_fastica.py                              170      4    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_incremental_pca.py                      199      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_kernel_pca.py                           119      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_nmf.py                                  314      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_online_lda.py                           233      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_pca.py                                  439      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_sparse_pca.py                           154     19    88%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/tests/test_truncated_svd.py                        129      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/truncated_svd.py                                    48      1    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/discriminant_analysis.py                                         235      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/dummy.py                                                         172      3    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/__init__.py                                              20      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/bagging.py                                              264      5    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/base.py                                                  54      1    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/forest.py                                               343      6    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/gradient_boosting.py                                    665    132    80%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/iforest.py                                              111      2    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/partial_dependence.py                                   160    144    10%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/setup.py                                                 10      2    80%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/__init__.py                                         0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_bagging.py                                   379      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_base.py                                       73      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_forest.py                                    699      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting.py                         727    362    50%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_gradient_boosting_loss_functions.py          121      6    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_iforest.py                                   165      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_partial_dependence.py                         97     63    35%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_voting_classifier.py                         237      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/tests/test_weight_boosting.py                           257      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/voting_classifier.py                                     91      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/weight_boosting.py                                      284     11    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/exceptions.py                                                     11      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/__init__.py                                              1      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/_arff.py                                               423    199    53%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/_pilutil.py                                            152     72    53%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/conftest.py                                              2      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/funcsigs.py                                            424    350    17%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/__init__.py                                      15      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_compat.py                                       11      1    91%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_dask.py                                        158    121    23%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_memmapping_reducer.py                          180     80    56%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_memory_helpers.py                               65     63     3%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_multiprocessing_helpers.py                      21      8    62%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_parallel_backends.py                           250     60    76%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/_store_backends.py                              189     75    60%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/backports.py                                     48     33    31%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/compressor.py                                   309    108    65%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/disk.py                                          60     33    45%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/executor.py                                      29      9    69%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/__init__.py                             0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/cloudpickle/__init__.py                 3      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py            533    397    26%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/__init__.py                        5      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/_base.py                         279    169    39%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/__init__.py               11      2    82%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/_posix_reduction.py       41     11    73%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/_posix_wait.py            60     39    35%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/_win_reduction.py         64     53    17%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/_win_wait.py              28     24    14%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/compat.py                 12      4    67%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/compat_posix.py            4      1    75%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/compat_win32.py           35     32     9%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/context.py               117     65    44%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/fork_exec.py               7      1    86%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/managers.py               18     13    28%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/popen_loky_posix.py      150     46    69%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/popen_loky_win32.py       56     39    30%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/process.py                44     26    41%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/queues.py                131     45    66%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/reduction.py              99     34    66%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/semaphore_tracker.py     129     56    57%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/semlock.py               179    133    26%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/spawn.py                 117     59    50%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/synchronize.py           221    135    39%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/backend/utils.py                  68     37    46%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/process_executor.py              502    173    66%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/externals/loky/reusable_executor.py              91     16    82%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/format_stack.py                                 209    188    10%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/func_inspect.py                                 176     72    59%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/hashing.py                                      114     31    73%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/logger.py                                        76     40    47%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/memory.py                                       344    125    64%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/my_exceptions.py                                 53     20    62%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/numpy_pickle.py                                 203     45    78%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/numpy_pickle_compat.py                          100     71    29%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/numpy_pickle_utils.py                            93     21    77%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/parallel.py                                     366     69    81%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/pool.py                                         116     83    28%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/joblib/testing.py                                       40     22    45%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/setup.py                                                 9      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/externals/six.py                                                 296    136    54%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/__init__.py                                     5      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/dict_vectorizer.py                            125      3    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/hashing.py                                     47      2    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/image.py                                      154      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/setup.py                                       13      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/stop_words.py                                   1      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/tests/__init__.py                               0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/tests/test_dict_vectorizer.py                  68      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/tests/test_feature_hasher.py                  100      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/tests/test_image.py                           217      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/tests/test_text.py                            637      9    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/text.py                                       460     16    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/__init__.py                                     16      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/base.py                                         39      1    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/from_model.py                                   85      6    93%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/mutual_info_.py                                113      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/rfe.py                                         129      3    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/tests/__init__.py                                0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/tests/test_base.py                              72      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/tests/test_chi2.py                              53      2    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/tests/test_feature_select.py                   389      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/tests/test_from_model.py                       187      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/tests/test_mutual_info.py                       92      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/tests/test_rfe.py                              236      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/tests/test_variance_threshold.py                14      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/univariate_selection.py                        188      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/variance_threshold.py                           23      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/__init__.py                                       7      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/correlation_models.py                            79     70    11%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/gpc.py                                          190     15    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/gpr.py                                          152     12    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/kernels.py                                      544     46    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/regression_models.py                             20     14    30%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/tests/__init__.py                                 0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/tests/test_gpc.py                                77      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/tests/test_gpr.py                               179      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/tests/test_kernels.py                           161      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/impute.py                                                        223      4    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/isotonic.py                                                       98      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/kernel_approximation.py                                          160      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/kernel_ridge.py                                                   41      1    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/__init__.py                                          17      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/base.py                                             180      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/bayes.py                                            162      7    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/coordinate_descent.py                               433     13    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/huber.py                                             89      2    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/least_angle.py                                      401     15    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/logistic.py                                         487      8    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/omp.py                                              269     10    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/passive_aggressive.py                                31      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/perceptron.py                                         4      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/randomized_l1.py                                    184      8    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/ransac.py                                           144      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/ridge.py                                            421     15    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/sag.py                                               71      4    94%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/setup.py                                             18      2    89%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/stochastic_gradient.py                              429      6    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/__init__.py                                     0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_base.py                                  278      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_bayes.py                                 102      9    91%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_coordinate_descent.py                    501      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_huber.py                                 123      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_least_angle.py                           336      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_logistic.py                              750      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_omp.py                                   144      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_passive_aggressive.py                    174      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_perceptron.py                             51      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_randomized_l1.py                         112      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_ransac.py                                269      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_ridge.py                                 580      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_sag.py                                   502      7    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_sgd.py                                   822      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_sparse_coordinate_descent.py             193      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/tests/test_theil_sen.py                             192      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/theil_sen.py                                        113      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/__init__.py                                               6      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/isomap.py                                                48      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/locally_linear.py                                       219     17    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/mds.py                                                  104      6    94%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/setup.py                                                 20      2    90%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/spectral_embedding_.py                                  163     26    84%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/t_sne.py                                                254      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/tests/__init__.py                                         0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/tests/test_isomap.py                                     71      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/tests/test_locally_linear.py                             84      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/tests/test_mds.py                                        22      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/tests/test_spectral_embedding.py                        158      6    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/tests/test_t_sne.py                                     488      5    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/__init__.py                                               57      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/base.py                                                   47      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/classification.py                                        405      7    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/__init__.py                                       18      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/bicluster.py                                      31      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/setup.py                                          14      2    86%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/supervised.py                                    154      3    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/tests/__init__.py                                  0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/tests/test_bicluster.py                           30      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/tests/test_common.py                              85      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/tests/test_supervised.py                         189      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/tests/test_unsupervised.py                       106      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/unsupervised.py                                   86      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/pairwise.py                                              299      9    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/ranking.py                                               192      3    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/regression.py                                            110      4    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/scorer.py                                                164      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/setup.py                                                  17      2    88%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/tests/__init__.py                                          0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/tests/test_classification.py                             811      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/tests/test_common.py                                     419      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/tests/test_pairwise.py                                   522      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/tests/test_ranking.py                                    667      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/tests/test_regression.py                                 116      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/tests/test_score_objects.py                              290      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/mixture/__init__.py                                                3      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/mixture/base.py                                                  172     12    93%
/home/travis/build/scikit-learn/scikit-learn/sklearn/mixture/bayesian_mixture.py                                      160      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/mixture/gaussian_mixture.py                                      187      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/mixture/tests/__init__.py                                          0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/mixture/tests/test_bayesian_mixture.py                           222      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/mixture/tests/test_gaussian_mixture.py                           571      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/mixture/tests/test_mixture.py                                     11      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/__init__.py                                       29      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/_search.py                                       280      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/_split.py                                        479      4    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/_validation.py                                   303      8    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/tests/__init__.py                                  0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/tests/common.py                                   12      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/tests/test_search.py                             899      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/tests/test_split.py                              850     65    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/tests/test_validation.py                         816     12    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/multiclass.py                                                    240     12    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/multioutput.py                                                   188     14    93%
/home/travis/build/scikit-learn/scikit-learn/sklearn/naive_bayes.py                                                   264      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/__init__.py                                             12      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/approximate.py                                         193      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/base.py                                                347     13    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/classification.py                                       93      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/graph.py                                                25      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/kde.py                                                  82      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/lof.py                                                  79      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/nearest_centroid.py                                     66      1    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/regression.py                                           51      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/setup.py                                                15      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/__init__.py                                        0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_approximate.py                              249      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_ball_tree.py                                187      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_dist_metrics.py                             116      2    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_kd_tree.py                                  155      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_kde.py                                      139      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_lof.py                                      125      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_nearest_centroid.py                          95      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_neighbors.py                                754      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/tests/test_quad_tree.py                                 55      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/unsupervised.py                                          7      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/__init__.py                                         4      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/_base.py                                           39      2    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/_stochastic_optimizers.py                          62      4    94%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/multilayer_perceptron.py                          369      5    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/rbm.py                                            105      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/tests/__init__.py                                   0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/tests/test_mlp.py                                 355      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/tests/test_rbm.py                                 114      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/tests/test_stochastic_optimizers.py                74      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/pipeline.py                                                      237      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/__init__.py                                         30      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/_discretization.py                                 100      2    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/_encoders.py                                       280      4    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/_function_transformer.py                            53      1    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/base.py                                             33      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/data.py                                            733     11    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/imputation.py                                      153      8    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/label.py                                           304      9    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/__init__.py                                    0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_base.py                                  50      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_common.py                                78      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py                                1346      4    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_discretization.py                       127      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_encoders.py                             332     21    94%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py                  78      4    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_imputation.py                           166      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_label.py                                322      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/random_projection.py                                             107      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/semi_supervised/__init__.py                                        2      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/semi_supervised/label_propagation.py                             135      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/semi_supervised/tests/__init__.py                                  0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/semi_supervised/tests/test_label_propagation.py                  107      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/setup.py                                                          58     52    10%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/__init__.py                                                    4      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/base.py                                                      328      9    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/bounds.py                                                     19      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/classes.py                                                    99      2    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/setup.py                                                      24      2    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/tests/__init__.py                                              0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/tests/test_bounds.py                                          43      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/tests/test_sparse.py                                         192      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/svm/tests/test_svm.py                                            522      5    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/__init__.py                                                  0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_base.py                                               245      4    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_calibration.py                                        163      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_check_build.py                                          4      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_common.py                                             103      3    97%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_config.py                                              44      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_discriminant_analysis.py                              193      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_docstring_parameters.py                                80     49    39%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_dummy.py                                              393      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_impute.py                                             275     13    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_init.py                                                25      3    88%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_isotonic.py                                           241      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_kernel_approximation.py                               151      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_kernel_ridge.py                                        56      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_metaestimators.py                                      80      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_multiclass.py                                         449      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_multioutput.py                                        305     11    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_naive_bayes.py                                        372      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_pipeline.py                                           584      4    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_random_projection.py                                  181      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_site_joblib.py                                         24      9    62%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tree/__init__.py                                                   6      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tree/export.py                                                   193     10    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tree/setup.py                                                     17      2    88%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tree/tests/__init__.py                                             0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tree/tests/test_export.py                                         92      4    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tree/tests/test_tree.py                                          946      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/tree/tree.py                                                     267      3    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/__init__.py                                                140      7    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/_joblib.py                                                  14      6    57%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/_scipy_sparse_lsqr_backport.py                             199    191     4%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/_show_versions.py                                           44      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/_unittest_backport.py                                       85     10    88%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/arpack.py                                                    8      2    75%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/bench.py                                                     3      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/class_weight.py                                             59      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/deprecation.py                                              65      7    89%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/estimator_checks.py                                       1328     87    93%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/extmath.py                                                 211      9    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/fixes.py                                                   171     70    59%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/graph.py                                                    26      3    88%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/linear_assignment_.py                                      106      5    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/metaestimators.py                                           89      2    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/mocking.py                                                  52      1    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/multiclass.py                                              145      6    96%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/optimize.py                                                 68      9    87%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/random.py                                                   47      4    91%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/setup.py                                                    30      2    93%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/sparsefuncs.py                                             196     12    94%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/sparsetools/__init__.py                                      4      1    75%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/sparsetools/setup.py                                         8      2    75%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/sparsetools/tests/__init__.py                                0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/stats.py                                                    12      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/testing.py                                                 397     97    76%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/__init__.py                                            0      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_bench.py                                          6      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_class_weight.py                                 140      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_deprecation.py                                   47      5    89%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_estimator_checks.py                             278     23    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_extmath.py                                      374      2    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_fast_dict.py                                     23      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_fixes.py                                         28      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_graph.py                                         14      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_linear_assignment.py                             18      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_metaestimators.py                                38      6    84%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_multiclass.py                                   152      3    98%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_murmurhash.py                                    51      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_optimize.py                                      18      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_random.py                                        89      1    99%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_seq_dataset.py                                   56      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_shortest_path.py                                 53      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_show_versions.py                                 23      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_sparsefuncs.py                                  327      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_stats.py                                          8      0   100%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_testing.py                                      257     39    85%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_utils.py                                        177     14    92%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_validation.py                                   423     23    95%
/home/travis/build/scikit-learn/scikit-learn/sklearn/utils/validation.py                                              262      6    98%
---------------------------------------------------------------------------------------------------------------------------------------
TOTAL                                                                                                               74283   6958    91%

=========================== short test summary info ============================
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/neural_network/rbm.py: doctests are only run for numpy >= 1.14
SKIP [2] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: kddcup99 dataset can not be loaded.
SKIP [1] /home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_encoders.py:452: could not import 'pandas'
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/nmf.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/logistic.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: Download RCV1 dataset to run this test.
SKIP [2] /home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:926: RobustScaler cannot center sparse matrix
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/forest.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/stochastic_gradient.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/build/scikit-learn/scikit-learn/sklearn/compose/tests/test_column_transformer.py:375: could not import 'pandas'
SKIP [3] /home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_encoders.py:271: could not import 'pandas'
SKIP [13] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/data.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/unsupervised.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/utils/graph.py: doctests are only run for numpy >= 1.14
SKIP [7] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/base.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/factor_analysis.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/isomap.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: numpydoc is required to test the docstrings, as well as python version >= 3.5
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: Not testing NuSVC class weight as it is ignored.
SKIP [9] /home/travis/build/scikit-learn/scikit-learn/sklearn/compose/tests/test_column_transformer.py:690: could not import 'pandas'
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: pyamg not available.
SKIP [1] /home/travis/build/scikit-learn/scikit-learn/sklearn/utils/tests/test_validation.py:699: could not import 'pandas'
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: Covertype dataset can not be loaded.
SKIP [4] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/ridge.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: numpydoc is required to test the docstrings
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/tree/export.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/build/scikit-learn/scikit-learn/sklearn/compose/tests/test_column_transformer.py:878: could not import 'pandas'
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/mean_shift_.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/incremental_pca.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/base.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cross_decomposition/cca_.py: doctests are only run for numpy >= 1.14
SKIP [7] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/coordinate_descent.py: doctests are only run for numpy >= 1.14
SKIP [3] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/semi_supervised/label_propagation.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/affinity_propagation_.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/nearest_centroid.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/variance_threshold.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/hashing.py: doctests are only run for numpy >= 1.14
SKIP [4] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/naive_bayes.py: doctests are only run for numpy >= 1.14
SKIP [3] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: Pandas not found
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/sag.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/spectral.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/scorer.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/_encoders.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/impute.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_encoders.py:555: could not import 'pandas'
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/gpr.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: This test is failing on the buildbot, but cannot reproduce. Temporarily disabling it until it can be reproduced and  fixed.
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/bicluster.py: doctests are only run for numpy >= 1.14
SKIP [3] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/random_projection.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/utils/random.py: doctests are only run for numpy >= 1.14
SKIP [2] /home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_impute.py:301: could not import 'pandas'
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/k_means_.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/image.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/_discretization.py: doctests are only run for numpy >= 1.14
SKIP [3] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: Skipping check_estimators_data_not_an_array for cross decomposition module as estimators are not deterministic.
SKIP [4] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/pairwise.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/bayes.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/voting_classifier.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/mds.py: doctests are only run for numpy >= 1.14
SKIP [6] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/ranking.py: doctests are only run for numpy >= 1.14
SKIP [16] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/classification.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/perceptron.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/locally_linear.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/randomized_l1.py: doctests are only run for numpy >= 1.14
SKIP [7] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/utils/__init__.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/utils/deprecation.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/tree/tree.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: transform of MiniBatchSparsePCA is not invariant when applied to a subset.
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/ensemble/partial_dependence.py: doctests are only run for numpy >= 1.14
SKIP [3] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/utils/extmath.py: doctests are only run for numpy >= 1.14
SKIP [6] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/svm/classes.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/graph.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: This test will run only on python3.5 and above
SKIP [4] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/_validation.py: doctests are only run for numpy >= 1.14
SKIP [3] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/least_angle.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/sparse_pca.py: doctests are only run for numpy >= 1.14
SKIP [4] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/kernel_approximation.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/t_sne.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/datasets/samples_generator.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/classification.py: doctests are only run for numpy >= 1.14
SKIP [2] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: skipping mini_batch_fit_transform.
SKIP [4] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/base.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/_config.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/compose/_column_transformer.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_init.py:62: soft-failed test_import_sklearn_no_warnings.
 assert 'Warning' not in '/home/travis/build/sc...ingDeprecationWarning)'
  'Warning' is contained here:
    13: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.
  ?           +++++++
      _nan_object_mask = _nan_object_array != _nan_object_array
    /home/travis/miniconda/envs/testenv/lib/python3.4/imp.py:32: PendingDeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
      PendingDeprecationWarning), 
 /home/travis/build/scikit-learn/scikit-learn/sklearn/utils/fixes.py:313: FutureWarning: numpy not_equal will not check object identity in the future. The comparison did not return the same result as suggested by the identity (`is`)) and will change.
  _nan_object_mask = _nan_object_array != _nan_object_array
/home/travis/miniconda/envs/testenv/lib/python3.4/imp.py:32: PendingDeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  PendingDeprecationWarning)
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: score_samples of BernoulliRBM is not invariant when applied to a subset.
SKIP [46] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: pandas is not installed: not testing for input of type pandas.Series to class weight.
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/exceptions.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/kernel_pca.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: California housing dataset can not be loaded.
SKIP [3] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: Matplotlib not available.
SKIP [6] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/regression.py: doctests are only run for numpy >= 1.14
SKIP [7] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/metrics/cluster/supervised.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/regression.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/kernel_ridge.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/build/scikit-learn/scikit-learn/sklearn/compose/tests/test_column_transformer.py:134: could not import 'pandas'
SKIP [6] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/univariate_selection.py: doctests are only run for numpy >= 1.14
SKIP [3] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: Download 20 newsgroups to run this test
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/pca.py: doctests are only run for numpy >= 1.14
SKIP [3] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/utils/multiclass.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: transform of SparsePCA is not invariant when applied to a subset.
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/gaussian_process/gpc.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/utils/validation.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/feature_selection/rfe.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/truncated_svd.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_function_transformer.py:206: could not import 'pandas'
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: test_bayesian_on_diabetes is broken
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/compose/_target.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/dbscan_.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/manifold/spectral_embedding_.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/linear_model/passive_aggressive.py: doctests are only run for numpy >= 1.14
SKIP [3] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/_search.py: doctests are only run for numpy >= 1.14
SKIP [2] /home/travis/build/scikit-learn/scikit-learn/sklearn/tests/test_impute.py:414: could not import 'pandas'
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/utils/testing.py: doctests are only run for numpy >= 1.14
SKIP [4] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/label.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/fastica_.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/discriminant_analysis.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/birch.py: doctests are only run for numpy >= 1.14
SKIP [3] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/text.py: doctests are only run for numpy >= 1.14
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/decomposition/online_lda.py: doctests are only run for numpy >= 1.14
SKIP [3] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cross_decomposition/pls_.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/feature_extraction/dict_vectorizer.py: doctests are only run for numpy >= 1.14
SKIP [2] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/cluster/hierarchical.py: doctests are only run for numpy >= 1.14
SKIP [4] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/pipeline.py: doctests are only run for numpy >= 1.14
SKIP [4] /home/travis/build/scikit-learn/scikit-learn/sklearn/preprocessing/tests/test_data.py:719: 'with_mean=True' cannot be used with sparse matrix.
SKIP [1] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/neighbors/approximate.py: doctests are only run for numpy >= 1.14
SKIP [1] /home/travis/miniconda/envs/testenv/lib/python3.4/site-packages/_pytest/nose.py:24: decision_function of SVC is not invariant when applied to a subset.
SKIP [15] ../../home/travis/build/scikit-learn/scikit-learn/sklearn/model_selection/_split.py: doctests are only run for numpy >= 1.14
========================== slowest 20 test durations ===========================
33.02s call     sklearn/utils/tests/test_extmath.py::test_randomized_svd_power_iteration_normalizer
18.06s call     sklearn/decomposition/tests/test_dict_learning.py::test_sparse_coder_parallel_mmap
13.64s call     sklearn/linear_model/tests/test_theil_sen.py::test_theil_sen_parallel
13.15s call     sklearn/utils/tests/test_estimator_checks.py::test_check_estimator
12.96s call     sklearn/metrics/tests/test_classification.py::test_matthews_corrcoef_overflow[1000000]
11.86s call     sklearn/tests/test_multioutput.py::test_multi_output_classification_partial_fit_parallelism
10.21s call     sklearn/cluster/tests/test_mean_shift.py::test_parallel
10.16s call     sklearn/utils/tests/test_estimator_checks.py::test_check_estimator_clones
5.90s call     sklearn/neural_network/tests/test_mlp.py::test_n_iter_no_change_inf
4.72s call     sklearn/tree/tests/test_tree.py::test_min_impurity_decrease
4.71s call     sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_callable_affinity
4.39s call     sklearn/tests/test_common.py::test_non_meta_estimators[LogisticRegressionCV-LogisticRegressionCV-check_estimator_sparse_data]
4.37s call     sklearn/manifold/tests/test_spectral_embedding.py::test_spectral_embedding_precomputed_affinity
4.14s call     sklearn/tests/test_common.py::test_non_meta_estimators[GaussianProcessClassifier-GaussianProcessClassifier-check_classifiers_train]
4.02s call     sklearn/manifold/tests/test_mds.py::test_MDS
3.84s call     sklearn/tests/test_common.py::test_non_meta_estimators[GaussianProcessClassifier-GaussianProcessClassifier-check_classifiers_train(readonly_memmap=True)]
3.84s call     sklearn/model_selection/tests/test_search.py::test_random_search_cv_results_multimetric
3.69s call     sklearn/manifold/tests/test_t_sne.py::test_uniform_grid[exact]
3.61s call     sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-False-threshold]
3.49s call     sklearn/decomposition/tests/test_dict_learning.py::test_dict_learning_online_positivity[True-False-lasso_lars]
[31m[1m===== 117 failed, 9777 passed, 348 skipped, 993 warnings in 786.02 seconds =====[0m
